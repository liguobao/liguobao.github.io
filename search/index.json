[{"content":" PS: Holiday\u0026rsquo;s coming, time for more tinkering!!\nManual dog head emoji\nã€Programming Tech Zone Disclaimerã€‘\nMy last k3s article was written in 2025, using WireGuard for cross-cloud networking.\nA year has passed, and WireGuard configuration is still too tedious.\nâ€”Every time you add a machine, you have to manually edit configs, add peers, adjust routes\u0026hellip;\nSo this year, I decided to switch the underlying network entirely to Tailscale.\nMore specificallyâ€”self-hosting a Headscale control plane, without relying on the official SaaS.\nThis way, you can add as many nodes to the internal network as you want,\nwith a one-line command to join the network, no more manual WireGuard Peer configuration management.\nAt the same time, the k3s cluster has been upgraded from a humble 2-node version,\nto an 8-node HA cluster with 4 Masters + 4 Edges.\nSmall but complete.\nLet\u0026rsquo;s get to it.\nOverall Architecture First, here\u0026rsquo;s an architecture diagram for a global overview:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Internet â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ [Tencent Cloud] [Rainyun] [Local] (Domestic) (Singapore) (Home/Office) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Tailscale VPN (100.64.0.0/10) Headscale Self-hosted (ts.example.com) Authentik OIDC Login (auth.example.com) â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ [control1] [control2] [control3] [control4] vm-0-8 vm-16-12 vm-28-17 vm-0-15 100.64.0.6 100.64.0.5 100.64.0.7 100.64.0.10 (Primary) (Member) (Member) (TS Control) â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ K3S HA Cluster (Flannel VXLAN over Tailscale) Pod: 10.42.0.0/16 | Service: 10.43.0.0/16 â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ [haru] [lgb-amd] [rainyun] [hc1] Domestic Local Overseas Domestic Ready âœ… Ready âœ… Ready âœ… Ready âœ… Current cluster state:\nNode Name Role Tailscale IP Status Notes vm-0-8-ubuntu-new Master 100.64.0.6 Ready âœ… Initial Master, \u0026ndash;cluster-init vm-16-12-ubuntu Master 100.64.0.5 Ready âœ… HA Master vm-28-17-ubuntu Master 100.64.0.7 Ready âœ… HA Master vm-0-15-ubuntu Master 100.64.0.10 Ready âœ… Headscale control plane node, NoSchedule haru Edge 100.64.0.3 Ready âœ… Domestic node lgb-amd-3700 Edge 100.64.0.4 Ready âœ… Local AMD host, 16GB rainyun-ssh7pavp Edge 100.64.0.12 Ready âœ… Overseas Singapore node hc1 Edge 100.64.0.9 Ready âœ… Domestic node 8-node cluster running dozens of services, stable as a rock.\nPart I: Setting up Tailscale - Cloud Internal Network Cluster Why Not Manually Configure WireGuard Anymore? In the 2025 article, I used the native WireGuard solution + k3s Flannel wireguard-native mode.\nTo be honest, WireGuard itself is very stable with good performance. But manually managing peers becomes a nightmare when you have multiple nodes:\nEvery time you add a node, you need to update WireGuard configs on all other nodes Public key exchange and IP allocation are all manual If a machine\u0026rsquo;s public IP changes, all peers need to be updated No unified management interface So this time, I went straight to Tailscale.\nWhy Self-host Headscale? Tailscale\u0026rsquo;s official SaaS service is certainly convenient, and the personal plan now supports up to 100 devices, which is more than enough. But there are a few issues:\nLogin requires external networkâ€”Tailscale login authentication uses Google/Microsoft/GitHub OAuth, basically unusable from mainland China without VPN, especially troublesome on servers Data controlâ€”All node information is on someone else\u0026rsquo;s servers, not reassuring Domestic accessâ€”Tailscale\u0026rsquo;s official coordination server is overseas, domestic nodes have unstable connections, occasionally slow to establish connections between nodes So I chose Headscaleâ€”an open-source Tailscale control plane implementation.\nCombined with Authentik for OIDC login, the experience is almost identical to official Tailscale, even more flexible.\nHeadscale Deployment Server requirements: A small 2C2G machine is enough, I\u0026rsquo;m using Tencent Cloud Lighthouse Ubuntu 24.04.\nThe core is a docker-compose.yml containing:\nComponent Description Headscale v0.28.0 Tailscale control plane Authentik 2025.2.4 OIDC username/password login PostgreSQL 16 Authentik database Redis Authentik cache Nginx HTTPS reverse proxy After deployment, two domains are ready:\nhttps://ts.example.com â†’ Headscale control plane https://auth.example.com â†’ Authentik login management I won\u0026rsquo;t expand on the detailed deployment process here (that\u0026rsquo;s for another article), but the key points are:\nUse acme.sh + Nginx for HTTPS, don\u0026rsquo;t mess with fancy Traefik stuff Configure OIDC in Headscale, with Issuer pointing to Authentik Use 100.64.0.0/10 IP range, this is the CGNAT address range that won\u0026rsquo;t conflict with internal networks Node Onboarding After setting up Headscale, getting any machine on the network is a one-line command:\n1 2 3 4 5 # Install Tailscale client curl -fsSL https://tailscale.com/install.sh | sh # Connect to self-hosted control plane tailscale up --login-server=https://ts.example.com --hostname=your-hostname --accept-dns=false The terminal will output a link, open it in a browser, it jumps to the Authentik login page, enter username and password, authorizeâ€”node is on the network.\nThat simple.\nKey recommendation:\nDon\u0026rsquo;t rely on public cloud internal networks!\nEven if your Master nodes are in the same cloud, it\u0026rsquo;s recommended to communicate uniformly through Tailscale.\nThe reason is simple: public cloud internal networks are black boxes. When you change machines or availability zones, internal IPs change. While Tailscale IPs are allocated by you and won\u0026rsquo;t change.\nAll nodes, whether cloud-based, local, or on other cloud platforms, should uniformly access via Tailscale for a clean network architecture.\nMy current Tailscale network looks like this:\n1 2 3 4 5 6 7 8 9 10 11 100.64.0.2 lgb-macbookair-m4 macOS â† My dev machine 100.64.0.3 haru linux â† Edge node 100.64.0.4 lgb-amd-3700 linux â† Local Edge node 100.64.0.5 vm-16-12-ubuntu linux â† K3s Master 100.64.0.6 vm-0-8-ubuntu-new linux â† K3s Master (Primary) 100.64.0.7 vm-28-17-ubuntu linux â† K3s Master 100.64.0.8 rainyun-vja2g92e linux â† Overseas node 100.64.0.9 hc1 linux â† Edge node 100.64.0.10 ts-headscale linux â† Headscale control plane + K3s Master 100.64.0.12 ipv6radar linux â† Overseas node 100.64.0.13 localhost android â† Phone can join too (for slacking off) No matter where you are, ping 100.64.0.6 works. This is what a proper internal network experience should be.\nPart II: K3s Cluster Setup - Based on Tailscale Network Core Principles All Master nodes in the same cloud regionâ€”My 4 Masters are all in Tencent Cloud, low etcd sync latency Masters communicate using Tailscale IPsâ€”Don\u0026rsquo;t rely on cloud internal network Edge nodes anywhereâ€”Home desktop, overseas VPS, office workstation, as long as Tailscale can reach Gateway nodes on high-bandwidth machinesâ€”Ingress runs on lightweight cloud with sufficient traffic Installing Masters Installation is actually straightforward, just follow the official docs: https://docs.k3s.io/quick-start\nBut there are a few critical parameters you must pay attention to.\nFirst Master (initialize cluster):\n1 2 3 4 5 6 7 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\ INSTALL_K3S_MIRROR=cn \\ sh -s - server \\ --cluster-init \\ --flannel-iface=tailscale0 \\ --node-ip=100.64.0.6 \\ --tls-san=100.64.0.6 Subsequent Masters joining cluster:\n1 2 3 4 5 6 7 8 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\ INSTALL_K3S_MIRROR=cn \\ K3S_TOKEN=\u0026#34;your-token\u0026#34; \\ sh -s - server \\ --server https://100.64.0.6:6443 \\ --flannel-iface=tailscale0 \\ --node-ip=$(tailscale ip -4) \\ --tls-san=$(tailscale ip -4) Core parameter explanation:\nParameter Why it\u0026rsquo;s necessary --cluster-init First Master uses this, enables embedded etcd HA --flannel-iface=tailscale0 Most critical! Makes Flannel VXLAN use Tailscale NIC, not physical NIC --node-ip=$(tailscale ip -4) Node IP uses Tailscale IP, ensures cross-cloud communication --tls-san=\u0026lt;ip\u0026gt; API Server certificate includes Tailscale IP INSTALL_K3S_MIRROR=cn Domestic mirror acceleration, overseas nodes don\u0026rsquo;t need --flannel-iface=tailscale0 is the hard-learned lesson after countless pitfalls.\nWithout this parameter, Flannel will default to using the physical NIC\u0026rsquo;s IP (like public IP or cloud internal IP) to build VXLAN tunnels. The result isâ€”same-cloud nodes can communicate, cross-cloud nodes have complete Pod communication failure.\nWith this parameter, Flannel\u0026rsquo;s VXLAN tunnels all go through the Tailscale virtual NIC, cross-cloud Pod communication is perfect.\nInstalling Edge Nodes Edge nodes are Agents, even simpler.\nDomestic nodes:\n1 2 3 4 5 6 7 8 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\ K3S_URL=\u0026#34;https://100.64.0.6:6443\u0026#34; \\ K3S_TOKEN=\u0026#34;your-token\u0026#34; \\ INSTALL_K3S_MIRROR=cn \\ sh -s - agent \\ --node-ip=$(tailscale ip -4) \\ --flannel-iface=tailscale0 \\ --node-label=\u0026#34;node.kubernetes.io/role=edge\u0026#34; Overseas nodes:\n1 2 3 4 5 6 7 curl -sfL https://get.k3s.io | \\ K3S_URL=\u0026#34;https://100.64.0.6:6443\u0026#34; \\ K3S_TOKEN=\u0026#34;your-token\u0026#34; \\ sh -s - agent \\ --node-ip=$(tailscale ip -4) \\ --flannel-iface=tailscale0 \\ --node-label=\u0026#34;node.kubernetes.io/role=edge\u0026#34; The difference is domestic uses rancher-mirror.rancher.cn, overseas uses official get.k3s.io.\nDomestic Docker Image Pulling Issues I need to specifically mention: Docker/containerd image pulling in domestic environments is a big hassle.\nDocker Hub, ghcr.io, gcr.io and other image sources are basically semi-blocked in China. Some k3s built-in system component images (like pause, coredns, metrics-server) might not be pullable, causing nodes to stay NotReady.\nSeveral solutions:\nConfigure image acceleratorsâ€”Configure available domestic mirrors in /etc/rancher/k3s/registries.yaml (if you can still find live ones) Local export then import (recommended)â€”Pull images on overseas nodes or machines that can pull normally, export and transfer to domestic nodes for import: 1 2 3 4 5 6 7 8 # Export on overseas node ctr -n k8s.io images export pause.tar registry.k8s.io/pause:3.9 # Transfer to domestic node scp pause.tar root@\u0026lt;domestic-node-IP\u0026gt;:/tmp/ # Import on domestic node ctr -n k8s.io images import /tmp/pause.tar Self-host Harbor image registryâ€”If you have many nodes, it\u0026rsquo;s recommended to set up a private image registry as a proxy cache, once and for all My approach is to pre-pull needed images on overseas nodes, then ctr images export to export, transfer through Tailscale internal network to domestic nodes, and ctr images import to import. Though crude, it\u0026rsquo;s stable and reliable.\nGet the token on the first Master:\n1 cat /var/lib/rancher/k3s/server/node-token If everything\u0026rsquo;s fine, you\u0026rsquo;ll see new nodes online within seconds:\n1 2 3 4 5 6 7 8 9 10 $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION vm-0-8-ubuntu-new Ready control-plane,etcd,master 30d v1.32.3+k3s1 vm-16-12-ubuntu Ready control-plane,etcd,master 30d v1.32.3+k3s1 vm-28-17-ubuntu Ready control-plane,etcd,master 30d v1.32.3+k3s1 vm-0-15-ubuntu Ready control-plane,etcd,master 1d v1.32.3+k3s1 haru Ready \u0026lt;none\u0026gt; 20d v1.32.3+k3s1 lgb-amd-3700 Ready \u0026lt;none\u0026gt; 15d v1.32.3+k3s1 rainyun-ssh7pavp Ready \u0026lt;none\u0026gt; 10d v1.32.3+k3s1 hc1 Ready \u0026lt;none\u0026gt; 5d v1.32.3+k3s1 All 8 nodes Ready.\nBeautiful.\nAbout Gateway Nodes Ingress traffic entry requires public IP + sufficient bandwidth.\nMy approach:\nGateway nodes use lightweight cloud serversâ€”Tencent Cloud Lighthouse, Alibaba Cloud Lighthouse, etc., monthly payment of tens of yuan, traffic package is sufficient k3s built-in Traefik Ingress runs on all nodes by default (DaemonSet), but only one or two nodes need to be exposed externally Domain DNS resolves to these lightweight cloud public IPs Traffic path: User request â†’ Lightweight cloud public IP â†’ Traefik Ingress â†’ Service â†’ Pod (can be on any node)\nEven if the Pod is on an overseas node, no problemâ€”Flannel over Tailscale will route the traffic.\nPart III: Pitfall Chronicles Pitfall 1: Flannel Using Wrong NIC (Hard-learned Lesson) Symptom: Edge node is Ready, but cross-node Pod communication fails completely.\nCause: Didn\u0026rsquo;t add --flannel-iface=tailscale0, Flannel defaulted to public network NIC.\nDiagnosis:\n1 2 kubectl describe node rainyun-ssh7pavp | grep flannel # See flannel.alpha.coreos.com/public-ip using public IP instead of 100.64.0.x Solution: Uninstall k3s agent, reinstall with --flannel-iface=tailscale0.\nThis parameter is so important, say it three times:\n--flannel-iface=tailscale0 --flannel-iface=tailscale0 --flannel-iface=tailscale0 Pitfall 2: Unstable Tailscale Connection A Vultr VPS had Tailscale connections dropping every few days.\nNode kept bouncing between Ready and NotReady, kubelet frantically reporting unable to update node status.\nInvestigation revealed it was a VPN link issue, nothing to do with K3s.\nSolution: First use taint to isolate, later just replaced the machine.\n1 2 kubectl taint nodes vultr.guest node-problem=true:NoSchedule --overwrite kubectl taint nodes vultr.guest node-problem=true:NoExecute --overwrite Lesson: Better to remove unstable nodes than struggle. Replacing with a new machine is faster than troubleshooting network issues.\nPitfall 3: Don\u0026rsquo;t Run Business Pods on Low-spec Masters My fourth Master (vm-0-15-ubuntu) only has 2G memory and also runs the Headscale control plane.\nIf you let it run business Pods, it\u0026rsquo;ll OOM in minutes.\nSolution: Add NoSchedule taint, only run control plane components + etcd.\n1 kubectl taint nodes vm-0-15-ubuntu node-role.kubernetes.io/control-plane=:NoSchedule 2G memory running k3s control-plane + etcd + Headscale full stack, CPU and memory usage around 60%, holds up fine.\nPart IV: Troubleshooting Quick Reference Node added but having issues? Check in this order:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 1. Check node status kubectl get nodes -o wide # 2. Check node events kubectl describe node \u0026lt;node-name\u0026gt; # 3. Check Tailscale connectivity tailscale ping \u0026lt;target-node-tailscale-IP\u0026gt; # 4. Check Flannel configuration kubectl describe node \u0026lt;node-name\u0026gt; | grep flannel # 5. Check kubelet logs ssh root@\u0026lt;node-IP\u0026gt; \u0026#34;journalctl -u k3s-agent -n 50\u0026#34; # 6. Test cross-node Pod communication kubectl run test --image=busybox --rm -it -- wget -qO- http://\u0026lt;other-node-PodIP\u0026gt; 90% of issues can be found in the first 4 steps.\nPart V: Summary Compared to the 2025 solution, key changes in this upgrade:\nItem 2025 2026 Networking WireGuard manual config Tailscale (Headscale self-hosted) Control plane Official Tailscale / manual WG Self-hosted Headscale + OIDC Master count 2 4 (HA) Edge nodes 0 4 Total nodes 2 8 New node onboarding Change lots of configs One-line command Cross-cloud communication Flannel wireguard-native Flannel VXLAN over Tailscale Management complexity High Low One-sentence solution summary:\nUse Tailscale (Headscale self-hosted) for underlying networkâ€”All nodes uniformly join, don\u0026rsquo;t rely on any public cloud internal network K3s Masters in same cloud regionâ€”Low etcd latency, stable control plane Edge nodes add freelyâ€”Home machines, overseas VPS, office workstations, as long as Tailscale can reach Gateway nodes use lightweight cloudâ€”Cheap, sufficient bandwidth, fixed public IP The entire cluster has been running for a month, stable as an old dog.\n8/8 nodes all available (previously had 1 Vultr with network issues, already replaced).\nDone.\nEntire article completed in the study at dawn, Happy New Year everyone~\nManual dog head emoji\nRelated links:\nK3s Official Docs: https://docs.k3s.io/ Headscale: https://github.com/juanfont/headscale Authentik: https://goauthentik.io/ Previous article: Best Edge Computing Cluster Solution This article was written with AI assistance.\n","date":"2026-02-12T08:00:00Z","image":"https://liguobao.github.io/zh/p/k3s-tailscale-cluster-2026/logo_hu_5a496e6dfb978d62.png","permalink":"https://liguobao.github.io/p/k3s-tailscale-cluster-2026/","title":"Best Edge Computing Cluster Solution: Tailscale + K3s (2026 Edition)"},{"content":"Preface Who doesn\u0026rsquo;t have a k8s cluster these days? Right.\nIf your cluster hasn\u0026rsquo;t crashed at least once or twice a year, are you even serious?\nAnd who doesn\u0026rsquo;t spin up a machine here and a server there?\nSo I started tinkering with building cross-LAN/private-network connectivity.\nSome time ago I manually set up WireGuard in the cloud and got several nodes running â€” it was basically usable.\nBut manually deploying this stuff is always a pain; if you misconfigure a node, debugging is a nightmare.\nI had also seen Tailscale recently and ran the official demo using their hosted service â€” it worked fine, but the free tier only allows five devices, which isn\u0026rsquo;t suitable for us.\nThen an ops colleague started experimenting with self-hosting Tailscale auth, and once our machines could talk to each other I suddenly understood how to do it.\nSo, let\u0026rsquo;s roll.\nTailscale: Concepts First 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Tailscale æœ¬è´¨æ˜¯ä¸‰å±‚ä¸œè¥¿ï¼š å®¢æˆ·ç«¯ï¼ˆå¿…éœ€ï¼‰ tailscale CLI / GUIï¼ˆmacOS / Linux / Windows / iOS / Androidï¼‰ è´Ÿè´£ï¼šWireGuard åŠ å¯†ã€æ‰“æ´ã€ç›´è¿/ä¸­ç»§ æ§åˆ¶é¢ï¼ˆControl Planeï¼‰ è®¾å¤‡æ³¨å†Œ èº«ä»½è®¤è¯ ä¸‹å‘èŠ‚ç‚¹åˆ—è¡¨ã€ACLã€DERP ä¿¡æ¯ - å®˜æ–¹çš„æ˜¯ Tailscale SaaS - è‡ªå»ºçš„æ˜¯ Headscale ä¸­ç»§é¢ï¼ˆDERPï¼Œå¯é€‰ï¼‰ æ‰“æ´å¤±è´¥æ—¶è½¬å‘æµé‡ å¯ç”¨å®˜æ–¹ DERPï¼Œä¹Ÿå¯ä»¥è‡ªå»º So, first get a server â€” a lightweight cloud VM will do.\nDiscussing the Plan with GPT I talked with GPT to confirm feasibility. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 ç”¨ Headscale ä½œä¸º Tailscale Control Serverï¼Œ å…³é—­ Tailscale å®˜æ–¹ SaaSï¼Œ é‰´æƒæ¥å…¥ä½ è‡ªå·±çš„ OAuth / LDAP / Token / SSOã€‚ ## äºŒã€å…³é”®ç»„ä»¶è¯´æ˜ï¼ˆé‡è¦ï¼‰ ### Tailscale å®˜æ–¹å®¢æˆ·ç«¯ + å®˜æ–¹ SaaS ä½ ä¸ç”¨å®˜æ–¹ SaaS åªä½¿ç”¨å®¢æˆ·ç«¯ï¼ˆtailscale up --login-server=...ï¼‰ ### Headscaleï¼ˆæ ¸å¿ƒï¼‰ Tailscale å®˜æ–¹åè®®çš„ å¼€æº Control Server 100% è‡ªæ‰˜ç®¡ MIT License æ”¯æŒï¼š Namespaces / Users ACL PreAuthKey OIDCï¼ˆå¯æ‰©å±•ï¼‰ ğŸ‘‰ Headscale = ä½ è‡ªå·±çš„ Tailscale Server Got the idea â€” let\u0026rsquo;s get to work.\nVibe Coding DevOps: Getting Started I decided to have GPT generate a Vibe Coding task and let Copilot run it on the server to see how it goes.\nSo I prepared the following prompt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 ä½ æ˜¯ä¸€ä¸ªèµ„æ·± SRE / åŸºç¡€è®¾æ–½å·¥ç¨‹å¸ˆï¼Œè¯·å¸®æˆ‘åœ¨ä¸€å° Linux æœåŠ¡å™¨ä¸Šï¼Œ ä» 0 åˆ° 1 å®Œæ•´éƒ¨ç½²ä¸€ä¸ªã€è‡ªå»º Tailscale æ§åˆ¶é¢ç³»ç»Ÿã€‘ã€‚ ================================================ ã€æœåŠ¡å™¨ä¸åŸºç¡€ä¿¡æ¯ã€‘ ================================================ æœåŠ¡å™¨åœ°å€ï¼š ts.example.cn SSH ç™»å½•æ–¹å¼ï¼š ssh root@ts.example.cn æ“ä½œç³»ç»Ÿï¼š å‡è®¾ä¸º Ubuntu 20.04 / 22.04ï¼ˆä¸»æµ Linux å³å¯ï¼‰ æƒé™ï¼š - root æƒé™ - å¯æ‰§è¡Œä»»æ„ç³»ç»Ÿå‘½ä»¤ åŸŸåçŠ¶æ€ï¼ˆå·²å®Œæˆï¼‰ï¼š - ts.example.cn â†’ æœ¬æœºå…¬ç½‘ IP - auth.example.cn â†’ æœ¬æœºå…¬ç½‘ IP DNS å·²è§£æå®Œæˆï¼Œä¸éœ€è¦ä½ å¤„ç† DNS HTTPS è¦æ±‚ï¼š - ä½¿ç”¨ acme.sh ç”³è¯· Letâ€™s Encrypt è¯ä¹¦ - ä½¿ç”¨ Nginx æ¨¡å¼æˆ– standalone å‡å¯ - éœ€è¦é…ç½®è‡ªåŠ¨ç»­æœŸ - è¯ä¹¦è·¯å¾„è¦æ¸…æ™°ã€å¯ç»´æŠ¤ ================================================ ã€æœ€ç»ˆç›®æ ‡ã€‘ ================================================ åœ¨è¿™å°æœåŠ¡å™¨ä¸Šå®Œæˆä»¥ä¸‹ç³»ç»Ÿéƒ¨ç½²ï¼Œå¹¶ç¡®ä¿é•¿æœŸç¨³å®šè¿è¡Œï¼š 1. Headscale - ä½œä¸ºå”¯ä¸€çš„ Tailscale æ§åˆ¶é¢ï¼ˆæ›¿ä»£å®˜æ–¹ SaaSï¼‰ - å¯¹å¤–æœåŠ¡åœ°å€ï¼šhttps://ts.example.cn 2. è´¦å· / å¯†ç ç™»å½•ç³»ç»Ÿ - ä½¿ç”¨ Authentik - é€šè¿‡ OIDC ä¸ Headscale é›†æˆ - ç™»å½•å…¥å£ï¼šhttps://auth.example.cn 3. PostgreSQL - ä»…ä¾› Authentik ä½¿ç”¨ 4. Tailscale Client - æœ¬æœºä½œä¸ºä¸€ä¸ªæ™®é€š Tailscale èŠ‚ç‚¹åŠ å…¥ç½‘ç»œ 5. HTTPS - ç”±å®¿ä¸»æœº Nginx ç»Ÿä¸€è´Ÿè´£ - ä¸ä½¿ç”¨ Traefik / Caddy 6. å®¹å™¨ç®¡ç† - ä½¿ç”¨ Docker + docker-compose - æ‰€æœ‰æœåŠ¡å¯ä¸€é”®å¯åŠ¨ / åœæ­¢ æœ€ç»ˆæ•ˆæœåº”æ”¯æŒï¼š tailscale up --login-server=https://ts.example.cn ================================================ ã€å¼ºåˆ¶æ¶æ„çº¦æŸï¼ˆå¿…é¡»éµå®ˆï¼‰ã€‘ ================================================ - âŒ ä¸ä½¿ç”¨å®˜æ–¹ Tailscale SaaS - âœ… ä½¿ç”¨ Headscale ä½œä¸ºå”¯ä¸€æ§åˆ¶é¢ - âœ… ä½¿ç”¨ Authentik æä¾›è´¦å· / å¯†ç ç™»å½• - âœ… Headscale + Authentik + PostgreSQL ä½¿ç”¨ docker-compose - âœ… Tailscale client ä½¿ç”¨å®˜æ–¹ tailscale å®¹å™¨ - âœ… HTTPS å®Œå…¨ç”±å®¿ä¸»æœº Nginx æä¾› - âŒ ä¸åœ¨è¯¥æœºå™¨ä¸Šè¿è¡Œ k3s master / etcd - âœ… æ‰€æœ‰é…ç½®å¿…é¡»æ˜¯çœŸå®å¯è¿è¡Œé…ç½®ï¼Œä¸å…è®¸ä¼ªä»£ç  ================================================ ã€åŸŸåä¸ç«¯å£è§„åˆ’ï¼ˆå›ºå®šï¼‰ã€‘ ================================================ å¯¹å¤–åŸŸåï¼š - Headscaleï¼š https://ts.example.cn - é‰´æƒå…¥å£ï¼š https://auth.example.cn å†…éƒ¨ç«¯å£ï¼ˆå¯è‡ªè¡Œé€‰æ‹©ï¼Œä½†éœ€åˆç†ï¼‰ï¼š - Headscaleï¼š8080 - Authentikï¼š9000 - PostgreSQLï¼š5432 ================================================ ã€ä½ éœ€è¦è¾“å‡ºçš„å†…å®¹ï¼ˆä¸€æ­¥éƒ½ä¸èƒ½å°‘ï¼‰ã€‘ ================================================ è¯·ä½ ã€ä¸¥æ ¼æŒ‰é¡ºåºã€‘è¾“å‡ºå¹¶è§£é‡Šä»¥ä¸‹å†…å®¹ï¼š 1. ğŸ§± æœåŠ¡å™¨åˆå§‹åŒ– - æ›´æ–°ç³»ç»Ÿ - å®‰è£… Docker - å®‰è£… docker-composeï¼ˆplugin æˆ– standaloneï¼‰ - å®‰è£… Nginx - å®‰è£… acme.sh 2. ğŸ” HTTPS è¯ä¹¦ç”³è¯· - ä½¿ç”¨ acme.sh ä¸ºä»¥ä¸‹åŸŸåç”³è¯·è¯ä¹¦ï¼š - ts.example.cn - auth.example.cn - ç»™å‡ºå®Œæ•´ã€çœŸå®çš„å‘½ä»¤ - é…ç½®è‡ªåŠ¨ç»­æœŸ - æ˜ç¡®è¯ä¹¦å­˜æ”¾è·¯å¾„ 3. ğŸŒ Nginx é…ç½® - ä¸º ts.example.cn ç”Ÿæˆ server blockï¼ˆåä»£ Headscaleï¼‰ - ä¸º auth.example.cn ç”Ÿæˆ server blockï¼ˆåä»£ Authentikï¼‰ - å¯ç”¨ HTTPS - è®¾ç½®æ­£ç¡®çš„ proxy headersï¼ˆHost / X-Forwarded-Proto ç­‰ï¼‰ 4. ğŸ“ ç›®å½•ç»“æ„è§„åˆ’ - ç»™å‡ºä¸€ä¸ªæ¨èçš„éƒ¨ç½²ç›®å½•ï¼ˆä¾‹å¦‚ /opt/headscale-stackï¼‰ - è¾“å‡ºå®Œæ•´ç›®å½•æ ‘ 5. ğŸ³ Docker Compose - PostgreSQLï¼ˆä»… Authentik ä½¿ç”¨ï¼‰ - Authentikï¼ˆè´¦å· / å¯†ç  + OIDCï¼‰ - Headscale - Tailscale client - åŒ…å«ï¼š - volumes - networks - restart policy 6. âš™ï¸ Headscale é…ç½®æ–‡ä»¶ - server_url è®¾ç½®ä¸ºï¼šhttps://ts.example.cn - IP æ®µä½¿ç”¨ï¼š100.64.0.0/10 - å¯ç”¨ OIDC - OIDC issuer è®¾ç½®ä¸ºï¼š https://auth.example.cn/application/o/headscale/ 7. ğŸ‘¤ Authentik åˆå§‹åŒ–ä¸ OIDC é…ç½® - å¦‚ä½•é¦–æ¬¡åˆå§‹åŒ–ç®¡ç†å‘˜è´¦å· - å¦‚ä½•åˆ›å»º OIDC Application - Client ID åº”è®¾ç½®ä¸ºï¼šheadscale - Redirect URI å¿…é¡»æ˜¯ï¼š https://ts.example.cn/oidc/callback - å¦‚ä½•è·å– Client Secret å¹¶å¡«å› Headscale 8. â–¶ï¸ å¯åŠ¨ä¸éªŒè¯ - docker compose up -d - å¦‚ä½•æ£€æŸ¥å®¹å™¨çŠ¶æ€ - å¦‚ä½•éªŒè¯ï¼š - https://ts.example.cn å¯è®¿é—® - https://auth.example.cn ç™»å½•é¡µæ­£å¸¸ 9. ğŸ§ª å®¢æˆ·ç«¯æ¥å…¥éªŒè¯ - ç»™å‡ºçœŸå®å‘½ä»¤ï¼š tailscale up --login-server=https://ts.example.cn - æè¿°å®Œæ•´ç™»å½•æµç¨‹ï¼š CLI â†’ æµè§ˆå™¨ â†’ è´¦å·å¯†ç  â†’ å›è°ƒ â†’ å…¥ç½‘ 10. âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹ - å“ªäº›ç›®å½•å¿…é¡»å¤‡ä»½ - ä¸ºä»€ä¹ˆ 2C2G èµ„æºè¶³å¤Ÿ - å“ªäº›äº‹æƒ…ç»å¯¹ä¸èƒ½åšï¼ˆä¾‹å¦‚è·‘ k3s master / etcdï¼‰ ================================================ ã€è¾“å‡ºé£æ ¼è¦æ±‚ã€‘ ================================================ - å¿…é¡»æ˜¯ã€Œä¸€æ­¥ä¸€æ­¥å¯æ‰§è¡Œçš„å®æ“æ‰‹å†Œã€ - æ‰€æœ‰å‘½ä»¤ã€é…ç½®éƒ½å¯ä»¥ç›´æ¥å¤åˆ¶æ‰§è¡Œ - ä¸å…è®¸è¯´â€œè‡ªè¡Œé…ç½®â€â€œç•¥â€ - å‡è®¾è¯»è€…æ˜¯æ‡‚ Linux / Docker / ç½‘ç»œçš„å·¥ç¨‹å¸ˆ - ä»¥å·¥ç¨‹å®æˆ˜ä¸ºç¬¬ä¸€ä¼˜å…ˆçº§ ================================================ ã€æœ€ç»ˆéªŒæ”¶æ ‡å‡†ã€‘ ================================================ å½“æˆ‘å®Œå…¨ç…§ä½ ç»™å‡ºçš„æ­¥éª¤æ‰§è¡Œåï¼Œåº”å½“æ»¡è¶³ï¼š - https://ts.example.cn æ­£å¸¸å·¥ä½œï¼ˆHeadscaleï¼‰ - https://auth.example.cn æ­£å¸¸ç™»å½•ï¼ˆè´¦å· / å¯†ç ï¼‰ - æ–°æœºå™¨å¯é€šè¿‡è´¦å·å¯†ç åŠ å…¥ Tailscale ç½‘ç»œ - è¯¥ç½‘ç»œå¯ç¨³å®šæ‰¿è½½ k3s over Tailscale è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸Šè¦æ±‚è¾“å‡ºå®Œæ•´éƒ¨ç½²æ–¹æ¡ˆã€‚ I bought the cloud VM and pointed the two domains at it â€” now let\u0026rsquo;s proceed.\nThe core Docker Compose used in the final result is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 version: \u0026#39;3.8\u0026#39; services: postgresql: image: docker.io/library/postgres:16-alpine container_name: headscale-postgresql restart: unless-stopped healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\u0026#34;] start_period: 20s interval: 30s retries: 5 timeout: 5s volumes: - ./postgres:/var/lib/postgresql/data environment: POSTGRES_PASSWORD: ${PG_PASS:-authentik_password_change_me} POSTGRES_USER: ${PG_USER:-authentik} POSTGRES_DB: ${PG_DB:-authentik} networks: - headscale-net ports: - \u0026#34;127.0.0.1:5432:5432\u0026#34; redis: image: docker.io/library/redis:alpine container_name: headscale-redis command: --save 60 1 --loglevel warning restart: unless-stopped healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;redis-cli ping | grep PONG\u0026#34;] start_period: 20s interval: 30s retries: 5 timeout: 3s volumes: - redis:/data networks: - headscale-net authentik-server: image: ghcr.io/goauthentik/server:latest container_name: authentik-server restart: unless-stopped command: server environment: AUTHENTIK_REDIS__HOST: redis AUTHENTIK_POSTGRESQL__HOST: postgresql AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik} AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik} AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS:-authentik_password_change_me} AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY:-change_this_to_random_string_min_50_chars_abcdefghijklmnopqrstuvwxyz} AUTHENTIK_ERROR_REPORTING__ENABLED: \u0026#34;false\u0026#34; AUTHENTIK_DISABLE_UPDATE_CHECK: \u0026#34;true\u0026#34; AUTHENTIK_DISABLE_STARTUP_ANALYTICS: \u0026#34;true\u0026#34; AUTHENTIK_AVATARS: \u0026#34;initials\u0026#34; volumes: - ./authentik/media:/media - ./authentik/custom-templates:/templates ports: - \u0026#34;127.0.0.1:9000:9000\u0026#34; networks: - headscale-net depends_on: - postgresql - redis authentik-worker: image: ghcr.io/goauthentik/server:latest container_name: authentik-worker restart: unless-stopped command: worker environment: AUTHENTIK_REDIS__HOST: redis AUTHENTIK_POSTGRESQL__HOST: postgresql AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik} AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik} AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS:-authentik_password_change_me} AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY:-change_this_to_random_string_min_50_chars_abcdefghijklmnopqrstuvwxyz} AUTHENTIK_ERROR_REPORTING__ENABLED: \u0026#34;false\u0026#34; AUTHENTIK_DISABLE_UPDATE_CHECK: \u0026#34;true\u0026#34; AUTHENTIK_DISABLE_STARTUP_ANALYTICS: \u0026#34;true\u0026#34; user: root volumes: - /var/run/docker.sock:/var/run/docker.sock - ./authentik/media:/media - ./authentik/certs:/certs - ./authentik/custom-templates:/templates networks: - headscale-net depends_on: - postgresql - redis headscale: image: headscale/headscale:latest container_name: headscale restart: unless-stopped command: serve volumes: - ./headscale/config.yaml:/etc/headscale/config.yaml:ro - ./headscale/data:/var/lib/headscale - ./headscale/run:/var/run/headscale ports: - \u0026#34;127.0.0.1:8080:8080\u0026#34; - \u0026#34;127.0.0.1:9090:9090\u0026#34; networks: - headscale-net networks: headscale-net: driver: bridge volumes: redis: driver: local Nginx config is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 upstream authentik { server 127.0.0.1:9000; } server { listen 80; server_name auth.example.cn; return 301 https://$server_name$request_uri; } server { listen 443 ssl http2; server_name auth.example.cn; ssl_certificate /etc/nginx/ssl/ts.example.cn.fullchain.cer; ssl_certificate_key /etc/nginx/ssl/ts.example.cn.key; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; client_max_body_size 25M; location / { proxy_pass http://authentik; proxy_http_version 1.1; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; add_header Strict-Transport-Security \u0026#34;max-age=15552000; includeSubDomains\u0026#34; always; } } upstream headscale { server 127.0.0.1:8080; } map $http_upgrade $connection_upgrade { default keep-alive; \u0026#39;websocket\u0026#39; upgrade; \u0026#39;\u0026#39; close; } server { listen 80; server_name ts.example.cn; return 301 https://$server_name$request_uri; } server { listen 443 ssl http2; server_name ts.example.cn; ssl_certificate /etc/nginx/ssl/ts.example.cn.fullchain.cer; ssl_certificate_key /etc/nginx/ssl/ts.example.cn.key; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / { proxy_pass http://headscale; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_set_header Host $server_name; proxy_redirect http:// https://; proxy_buffering off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto; add_header Strict-Transport-Security \u0026#34;max-age=15552000; includeSubDomains\u0026#34; always; } } Final Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 *** System restart required *** âœ ~ tailscale status 100.64.0.3 haru https://auth.example.cn/application/o/headscale dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - 100.64.0.9 hc1 https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - 100.64.0.4 lgb-amd-3700 https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux idle, tx 1052 rx 860 100.64.0.2 lgb-macbookair-m4 https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad macOS offline 100.64.0.8 rainyun https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - 100.64.0.10 ts-example-cn https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - 100.64.0.11 us-vultr https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux idle, tx 1012 rx 860 100.64.0.6 vm-0-8-ubuntu-new https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - 100.64.0.5 vm-16-12-ubuntu https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - 100.64.0.7 vm-28-17-ubuntu https://auth.example.cn/application/o/headscale/dcaba24e364c6ea7c3dd676f3d445f85843028f1f12880dcba46acabf82509ad linux - âœ ~ âœ ~ ping 100.64.0.3 PING 100.64.0.3 (100.64.0.3) 56(84) bytes of data. 64 bytes from 100.64.0.3: icmp_seq=1 ttl=64 time=0.049 ms 64 bytes from 100.64.0.3: icmp_seq=2 ttl=64 time=0.034 ms 64 bytes from 100.64.0.3: icmp_seq=3 ttl=64 time=0.043 ms ^C --- 100.64.0.3 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2026ms rtt min/avg/max/mdev = 0.034/0.042/0.049/0.006 ms âœ ~ ping 100.64.0.9 PING 100.64.0.9 (100.64.0.9) 56(84) bytes of data. 64 bytes from 100.64.0.9: icmp_seq=1 ttl=64 time=737 ms 64 bytes from 100.64.0.9: icmp_seq=2 ttl=64 time=12.5 ms 64 bytes from 100.64.0.9: icmp_seq=3 ttl=64 time=12.0 ms 64 bytes from 100.64.0.9: icmp_seq=4 ttl=64 time=12.1 ms 64 bytes from 100.64.0.9: icmp_seq=5 ttl=64 time=12.9 ms 64 bytes from 100.64.0.9: icmp_seq=6 ttl=64 time=12.3 ms 64 bytes from 100.64.0.9: icmp_seq=7 ttl=64 time=12.1 ms 64 bytes from 100.64.0.9: icmp_seq=8 ttl=64 time=12.4 ms ^C --- 100.64.0.9 ping statistics --- 8 packets transmitted, 8 received, 0% packet loss, time 7008ms rtt min/avg/max/mdev = 12.015/102.875/736.725/239.572 ms âœ ~ ping 100.64.0.5 PING 100.64.0.5 (100.64.0.5) 56(84) bytes of data. 64 bytes from 100.64.0.5: icmp_seq=1 ttl=64 time=96.3 ms 64 bytes from 100.64.0.5: icmp_seq=2 ttl=64 time=5.57 ms 64 bytes from 100.64.0.5: icmp_seq=3 ttl=64 time=4.93 ms 64 bytes from 100.64.0.5: icmp_seq=4 ttl=64 time=5.12 ms 64 bytes from 100.64.0.5: icmp_seq=5 ttl=64 time=5.40 ms ^C --- 100.64.0.5 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4006ms rtt min/avg/max/mdev = 4.932/23.468/96.318/36.425 ms âœ ~ Practical Notes Make sure to run Vibe Coding tasks on the target server; otherwise many steps might run locally instead of on the server. Docker images can be slow to pull in China â€” consider exporting them locally and importing them to the server. Authentik user creation may not complete automatically; have Vibe Coding generate the necessary scripts to perform the remaining steps. There aren\u0026rsquo;t many other pitfalls. Have fun!\n","date":"2026-02-09T08:00:00Z","image":"https://liguobao.github.io/zh/p/vibe-coding-deploy-tailscale/logo_hu_bffb7fc0fd4bf5c.png","permalink":"https://liguobao.github.io/p/vibe-coding-deploy-tailscale/","title":"Who Doesn't Want Their Own Tailscale Private Network?"},{"content":"Introduction If you want to debug the unitree-g1-edu robot, you\u0026rsquo;ve probably read their official documentation.\nThe quick development guide is here:\nG1_developer/Quick Development\nThe environment requirements are straightforwardâ€”based on Ubuntu 20.04 or 22.04. Install the required dependencies and most steps are easy to perform.\nHowever, if you want to actually debug the G1 robot, the official approach is roughly as follows.\nDemo video.\nPlug an Ethernet cable from the G1 robot\u0026rsquo;s neck port into your Ubuntu host\nSet a static IP for the Ubuntu host and communicate with the robot over the LAN interface\nRoughly like this:\n1 2 3 4 It is recommended that new users connect their computer to the G1 switch using an Ethernet cable and adapter, and set the NIC used for robot communication to the 192.168.123.X subnet, recommended using 192.168.123.99. There\u0026rsquo;s nothing wrong with that approach, but dragging a cable around is inconvenient.\nIs there another way?\nAfter thinking about it, you can use a Raspberry Pi as the master controller connected to the G1 network, while also connecting the Pi to the local LAN â€” that lets you control the robot remotely.\nI also noticed there are three power output ports next to the G1-EDU network port: 12V, 24V, and 48V, which are enough to power a Raspberry Pi (Pi needs 5Vâ€”use a buck converter). With that power, you can also supply other devices.\nFor example: a robotic gripperâ€”pretty interesting.\nRaspberry Pi Hardware Setup A buck converter (EVEPS DC step-down) + power cable,\nEVEPS DC step-down module\nPower cable\nAll together it\u0026rsquo;s about thirty RMB; you can buy them on Taobao.\nUse 3M tape to fix the Raspberry Pi to the robot\u0026rsquo;s back, connect the power cable, plug the Ethernet cable from the back of the robot into the Raspberry Pi, and finally put a cover on the G1 to hide everything.\nPython Action Debugging Install their unitree_sdk2py locally, then you can try debugging the G1.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 import time import sys from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelFactoryInitialize from unitree_sdk2py.g1.arm.g1_arm_action_client import G1ArmActionClient from unitree_sdk2py.g1.arm.g1_arm_action_client import action_map from dataclasses import dataclass @dataclass class TestOption: name: str id: int option_list = [ TestOption(name=\u0026#34;release arm\u0026#34;, id=0), TestOption(name=\u0026#34;shake hand\u0026#34;, id=1), TestOption(name=\u0026#34;high five\u0026#34;, id=2), TestOption(name=\u0026#34;hug\u0026#34;, id=3), TestOption(name=\u0026#34;high wave\u0026#34;, id=4), TestOption(name=\u0026#34;clap\u0026#34;, id=5), TestOption(name=\u0026#34;face wave\u0026#34;, id=6), TestOption(name=\u0026#34;left kiss\u0026#34;, id=7), TestOption(name=\u0026#34;heart\u0026#34;, id=8), TestOption(name=\u0026#34;right heart\u0026#34;, id=9), TestOption(name=\u0026#34;hands up\u0026#34;, id=10), TestOption(name=\u0026#34;x-ray\u0026#34;, id=11), TestOption(name=\u0026#34;right hand up\u0026#34;, id=12), TestOption(name=\u0026#34;reject\u0026#34;, id=13), TestOption(name=\u0026#34;right kiss\u0026#34;, id=14), TestOption(name=\u0026#34;two-hand kiss\u0026#34;, id=15), ] class UserInterface: def __init__(self): self.test_option_ = None def convert_to_int(self, input_str): try: return int(input_str) except ValueError: return None def terminal_handle(self): input_str = input(\u0026#34;Enter id or name: \\n\u0026#34;) if input_str == \u0026#34;list\u0026#34;: self.test_option_.name = None self.test_option_.id = None for option in option_list: print(f\u0026#34;{option.name}, id: {option.id}\u0026#34;) return for option in option_list: if input_str == option.name or self.convert_to_int(input_str) == option.id: self.test_option_.name = option.name self.test_option_.id = option.id print(f\u0026#34;Test: {self.test_option_.name}, test_id: {self.test_option_.id}\u0026#34;) return print(\u0026#34;No matching test option found.\u0026#34;) def shake_hand(network_interface): ChannelFactoryInitialize(0, network_interface) armAction_client = G1ArmActionClient() armAction_client.SetTimeout(10.0) armAction_client.Init() time.sleep(0.5) armAction_client.ExecuteAction(action_map.get(\u0026#34;shake hand\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) print(\u0026#34;Shaked hand and released arm.\u0026#34;) def release_arm(network_interface): ChannelFactoryInitialize(0, network_interface) armAction_client = G1ArmActionClient() armAction_client.SetTimeout(10.0) armAction_client.Init() time.sleep(0.5) result = armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) print(f\u0026#34;Released arm with result: {result}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) \u0026lt; 2: print(f\u0026#34;Usage: python3 {sys.argv[0]} networkInterface\u0026#34;) sys.exit(-1) print(\u0026#34;WARNING: Please ensure there are no obstacles around the robot while running this example.\u0026#34;) input(\u0026#34;Press Enter to continue...\u0026#34;) ChannelFactoryInitialize(0, sys.argv[1]) test_option = TestOption(name=None, id=None) user_interface = UserInterface() user_interface.test_option_ = test_option armAction_client = G1ArmActionClient() armAction_client.SetTimeout(10.0) armAction_client.Init() # actionList = armAction_client.GetActionList() # print(\u0026#34;actionList\\n\u0026#34;,actionList) print(\u0026#34;Input \\\u0026#34;list\\\u0026#34; to list all test option ...\u0026#34;) while True: user_interface.terminal_handle() print(f\u0026#34;Updated Test Option: Name = {test_option.name}, ID = {test_option.id}\u0026#34;) if test_option.id == 0: armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 1: armAction_client.ExecuteAction(action_map.get(\u0026#34;shake hand\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 2: armAction_client.ExecuteAction(action_map.get(\u0026#34;high five\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 3: armAction_client.ExecuteAction(action_map.get(\u0026#34;hug\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 4: armAction_client.ExecuteAction(action_map.get(\u0026#34;high wave\u0026#34;)) elif test_option.id == 5: armAction_client.ExecuteAction(action_map.get(\u0026#34;clap\u0026#34;)) elif test_option.id == 6: armAction_client.ExecuteAction(action_map.get(\u0026#34;face wave\u0026#34;)) elif test_option.id == 7: armAction_client.ExecuteAction(action_map.get(\u0026#34;left kiss\u0026#34;)) elif test_option.id == 8: armAction_client.ExecuteAction(action_map.get(\u0026#34;heart\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 9: armAction_client.ExecuteAction(action_map.get(\u0026#34;right heart\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 10: armAction_client.ExecuteAction(action_map.get(\u0026#34;hands up\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 11: armAction_client.ExecuteAction(action_map.get(\u0026#34;x-ray\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 12: armAction_client.ExecuteAction(action_map.get(\u0026#34;right hand up\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 13: armAction_client.ExecuteAction(action_map.get(\u0026#34;reject\u0026#34;)) time.sleep(2) armAction_client.ExecuteAction(action_map.get(\u0026#34;release arm\u0026#34;)) elif test_option.id == 14: armAction_client.ExecuteAction(action_map.get(\u0026#34;right kiss\u0026#34;)) elif test_option.id == 15: armAction_client.ExecuteAction(action_map.get(\u0026#34;two-hand kiss\u0026#34;)) time.sleep(1) ","date":"2026-01-24T08:00:00Z","image":"https://liguobao.github.io/zh/p/unitree-g1-develop-on-raspberry-pi/logo_hu_a3946870fe30519f.png","permalink":"https://liguobao.github.io/p/unitree-g1-develop-on-raspberry-pi/","title":"Unitree G1 Portable Development Guide: Raspberry Pi-based Development Environment"},{"content":"When performing tasks such as web crawling or data collection, using an IP proxy pool can effectively hide your real IP and avoid being banned by target websites.\nThis article will introduce how to build a simple IP proxy pool.\n1. Preparation Before starting, ensure you have a dial-up VPS (Virtual Private Server).\nThe price of dial-up VPS is basically from dozens to hundreds of yuan, and the optional systems are Windows or Linux.\nVPS service providers basically help you configure the dial-up environment before delivery, and you can simply use the pppoe command to redial.\n2. Building the Proxy Server Generally, we won\u0026rsquo;t directly expose the proxy service on the VPS because it basically doesn\u0026rsquo;t give you a public IP.\nTherefore, we need a proxy server to relay traffic.\nAt the same time, we need a set of \u0026ldquo;proxy server software\u0026rdquo; to connect to the dial-up VPS internally and provide proxy services externally.\nHere we use majora3 proxy server to build our proxy server.\nMajora is a complete proxy IP construction cluster solution for the proxy IP pool supply chain system.\nIf you have a large number of network devices that can access the internet (VPS servers, router devices, mobile phones, etc.),\nThen you can use these network devices to conveniently build your proxy IP pool.\nMajora can meet the following application scenarios:\nIf you are a proxy IP supplier with a large number of VPS nodes, you can quickly build your proxy IP pool through Majora You can flash the Majora terminal program into router and other network devices, so you can build your home IP resource pool or ADSL IP resource pool through Majora You can install Majora\u0026rsquo;s apk, or use Majora\u0026rsquo;s SDK integrated into mobile programs, so you can quickly build your mobile outlet IP resource pool through Majora (if your phone is rooted, you can also have the function of timed redial) Majora Features Easy to use: All terminal nodes are one-click installation to access the proxy IP cluster, no need for complex parameter configuration on the server Multi-supplier and purchaser concept: You can let multiple suppliers holding network devices access the Majora system at the same time, and also let multiple IP users access Majora\u0026rsquo;s IP resources. Majora records the detailed billing statements of each supplier and consumer, and you can settle fees with suppliers and purchasers based on the statements Complete protocol: The system fully supports http/https/socks5 proxy protocols, and all proxy ports automatically identify the proxy protocol. No need for users to manually select ports IP quality reliability: Majora exposes port ranges and provides services in the form of tunnel proxies. And provides memory-level IP resource failure routing function. Except for very few cases (links interrupted in the request), Majora guarantees that each proxy port on the supply side can provide equivalent stable proxy services. There will never be situations where the proxy cannot be connected or connected but cannot be used Multi-network device support: Supports servers (VPS), mobile phones, routers, PCs (ordinary Windows or Mac computers) High concurrency and high bandwidth: NIO framework naturally supports very high throughput, and Majora\u0026rsquo;s predecessor system (Rogue) has undergone single-node 100M bandwidth pressure majora3 server installation Docker installation method 1 2 3 4 5 6 7 8 9 # Install docker yum install -y docker # Download image: docker pull registry.cn-beijing.aliyuncs.com/iinti/majora3:all-in-one # Start majora server docker run -d --network host -v ~/majora3-mysql-data:/var/lib/mysql registry.cn-beijing.aliyuncs.com/iinti/majora3:all-in-one ## The all-in-one image contains majora3 server and mysql database, data is saved in the host\u0026#39;s ~/majora3-mysql-data directory Test if deployment is successful 1 curl http://127.0.0.1:6879/ Visit the website: http://your IP:Web service port such as http://127.0.0.1:6879/,\nFor the first time opening the website, please register an account, the first registered account will become the administrator,\nAfter registration, remember to set the \u0026ldquo;proxy authentication account password\u0026rdquo;, such as admin/admin\nIf you can successfully open the website and complete account registration, it means the deployment is complete\nProduction environment deployment Production environment data generally uses cloud RDS and other databases to avoid single point of failure.\nMajora is developed using Java and uses MySQL as the database,\nIf you want to manually deploy Majora, you need to manually complete the installation of Java and MySQL, and then perform related configuration\nInstallation preparation Download installation package Install jdk17 Install MySQL, or purchase MySQL service Configuration and initialization Unzip the installation package Database configuration initialization configuration in: assets/ddl.sql, please build the table initialization according to this SQL file\nConf folder related configuration\nThe project uses springboot, and the project optional configuration is in conf/application.properties, please configure your database connection information here (the database is the MySQL installation and database configuration you completed in the previous step) conf/static/* is the front-end resources, if you want to replace the front-end web skin, you can replace the content here, majora front-end is open source, supports secondary development conf/static/majora-doc/* is the document resources, if you want to modify the document content, you can edit here Run Execute bin/startup.sh (if it\u0026rsquo;s Windows, execute xxx.bat)\nObserve if the log is normal\n3. majora client access VPS After logging into the majora admin backend, select \u0026ldquo;Terminal Download\u0026rdquo; in the right menu, and download the corresponding version terminal.\nCurrently supported versions include:\nWindows version (windows_amd64_v3) Linux version (linux_amd64_v3) Android version (android_all) Router version (linux_mipsle_softfloat) Choose to download according to different chip architecture versions.\nBelow we take the Linux version as an example.\nDownload majora3_linux_amd64_v3.tar.gz to local, unzip.\nModify the tunnel_addr address in the majora.toml configuration file to your majora3 server address,\nModify client_id to your terminal unique ID (can be seen in the \u0026ldquo;Terminal Management\u0026rdquo; in the majora admin backend)\n1 2 3 4 5 6 7 8 9 10 11 12 13 group = \u0026#34;majora3\u0026#34; extra = \u0026#34;zone_de\u0026#34; client_id = \u0026#34;helloworld\u0026#34; tunnel_addr = \u0026#34;majora3.iinti.cn:6879\u0026#34; #tunnel_addr = \u0026#34;majora3.iinti.cn:6879\u0026#34; # trace/debug/info/error/fatal/disabled log_level = \u0026#34;info\u0026#34; [redial] # Execute script shell bash/ash/cmd command = \u0026#34;bash\u0026#34; # Script full path exec_path = \u0026#34;/root/majora3/ppp.sh\u0026#34; Repackage into majora3_linux_amd64_v3.tar.gz, then upload to your VPS.\nLog in to the VPS directly, unzip and execute systemd.sh, automatically install as systemd service\n1 2 3 tar zxvf majora3_linux_amd64_v3.tar.gz cd majora3_linux_amd64_v3 ./systemd.sh If there is no problem, the majora client will connect to the majora server, and you can see your terminal in \u0026ldquo;Terminal Management\u0026rdquo;.\nAt the same time, the program will automatically take over the device pppoe dial-up, timed redial.\n4. Using Proxy IP After completing the above steps, your proxy IP pool is built.\nAt this time, you need to create an IP pool in majora admin and add VPS devices to the pool.\nIP Pool List\n-\u0026gt; Create IP Pool\n-\u0026gt; Select \u0026ldquo;IP Pool Type\u0026rdquo; + Set Port\n-\u0026gt; Device List\n-\u0026gt; Add Device\n-\u0026gt; Save\nAfter creation, you can see the IP pool you created in the \u0026ldquo;IP Pool List\u0026rdquo;.\nThen use the set port for testing.\n1 2 3 curl -x majora:majora3@majora3.iinti.cn:6879 https://www.baidu.com/ curl -x majora:majora3@majora3.iinti.cn:6879 https://myip.ipip.net ","date":"2025-09-17T08:00:00Z","image":"https://liguobao.github.io/zh/p/how-to-build-your-ip-proxy/logo_hu_dd28101035aa7aaf.png","permalink":"https://liguobao.github.io/p/how-to-build-your-ip-proxy-en/","title":"How to Build Your IP Proxy Pool 2025 Edition"},{"content":"Recently, while browsing \u0026ldquo;What Worth Buying\u0026rdquo;, I saw that the\n[Wandering Earth 2] collaboration internet host key power bank was on sale,\nhttps://post.smzdm.com/p/aev3zw74/\nOriginally priced at 200-300 yuan, faulty ones are about 40-50 yuan, working ones about 80-90 yuan.\nSpending 200 yuan on a power bank that can\u0026rsquo;t fly is a big smart move,\nSpending 90 yuan on a collaboration figurine, what else do you want?\nSo, I ordered one. When it arrived, it looked like this:\nA digital tube display screen, with three buttons below to control the display functions.\nYou can even see the charging power and battery level~\nIt works, it\u0026rsquo;s heavy, not suitable for going out.\nIt\u0026rsquo;s really just a figurine toy.\nAfter two days, I thought again, why not dismantle this thing,\nConnect the display screen to my own device,\nWrite a program on the device that supports Authenticator 2FA,\nMake it a real \u0026ldquo;internet host key\u0026rdquo; device?\nLet\u0026rsquo;s do it.\nFirst, choose a hardware board, needs to be small, and can drive the display screen.\nI looked at ESP32, it\u0026rsquo;s suitable, enough IO ports,\nCan drive small displays, can connect to WIFI,\nPrice is cheap~\nChoose a hardware board -\u0026gt; ESP32C3 Went on Taobao and looked around, finally found an ESP32C3 board.\nSupports USB-C, plug into computer and it\u0026rsquo;s done.\nNo drivers needed, even works on macOS directly~\nDevelopment framework -\u0026gt; micropython After looking at development frameworks, I found micropython is the simplest.\nboot.py is built-in to the framework, your own logic goes in main.py,\nIO ports and network are built-in, most needed drivers can be found as third-party libraries.\nHowever, you need to flash the firmware yourself -\u0026gt; Getting Started with MicroPython on ESP32.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Install esptool pip install esptool # Erase device ## macOS or linux esptool.py --port /dev/ttyUSB0 erase_flash ## windows python esptool --port COM6 erase_flash # Press boot key, execute flash firmware # Download link: https://micropython.org/download/ESP32_GENERIC_C3/ # For other versions, see for yourself esptool.py --chip esp32 --port /dev/ttyUSB0 --baud 460800 write_flash -z 0x0000 micropython.bin ## Windows version python esptool --port COM6 --baud 460800 write_flash -z 0x0000 micropython.bin ## After flashing successfully, power off and restart After flashing successfully, power off and restart. Verify micropython environment\nThe simplest way is VS Code Pymakr plugin: Pymakr - Visual Studio Marketplace\nAfter installing, in the left sidebar, click the \u0026quot;\n\u0026quot; icon, select \u0026ldquo;List Device\u0026rdquo;.\nIf ListDevice doesn\u0026rsquo;t appear or reports an error,\nYou may need to install Nodejs runtime. (Encountered on Windows: choco install nodejs done.)\nThen click \u0026ldquo;Device COM\u0026rdquo;, select \u0026ldquo;Connect\u0026rdquo; icon, then select \u0026ldquo;Terminal\u0026rdquo; icon.\nAfter selecting the terminal icon, you can see this command line interface, indicating micropython environment is good.\nIf you see errors, probably the firmware above wasn\u0026rsquo;t flashed correctly.\nTry flashing the firmware again.\nPS: May need to switch \u0026ndash;baud 460800, specifically see micropython docs or ask AI\nesptool.py \u0026ndash;chip esp32 \u0026ndash;port /dev/ttyUSB0 \u0026ndash;baud 460800 write_flash -z 0x0000 micropython.bin\nChoose a screen -\u0026gt; 2.25 inch long strip screen Choosing a screen was a big problem. Searched around, hard to find this specification screen.\nThis thing\u0026rsquo;s display area: 1.8CM high, 15CM wide, long strip.\nSearched Taobao, couldn\u0026rsquo;t find similar specs.\nFinally settled for: 2.25 inch TFT LCD screen.\nSupports ST7789 (micropython has st7789.py driver).\nThe display\u0026rsquo;s 8P interface is respectively: GND, VCC, SCL, SDA, RST, DC, CS, BL.\nFacing the ESP32 pins, just connect them one by one on the right.\nESP32 pins are: GND, 3.3V, GPIO02, GPIO03, GPIO02, GPIO10, GPIO06, GPIO07\nCorresponding code:\nYour wiring: SCL, SDA, RST, DC, CS -\u0026gt; GPI002, GPI003, GPI010, GPI006, GPI007 SCK_PIN = 2 # SCL pin (clock) -\u0026gt; GPI002 MOSI_PIN = 3 # SDA pin (data) -\u0026gt; GPI003\nRST_PIN = 10 # RST pin (reset) -\u0026gt; GPI010 DC_PIN = 6 # DC pin (data/command) -\u0026gt; GPI006 CS_PIN = 7 # CS pin (chip select) -\u0026gt; GPI007\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- Finally, the screen\u0026rsquo;s BE pin connects to another GND. (I didn\u0026rsquo;t figure this out either, tested it out)\nThen the screen lights up.\n\u0026ldquo;Showtime\u0026rdquo;: Project code -rw-r\u0026ndash;r\u0026ndash; 1 liguobao staff 6.3K 7 30 11:05 http_server.py -rw-r\u0026ndash;r\u0026ndash; 1 liguobao staff 4.3K 7 30 11:05 main.py -rw-r\u0026ndash;r\u0026ndash; 1 liguobao staff 265B 7 20 13:53 pymakr.conf -rw-r\u0026ndash;r\u0026ndash; 1 liguobao staff 1.6K 7 30 11:05 show_text.py -rw-r\u0026ndash;r\u0026ndash;@ 1 liguobao staff 30K 7 22 15:19 st7789.py -rw-r\u0026ndash;r\u0026ndash;@ 1 liguobao staff 3.6K 7 22 15:19 vga1_8x8.py\nst7789 and vga1 come from: https://github.com/russhughes/st7789py_mpy/\nConnect WIFI + Implement HTTP service http_server.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 import network import time import socket #from show_text import display_text_on_tft # --- WiFi Network Configuration List --- WIFI_NETWORKS = [ {\u0026#34;ssid\u0026#34;: \u0026#34;xiaomi_505\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;}, {\u0026#34;ssid\u0026#34;: \u0026#34;miaowuwu_505\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;} ] def connect_wifi(ssid, password): \u0026#34;\u0026#34;\u0026#34;Connect to the specified WiFi network\u0026#34;\u0026#34;\u0026#34; wlan = network.WLAN(network.STA_IF) wlan.active(True) if not wlan.isconnected(): print(f\u0026#34;Connecting to WiFi: {ssid}\u0026#34;) wlan.connect(ssid, password) for _ in range(15): # Wait up to 15 seconds if wlan.isconnected(): ip_address = wlan.ifconfig()[0] print(f\u0026#34;Connection successful, IP address: {ip_address}\u0026#34;) return ip_address time.sleep(1) print(f\u0026#34;WiFi connection failed: {ssid}\u0026#34;) return None else: ip_address = wlan.ifconfig()[0] print(f\u0026#34;WiFi already connected, IP address: {ip_address}\u0026#34;) return ip_address def connect_to_available_wifi(): \u0026#34;\u0026#34;\u0026#34;Try to connect to available WiFi networks\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Starting to scan and connect to available WiFi networks...\u0026#34;) wlan = network.WLAN(network.STA_IF) wlan.active(True) # If already connected, return IP address directly if wlan.isconnected(): ip_address = wlan.ifconfig()[0] print(f\u0026#34;WiFi already connected, IP address: {ip_address}\u0026#34;) return ip_address # Scan available WiFi networks print(\u0026#34;Scanning WiFi networks...\u0026#34;) networks = wlan.scan() available_ssids = [net[0].decode(\u0026#39;utf-8\u0026#39;) for net in networks] print(f\u0026#34;Scanned networks: {available_ssids}\u0026#34;) # Try to connect to configured WiFi networks for wifi_config in WIFI_NETWORKS: ssid = wifi_config[\u0026#34;ssid\u0026#34;] password = wifi_config[\u0026#34;password\u0026#34;] if ssid in available_ssids: print(f\u0026#34;Found configured network: {ssid}, trying to connect...\u0026#34;) ip_address = connect_wifi(ssid, password) if ip_address: return ip_address else: print(f\u0026#34;Network not found: {ssid}\u0026#34;) print(\u0026#34;Unable to connect to any configured WiFi network\u0026#34;) return None def url_decode(text): \u0026#34;\u0026#34;\u0026#34;Simple URL decoding\u0026#34;\u0026#34;\u0026#34; text = text.replace(\u0026#39;+\u0026#39;, \u0026#39; \u0026#39;) text = text.replace(\u0026#39;%20\u0026#39;, \u0026#39; \u0026#39;) text = text.replace(\u0026#39;%21\u0026#39;, \u0026#39;!\u0026#39;) text = text.replace(\u0026#39;%22\u0026#39;, \u0026#39;\u0026#34;\u0026#39;) text = text.replace(\u0026#39;%23\u0026#39;, \u0026#39;#\u0026#39;) text = text.replace(\u0026#39;%24\u0026#39;, \u0026#39;$\u0026#39;) text = text.replace(\u0026#39;%25\u0026#39;, \u0026#39;%\u0026#39;) text = text.replace(\u0026#39;%26\u0026#39;, \u0026#39;\u0026amp;\u0026#39;) text = text.replace(\u0026#39;%27\u0026#39;, \u0026#34;\u0026#39;\u0026#34;) text = text.replace(\u0026#39;%28\u0026#39;, \u0026#39;(\u0026#39;) text = text.replace(\u0026#39;%29\u0026#39;, \u0026#39;)\u0026#39;) text = text.replace(\u0026#39;%2A\u0026#39;, \u0026#39;*\u0026#39;) text = text.replace(\u0026#39;%2B\u0026#39;, \u0026#39;+\u0026#39;) text = text.replace(\u0026#39;%2C\u0026#39;, \u0026#39;,\u0026#39;) text = text.replace(\u0026#39;%2D\u0026#39;, \u0026#39;-\u0026#39;) text = text.replace(\u0026#39;%2E\u0026#39;, \u0026#39;.\u0026#39;) text = text.replace(\u0026#39;%2F\u0026#39;, \u0026#39;/\u0026#39;) return text def handle_request(tft, conn): \u0026#34;\u0026#34;\u0026#34;Handle HTTP request\u0026#34;\u0026#34;\u0026#34; try: request = conn.recv(1024).decode(\u0026#39;utf-8\u0026#39;) print(f\u0026#34;Received request:\\n{request}\u0026#34;) # Parse request line lines = request.split(\u0026#39;\\n\u0026#39;) if lines: request_line = lines[0] parts = request_line.split(\u0026#39; \u0026#39;) if len(parts) \u0026gt;= 2: method = parts[0] path = parts[1] print(f\u0026#34;Method: {method}, Path: {path}\u0026#34;) if method == \u0026#39;GET\u0026#39;: # Parse query parameters if \u0026#39;?\u0026#39; in path: path_part, query_part = path.split(\u0026#39;?\u0026#39;, 1) params = {} for param in query_part.split(\u0026#39;\u0026amp;\u0026#39;): if \u0026#39;=\u0026#39; in param: key, value = param.split(\u0026#39;=\u0026#39;, 1) params[key] = url_decode(value) # Get text parameter received_text = params.get(\u0026#39;text\u0026#39;, None) else: received_text = None if not received_text: response = \u0026#34;\u0026#34;\u0026#34;HTTP/1.1 400 Bad Request\\r Content-Type: text/plain\\r Connection: close\\r \\r Bad Request: \u0026#39;text\u0026#39; parameter is required\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Missing \u0026#39;text\u0026#39; parameter in request\u0026#34;) conn.send(response.encode(\u0026#39;utf-8\u0026#39;)) conn.close() return print(f\u0026#34;Text extracted from GET request: \u0026#39;{received_text}\u0026#39;\u0026#34;) show_text = f\u0026#34;Received Text:\\n{received_text}\u0026#34; # Display code # display_text_on_tft(tft, show_text) # Send success response response = \u0026#34;\u0026#34;\u0026#34;HTTP/1.1 200 OK\\r Content-Type: text/plain\\r Connection: close\\r \\r Text received and displayed!\u0026#34;\u0026#34;\u0026#34; else: # Method not allowed response = \u0026#34;\u0026#34;\u0026#34;HTTP/1.1 405 Method Not Allowed\\r Content-Type: text/plain\\r Connection: close\\r \\r Method Not Allowed. Only GET is supported.\u0026#34;\u0026#34;\u0026#34; else: response = \u0026#34;\u0026#34;\u0026#34;HTTP/1.1 400 Bad Request\\r Content-Type: text/plain\\r Connection: close\\r \\r Bad Request\u0026#34;\u0026#34;\u0026#34; else: response = \u0026#34;\u0026#34;\u0026#34;HTTP/1.1 400 Bad Request\\r Content-Type: text/plain\\r Connection: close\\r \\r Bad Request\u0026#34;\u0026#34;\u0026#34; conn.send(response.encode(\u0026#39;utf-8\u0026#39;)) except Exception as e: print(f\u0026#34;Error handling request: {e}\u0026#34;) try: error_response = \u0026#34;\u0026#34;\u0026#34;HTTP/1.1 500 Internal Server Error\\r Content-Type: text/plain\\r Connection: close\\r \\r Internal Server Error\u0026#34;\u0026#34;\u0026#34; conn.send(error_response.encode(\u0026#39;utf-8\u0026#39;)) except: pass finally: conn.close() def start_http_server(tft, ip_address, port=80): \u0026#34;\u0026#34;\u0026#34;Start HTTP server\u0026#34;\u0026#34;\u0026#34; addr = socket.getaddrinfo(ip_address, port)[0][-1] s = socket.socket() s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) s.bind(addr) s.listen(1) print(f\u0026#39;HTTP server started, listening address: http://{ip_address}:{port}\u0026#39;) display_text_on_tft(tft, f\u0026#34;HTTP Server\\nListening on {ip_address}:{port}\u0026#34;) while True: try: conn, addr = s.accept() print(f\u0026#39;Client connection from: {addr}\u0026#39;) handle_request(tft, conn) except KeyboardInterrupt: print(\u0026#34;Server stopped\u0026#34;) break except Exception as e: print(f\u0026#34;Server error: {e}\u0026#34;) s.close() Main function -\u0026gt; Screen initialization + Adapt height and width main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 import network import time import socket from machine import Pin, SPI import vga1_8x8 as font import st7789 from show_text import display_text_on_tft from http_server import connect_to_available_wifi, handle_request, start_http_server # -------------------------- # --- ST7789 Display Pin Definitions --- # Your wiring: SCL, SDA, RST, DC, CS -\u0026gt; GPI002, GPI003, GPI010, GPI006, GPI007 SCK_PIN = 2 # SCL pin (clock) -\u0026gt; GPI002 MOSI_PIN = 3 # SDA pin (data) -\u0026gt; GPI003 RST_PIN = 10 # RST pin (reset) -\u0026gt; GPI010 DC_PIN = 6 # DC pin (data/command) -\u0026gt; GPI006 CS_PIN = 7 # CS pin (chip select) -\u0026gt; GPI007 # ------------- # Initialize TFT object spi = SPI(1, baudrate=10000000, polarity=0, phase=0, sck=Pin(SCK_PIN), mosi=Pin(MOSI_PIN)) dc = Pin(DC_PIN, Pin.OUT) rst = Pin(RST_PIN, Pin.OUT) cs = Pin(CS_PIN, Pin.OUT) # Ensure CS pin initial state cs.value(1) time.sleep(0.01) # Hardware reset sequence rst.value(1) time.sleep(0.01) rst.value(0) time.sleep(0.01) rst.value(1) time.sleep(0.12) # Wait for reset to complete def test_display_basic(): \u0026#34;\u0026#34;\u0026#34;Basic display test function\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Starting basic display test...\u0026#34;) # Manually send basic commands for testing def send_command(cmd, data=None): cs.value(0) dc.value(0) # Command mode spi.write(cmd) if data: dc.value(1) # Data mode spi.write(data) cs.value(1) time.sleep(0.01) print(\u0026#34;Sending basic initialization commands...\u0026#34;) send_command(b\u0026#39;\\x01\u0026#39;) # Software reset time.sleep(0.15) send_command(b\u0026#39;\\x11\u0026#39;) # Exit sleep time.sleep(0.12) send_command(b\u0026#39;\\x29\u0026#39;) # Display on time.sleep(0.1) print(\u0026#34;Basic test completed\u0026#34;) def init_st7789_display(st_width, st_height, rotation=1): \u0026#34;\u0026#34;\u0026#34;Initialize ST7789 display\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Initializing ST7789 display: {st_width}x{st_height}, rotation: {rotation}\u0026#34;) tft = st7789.ST7789(spi, st_width, st_height, reset=rst, cs=cs, dc=dc, rotation=rotation) # rotation corresponds to MADCTL x36 register value rotation_to_x36 = { 0: 0x00, # Default portrait 1: 0x60, # 90 degrees clockwise, landscape 2: 0xC0, # 180 degrees rotation 3: 0xA0, # 270 degrees clockwise } x36_value = rotation_to_x36.get(rotation, 0x00) print(f\u0026#34;Memory access control value: {x36_value:#04x}\u0026#34;) # Select column and row address ranges based on rotation if rotation % 2 == 0: col_start, col_end = 0, 75 # width - 1 row_start, row_end = 0, 283 # height - 1 else: col_start, col_end = 0, 283 row_start, row_end = 0, 75 init_commands = [ (b\u0026#39;\\x01\u0026#39;, None, 150), # Software reset (b\u0026#39;\\x11\u0026#39;, None, 120), # Exit sleep mode (b\u0026#39;\\x3A\u0026#39;, b\u0026#39;\\x05\u0026#39;, 10), # Set pixel format RGB565 (b\u0026#39;\\x36\u0026#39;, bytes([x36_value]), 10), # Memory access control (b\u0026#39;\\x2A\u0026#39;, col_start.to_bytes(2, \u0026#39;big\u0026#39;) + col_end.to_bytes(2, \u0026#39;big\u0026#39;), 10), # Column address set (b\u0026#39;\\x2B\u0026#39;, row_start.to_bytes(2, \u0026#39;big\u0026#39;) + row_end.to_bytes(2, \u0026#39;big\u0026#39;), 10), # Row address set (b\u0026#39;\\x21\u0026#39;, None, 10), # Display inversion (b\u0026#39;\\x13\u0026#39;, None, 10), # Normal display (b\u0026#39;\\x29\u0026#39;, None, 100), # Display on ] tft.init(init_commands) print(\u0026#34;init_commands finished\u0026#34;) test_display_basic() # Test pixel display try: tft.pixel(0, 0, 0xFFFF) # Top left tft.pixel(st_width // 2, st_height // 2, 0xF800) # Center point red tft.pixel(st_width - 1, st_height - 1, 0xFFFF) # Bottom right print(f\u0026#34;{st_width}x{st_height} display area set successfully\u0026#34;) return tft except Exception as e: print(f\u0026#34;Display test failed: {e}\u0026#34;) return None # 135 x 240 display initialization # Assuming you are using ST7789 display with resolution 135x240 # If you are using other models, adjust parameters accordingly ST_WIDTH = 240 # Driver internally understands as width, but actually becomes height in landscape ST_HEIGHT = 320 # Initialize TFT object tft = init_st7789_display(ST_WIDTH, ST_HEIGHT, 3) # Rotation 3 means 270 degrees clockwise def main(): \u0026#34;\u0026#34;\u0026#34;Main program entry\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;--- ESP32-C3 ---\u0026#34;) tft.fill(st7789.BLACK) # Black background display_text_on_tft(tft, \u0026#34;ESP32-C3 TFT Display\\nReady to receive text...\u0026#34;) ip_address = connect_to_available_wifi() start_http_server(tft, ip_address) if __name__ == \u0026#34;__main__\u0026#34;: main() Multi-line text display show_text.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from machine import Pin, SPI import vga1_8x8 as font import st7789 def display_text_on_tft(tft, text_content): \u0026#34;\u0026#34;\u0026#34;Display text content on TFT screen (5 lines, 35 character limit, line start x=20)\u0026#34;\u0026#34;\u0026#34; if not tft: print(\u0026#34;Error: TFT display not initialized.\u0026#34;) return tft.fill(st7789.BLACK) # Black background # Title bar tft.text(font, \u0026#34;Received Text:\u0026#34;, 20, 2, 0x07FF) tft.text(font, \u0026#34;-\u0026#34; * 35, 20, 12, 0x07FF) x_offset = 20 # Line start x coordinate changed to 20 max_chars_per_line = 35 y_start = 85 # Line start y coordinate changed to 85 line_height = font.HEIGHT + 1 max_lines = 5 # Display only 5 lines lines = text_content.split(\u0026#39;\\n\u0026#39;) line_count = 0 for line in lines: while len(line) \u0026gt; max_chars_per_line: if line_count \u0026lt; max_lines: y_offset = y_start + line_count * line_height line_text = line[:max_chars_per_line] print(f\u0026#34;Displaying line: {line_text} at y={y_offset}\u0026#34;) tft.text(font, line_text, x_offset, y_offset, st7789.WHITE) line = line[max_chars_per_line:] line_count += 1 else: # Lines used up, display prompt tft.text(font, \u0026#34;... (more)\u0026#34;, x_offset, y_start + (max_lines - 1) * line_height, 0xF800) return if line_count \u0026lt; max_lines: tft.text(font, line, x_offset, y_start + line_count * line_height, st7789.WHITE) line_count += 1 else: tft.text(font, \u0026#34;... (more)\u0026#34;, x_offset, y_start + (max_lines - 1) * line_height, 0xF800) return Final effect\nAfter putting it back, I found the display was reversed, couldn\u0026rsquo;t change the installation position, just modify the code.\nInternal wiring\nPut in a USB cable for power + data transmission\nSummary Because the screen size and driver supported size don\u0026rsquo;t match perfectly, the final solution was to implement cropping in code logic.\nAlso because when the screen size rotates, the memory address needs to be changed, I struggled with this for a long time.\nThe other code is all trial and error verification, no big problems.\nPostscript -\u0026gt; \u0026ldquo;Friendly Advice\u0026rdquo; This device is assembled with solid glue and snaps,\nNo possibility of later disassembly.\nAll disassembly is based on violence and destruction.\nSo:\nThe disassembly of this thing is a \u0026ldquo;destructive solution\u0026rdquo;, no non-destructive modification.\nThe disassembly of this thing is a \u0026ldquo;destructive solution\u0026rdquo;, no non-destructive modification.\nThe disassembly of this thing is a \u0026ldquo;destructive solution\u0026rdquo;, no non-destructive modification.\nThe internal screen didn\u0026rsquo;t find an adapted IC driver board,\nThe four power lines on the left and right sides are not very firmly soldered,\nEasy to break during disassembly,\nNormally hard to reuse this thing\n(If friends have solutions, DM me.\nBattery disassembly has fire risk, please operate carefully.\nBattery disassembly has fire risk, please operate carefully.\nBattery disassembly has fire risk, please operate carefully.\nFinally.\nHave fun~\nResource links:\nhttps://github.com/russhughes/st7789py_mpy/ ","date":"2025-07-30T23:36:00Z","image":"https://liguobao.github.io/zh/p/esp32-diy/logo_hu_7eebd6ecfe307200.png","permalink":"https://liguobao.github.io/p/esp32-diy/","title":"Starting from Scratch: Making an Internet Host Key (Based on ESP32)"},{"content":"Extract boot.img from ROM package If you have a ROM package, directly extract the boot.img file from the ROM package\u0026rsquo;s payload.bin.\nGenerally use FastbootEnhance, graphical interface click click and done.\nMany times we may not have the ROM package, or the ROM package is several GB, troublesome to download.\nAt this time, you can try using dsu-sideloader + system-squeak-arm64-ab-vanilla.img\nEnter sideload system mode, use adb shell to extract boot.img.\nDSU-Sideloader dsu-sideloader is a tool or service component in Android system for Dynamic System Updates (DSU) mechanism, it allows users to temporarily load and run an alternative system image without affecting the current system. This is an official mechanism for developers to debug or test GSI (Generic System Image).\nğŸ” What is DSU (Dynamic System Updates)?\nDSU is a feature introduced by Google from Android 10, allowing loading a system image (usually GSI) into dynamic partition for testing on supported devices, without destroying the current device\u0026rsquo;s actual system (userdata/system partitions).\nExtract boot.img from running Android system After installing dsu + system-squeak-arm64-ab-vanilla.img, enter sideload system mode.\nIn the sideload system, use the following script to extract boot.img:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/bin/bash set -e # Get current slot slot=$(adb shell getprop ro.boot.slot_suffix | tr -d \u0026#39;\\r\u0026#39;) if [[ -z \u0026#34;$slot\u0026#34; ]]; then echo \u0026#34;âŒ Unable to get slot_suffix\u0026#34; exit 1 fi echo \u0026#34;ğŸ” Current boot slot: $slot\u0026#34; # Get boot partition real path (use adb shell su to enter root then ls -l) boot_dev=$(adb shell \u0026lt;\u0026lt;EOF su ls -l /dev/block/by-name/boot$slot | awk \u0026#39;{print \\$NF}\u0026#39; exit EOF ) boot_dev=$(echo \u0026#34;$boot_dev\u0026#34; | tr -d \u0026#39;\\r\u0026#39; | tail -n 1) if [[ -z \u0026#34;$boot_dev\u0026#34; ]]; then echo \u0026#34;âŒ Unable to resolve boot$slot partition path\u0026#34; exit 1 fi echo \u0026#34;ğŸ“ Resolved boot$slot real path: $boot_dev\u0026#34; # Execute dd to extract boot.img adb shell \u0026lt;\u0026lt;EOF su dd if=$boot_dev of=/sdcard/boot.img exit EOF # Pull to local echo \u0026#34;ğŸ“¥ Pulling boot.img to local...\u0026#34; adb pull /sdcard/boot.img echo \u0026#34;âœ… Extraction completed! boot.img saved to current directory.\u0026#34; ","date":"2025-06-24T23:36:00Z","permalink":"https://liguobao.github.io/p/android-boot-image-extraction/","title":"Android Boot Image Extraction Memo"},{"content":"Migrating chat records on mobile basically uses WeChat\u0026rsquo;s internal tools.\nCross-machine migration on macOS requires some fiddling.\nAfter researching, you actually just need to backup\n\u0026ldquo;$HOME/Library/Containers/com.tencent.xinWeChat\u0026rdquo;\nThe entire folder to the new machine,\nKeep WeChat versions consistent on both sides,\nBasically nothing else needs to be done.\nSo with GPT\u0026rsquo;s help,\nI wrote the following script backup_wechat.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash set -euo pipefail BACKUP_SRC=\u0026#34;$HOME/Library/Containers/com.tencent.xinWeChat\u0026#34; BACKUP_DIR=\u0026#34;/Volumes/workspace/weixin\u0026#34; TIMESTAMP=$(date +%Y%m%d_%H%M%S) BACKUP_DEST=\u0026#34;$BACKUP_DIR/wechat_backup_${TIMESTAMP}.tar.gz\u0026#34; echo \u0026#34;ğŸš€ Starting WeChat chat record backup...\u0026#34; # Check WeChat process if pgrep -f WeChat \u0026gt; /dev/null; then echo \u0026#34;âš ï¸ WeChat is detected running, please exit WeChat first (Cmd + Q), then run the script again.\u0026#34; exit 1 fi # Check data directory if [ ! -d \u0026#34;$BACKUP_SRC\u0026#34; ]; then echo \u0026#34;âŒ WeChat data directory not found: $BACKUP_SRC\u0026#34; exit 1 fi # Check backup directory if [ ! -d \u0026#34;$BACKUP_DIR\u0026#34; ]; then echo \u0026#34;âŒ Backup directory does not exist: $BACKUP_DIR\u0026#34; exit 1 fi echo \u0026#34;ğŸ“¦ Starting packaging... (Start time: $(date \u0026#39;+%F %T\u0026#39;))\u0026#34; START_TIME=$(date +%s) tar --exclude=\u0026#39;.com.apple.containermanagerd.metadata.plist\u0026#39; --preserve-permissions -czf \u0026#34;$BACKUP_DEST\u0026#34; -C \u0026#34;$HOME/Library/Containers\u0026#34; com.tencent.xinWeChat END_TIME=$(date +%s) DURATION=$((END_TIME - START_TIME)) echo \u0026#34;âœ… Backup completed!\u0026#34; echo \u0026#34;ğŸ“ File location: $BACKUP_DEST\u0026#34; echo \u0026#34;â±ï¸ Time taken: ${DURATION} seconds ($(printf \u0026#39;%02d:%02d\u0026#39; $((DURATION/60)) $((DURATION%60))))\u0026#34; Note: /Volumes/workspace/weixin is a NAS disk mounted locally,\nAfter all, my computer doesn\u0026rsquo;t have 170G of available space.\nIt\u0026rsquo;s better to have an SSD portable hard drive,\nBut I don\u0026rsquo;t have one on hand~\nRestore script restore_wechat.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash set -euo pipefail BACKUP_DIR=\u0026#34;/Volumes/workspace/weixin\u0026#34; BACKUP_FILE=$(ls -t \u0026#34;$BACKUP_DIR\u0026#34;/wechat_backup_*.tar.gz 2\u0026gt;/dev/null | head -1 || true) TARGET_DIR=\u0026#34;$HOME/Library/Containers/com.tencent.xinWeChat\u0026#34; if [ -z \u0026#34;$BACKUP_FILE\u0026#34; ]; then echo \u0026#34;âŒ No backup file found, please confirm there is a WeChat backup file in $BACKUP_DIR directory.\u0026#34; exit 1 fi echo \u0026#34;ğŸš€ Starting WeChat chat record restore...\u0026#34; if pgrep -f WeChat \u0026gt; /dev/null; then echo \u0026#34;âš ï¸ WeChat is detected running, please exit WeChat first (Cmd + Q), then run the script again.\u0026#34; exit 1 fi echo \u0026#34;ğŸ“¦ Backup file to restore: $BACKUP_FILE\u0026#34; echo \u0026#34;ğŸ“¦ File size: $(du -sh \u0026#34;$BACKUP_FILE\u0026#34; | awk \u0026#39;{print $1}\u0026#39;)\u0026#34; echo \u0026#34;ğŸ“‚ Starting decompression and overwrite old data... (Start time: $(date \u0026#39;+%F %T\u0026#39;))\u0026#34; START_TIME=$(date +%s) # Decompress, overwrite old content (do not delete original directory) sudo tar -xzf \u0026#34;$BACKUP_FILE\u0026#34; -C \u0026#34;$HOME/Library/Containers\u0026#34; END_TIME=$(date +%s) DURATION=$((END_TIME - START_TIME)) echo \u0026#34;âœ… Restore completed!\u0026#34; echo \u0026#34;â±ï¸ Time taken: ${DURATION} seconds ($(printf \u0026#39;%02d:%02d\u0026#39; $((DURATION/60)) $((DURATION%60))))\u0026#34; echo \u0026#34;Please open WeChat to check if chat records are complete.\u0026#34; The whole process is even faster than mobile chat record backup and restore.\n(After all, NAS transmission speed with full bandwidth is fast)\nWeChat really\u0026hellip;.\nIn principle, Windows computers are similar,\nBut I basically don\u0026rsquo;t fiddle with Windows anymore,\nFigure it out yourself~\n","date":"2025-06-24T23:36:00Z","permalink":"https://liguobao.github.io/p/wechat-backup/","title":"macOS Lossless Migration of PC WeChat Chat Records"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 #!/bin/bash echo \u0026#39;Starting wireguard-install.sh\u0026#39; set -e WG_DIR=\u0026#34;/etc/wireguard\u0026#34; KEY_DIR=\u0026#34;/tmp/wg-keys\u0026#34; WG_CONF=\u0026#34;$WG_DIR/wg0.conf\u0026#34; WG_NET_BASE=\u0026#34;10.200.200\u0026#34; WG_PORT=51820 NODE_LIST=\u0026#34;$HOME/all-node.txt\u0026#34; # === Step 0: Check node list file === if [[ ! -f \u0026#34;$NODE_LIST\u0026#34; ]]; then echo \u0026#34;âŒ Node list file does not exist: $NODE_LIST\u0026#34; exit 1 fi mkdir -p \u0026#34;$WG_DIR\u0026#34; mkdir -p \u0026#34;$KEY_DIR\u0026#34; chmod 700 \u0026#34;$WG_DIR\u0026#34; # === Step 1: Read IP list === ALL_IPS=() while read -r ip; do [[ -z \u0026#34;$ip\u0026#34; || \u0026#34;$ip\u0026#34; =~ ^# ]] \u0026amp;\u0026amp; continue ALL_IPS+=(\u0026#34;$ip\u0026#34;) done \u0026lt; \u0026#34;$NODE_LIST\u0026#34; NODE_COUNT=${#ALL_IPS[@]} # === Step 2: Get public IP function, prefer iinti.cn, fallback to ipify === get_public_ip() { ip=$(curl -s https://iinti.cn/conn/getPublicIp) if [[ -z \u0026#34;$ip\u0026#34; || \u0026#34;$ip\u0026#34; == \u0026#34;null\u0026#34; ]]; then ip=$(curl -s https://api.ipify.org) fi echo \u0026#34;$ip\u0026#34; } echo \u0026#34;Preparing to get public IP...\u0026#34; MY_PUBLIC_IP=$(get_public_ip) if [[ -z \u0026#34;$MY_PUBLIC_IP\u0026#34; ]]; then echo \u0026#34;âŒ Unable to get public IP, exit\u0026#34; exit 1 fi echo \u0026#34;Got public IP: $MY_PUBLIC_IP\u0026#34; MY_ID=0 for i in \u0026#34;${!ALL_IPS[@]}\u0026#34;; do if [[ \u0026#34;${ALL_IPS[$i]}\u0026#34; == \u0026#34;$MY_PUBLIC_IP\u0026#34; ]]; then MY_ID=$((i + 1)) break fi done if [[ $MY_ID -eq 0 ]]; then echo \u0026#34;âŒ Current public IP not found in all-node.txt: $MY_PUBLIC_IP\u0026#34; exit 1 fi MY_WG_IP=\u0026#34;${WG_NET_BASE}.${MY_ID}/24\u0026#34; MY_KEY_NAME=\u0026#34;node${MY_ID}.pub\u0026#34; echo \u0026#34; Current node number: $MY_ID\u0026#34; echo \u0026#34; Public IP: $MY_PUBLIC_IP\u0026#34; echo \u0026#34; WireGuard IP: $MY_WG_IP\u0026#34; # === Step 3: Install WireGuard (if not installed) === if ! command -v wg \u0026amp;\u0026gt;/dev/null; then echo \u0026#34; Installing wireguard-tools...\u0026#34; apt update \u0026amp;\u0026amp; apt install -y wireguard fi # === Step 4: Generate keys === PRIVATE_KEY_FILE=\u0026#34;$WG_DIR/privatekey\u0026#34; PUBLIC_KEY_FILE=\u0026#34;$WG_DIR/publickey\u0026#34; if [ ! -f \u0026#34;$PRIVATE_KEY_FILE\u0026#34; ]; then echo \u0026#34; Generating key pair...\u0026#34; wg genkey | tee \u0026#34;$PRIVATE_KEY_FILE\u0026#34; | wg pubkey \u0026gt; \u0026#34;$PUBLIC_KEY_FILE\u0026#34; fi MY_PRIVATE_KEY=$(cat \u0026#34;$PRIVATE_KEY_FILE\u0026#34;) MY_PUBLIC_KEY=$(cat \u0026#34;$PUBLIC_KEY_FILE\u0026#34;) # === Step 5: Write local public key to local directory === echo \u0026#34;$MY_PUBLIC_KEY\u0026#34; \u0026gt; \u0026#34;$KEY_DIR/$MY_KEY_NAME\u0026#34; # === Step 6: Automatically scp local public key to all other nodes\u0026#39; KEY_DIR === echo \u0026#34; Automatically sync public key to other nodes...\u0026#34; for ip in \u0026#34;${ALL_IPS[@]}\u0026#34;; do # Skip local machine if [[ \u0026#34;$ip\u0026#34; == \u0026#34;$MY_PUBLIC_IP\u0026#34; ]]; then continue fi echo \u0026#34;â³ Copy public key to $ip:$KEY_DIR\u0026#34; ssh -o ConnectTimeout=5 root@\u0026#34;$ip\u0026#34; \u0026#34;mkdir -p $KEY_DIR \u0026amp;\u0026amp; chmod 700 $KEY_DIR\u0026#34; scp \u0026#34;$KEY_DIR/$MY_KEY_NAME\u0026#34; root@\u0026#34;$ip\u0026#34;:\u0026#34;$KEY_DIR/\u0026#34; done # === Step 7: Wait for all nodes\u0026#39; public key files to be ready === echo \u0026#34; Waiting for all node public keys to be written to $KEY_DIR ...\u0026#34; while true; do FOUND_KEYS=$(ls \u0026#34;$KEY_DIR\u0026#34;/*.pub 2\u0026gt;/dev/null | wc -l) if [ \u0026#34;$FOUND_KEYS\u0026#34; -ge \u0026#34;$NODE_COUNT\u0026#34; ]; then echo \u0026#34;âœ… Detected all $NODE_COUNT nodes\u0026#39; public keys\u0026#34; break fi echo -n \u0026#34;.\u0026#34; sleep 2 done # === Step 8: Generate WireGuard configuration === echo \u0026#34;âš™ï¸ Generating WireGuard configuration: $WG_CONF\u0026#34; echo \u0026#34;[Interface]\u0026#34; \u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;Address = $MY_WG_IP\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;ListenPort = $WG_PORT\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;PrivateKey = $MY_PRIVATE_KEY\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; for i in \u0026#34;${!ALL_IPS[@]}\u0026#34;; do ID=$((i + 1)) PEER_IP=\u0026#34;${ALL_IPS[$i]}\u0026#34; PEER_WG_IP=\u0026#34;${WG_NET_BASE}.${ID}/32\u0026#34; if [ \u0026#34;$ID\u0026#34; -eq \u0026#34;$MY_ID\u0026#34; ]; then continue fi PEER_KEY_FILE=\u0026#34;$KEY_DIR/node${ID}.pub\u0026#34; if [ ! -f \u0026#34;$PEER_KEY_FILE\u0026#34; ]; then echo \u0026#34;âŒ Missing $PEER_KEY_FILE, exit\u0026#34; exit 1 fi PEER_KEY=$(cat \u0026#34;$PEER_KEY_FILE\u0026#34;) echo \u0026#34;[Peer]\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;PublicKey = $PEER_KEY\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;AllowedIPs = $PEER_WG_IP\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;Endpoint = $PEER_IP:$WG_PORT\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026#34;PersistentKeepalive = 25\u0026#34; \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; echo \u0026gt;\u0026gt; \u0026#34;$WG_CONF\u0026#34; done # === Step 9: Start WireGuard === echo \u0026#34; Starting WireGuard...\u0026#34; systemctl stop wg-quick@wg0 \u0026amp;\u0026gt;/dev/null || true wg-quick up wg0 systemctl enable wg-quick@wg0 echo \u0026#34;âœ… Started successfully, current node WireGuard IP: ${WG_NET_BASE}.${MY_ID}\u0026#34; echo \u0026#34; Check status: wg show\u0026#34; Prerequisites Nodes can log in to each other without password All have reachable public IPs After some fiddling, basically achieved cross-cloud intranet connectivity, latency within acceptable range.\n(After all, it\u0026rsquo;s across oceans.\nThe reason for this setup is that earlier,\nDirectly fiddling with WireGuard on k3s always had strange issues.\nForget it, set it up myself.\nNow it looks much more stable.\nNo more fiddling with strange configurations on k3s side.\nDone.\n","date":"2025-06-08T08:00:00Z","image":"https://liguobao.github.io/zh/p/wireguard-setup/logo_hu_ade08796556b0394.jpg","permalink":"https://liguobao.github.io/p/wireguard-setup/","title":"Cross-Cloud Intranet WireGuard Configuration Memo"},{"content":"When doing things in China, you have to solve network issues daily.\nIt\u0026rsquo;s okay locally, but it\u0026rsquo;s a bit painful on the server.\nToday when fiddling with the k3s cluster, I accidentally wiped out etcd.\nSo I started reinstalling the entire cluster.\nEverything else is fine, the images are all on some cloud, but traefik is painful.\nThis thing uses docker hub source.\nIf it was a few years ago, it would be fine, there were so many mirror sources in China, just configure one and it\u0026rsquo;s done.\nBut after last year, the mirror sources died one by one.\nCan\u0026rsquo;t give the server a scientific internet either.\nAnd the premise of scientific internet is that you have to scientific internet first to download things.\nDead end.\nAfter fiddling for several hours, I had a sudden idea.\nJust export the image locally and import it, isn\u0026rsquo;t that done.\nk3s although it doesn\u0026rsquo;t use docker runtime, but it has compatible commands.\nAsked GPT, very simple.\n1 2 3 4 5 docker save docker.io/traefik:v3.4.1 -o traefik_v3.4.1.tar scp traefik_v3.4.1.tar root@my-ip:~/ ctr -n k8s.io images import /root/traefik_v3.4.1.tar Done.\nYes. That\u0026rsquo;s how simple it is.\nEverything is so harmonious.\n","date":"2025-06-07T08:00:00Z","permalink":"https://liguobao.github.io/p/k3s-image-pull-error/","title":"k3s Daily ImagePullError Memo"},{"content":"Part1 \u0026ldquo;The Sculpture Garden was planned to become Asia\u0026rsquo;s largest sculpture-themed park, covering hundreds of acres, after arduous construction, it finally became unfinished. The biggest feature of this sculpture garden is that there is not a single sculpture.\u0026rdquo;\n\u0026ldquo;My Understanding of Life - Answering Questions from Tinglin Youth\u0026rdquo; The initial memory about this place only comes from some article by Han Han.\nLater around 2022, during Shanghai\u0026rsquo;s \u0026ldquo;orderly\u0026rdquo; and \u0026ldquo;self-quarantine\u0026rdquo;, one of the few areas where you could move freely was in the \u0026ldquo;countryside\u0026rdquo; of Jinshan District 70km from the city center, at that time you could still see reporters interviewing on the news broadcast.\nBesides that, having lived in Shanghai for ten years, I had no concept of Jinshan at all.\nBefore having a car, places not covered by an hour\u0026rsquo;s subway ride weren\u0026rsquo;t places I wanted to go.\nAfter having a car, places not reachable within an hour\u0026rsquo;s drive need motivation and drive.\nHowever, this spring weekend, after watching the cherry blossoms at night,\nAfter sleeping well, it seemed like I could go out.\nSo I drove an hour to Tinglin Town.\nThe rapeseed flowers there are blooming, I can go see them.\nAfter getting off the highway, I arrived at the rural path,\nI roughly understood what kind of place this is.\nCountryside.\nEverything grows from the soil.\nThe land is a bit damp after the rain.\nSmall river flows by the village,\nBroad beans planted in the roadside gaps,\nNow blooming with black flowers.\nJust like familiar countryside.\nJust like the hometown I came from.\nPart2 After coming out of the countryside, I drove to the \u0026ldquo;real\u0026rdquo; Tinglin Town.\nLooking around, it\u0026rsquo;s basically the appearance of an ordinary southern town.\nProbably won\u0026rsquo;t associate with \u0026ldquo;Magic City\u0026rdquo; at all.\nOpened Dianping, found a mutton restaurant opposite the parking lot.\nAfter entering, couldn\u0026rsquo;t see the menu, a bit confused.\nBut the boss and boss lady were very enthusiastic in receiving, after all, it looked like no one was eating in the afternoon.\nSat down listening to the boss recommending ways to eat mutton.\nHalf understanding, felt indifferent, just eat a mutton offal hotpot.\nSoon the boss lady brought a pot of soup,\nMutton offal, blood, meat all there, white soup mixed with garlic sprouts tasted quite good.\nHalfway through, the boss lady added a bowl of rice for us, just enough to eat.\nWent through the long corridor inside the shop to the bathroom, saw there were several private rooms inside,\nAlthough they don\u0026rsquo;t seem to be used much now, the small shop must have been lively once.\nIt\u0026rsquo;s just that the signboards are all rusty now.\nPart3 Probably won\u0026rsquo;t go to Tinglin Town again.\nMany places in life, you go once and that\u0026rsquo;s it,\nMeetings have their time, that\u0026rsquo;s about it.\nSome people can meet once more times.\n","date":"2025-03-31T12:36:00Z","image":"https://liguobao.github.io/zh/p/tingling-world/logo_hu_c2d03dfdcb456be2.png","permalink":"https://liguobao.github.io/p/tingling-world/","title":"Tinglin Town \"Past Events\""},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 // ==UserScript== // @name Download Weibo Album Images as ZIP File // @namespace http://tampermonkey.net/ // @version 0.1 // @description Download all images from Weibo album and package them as ZIP file // @author You // @match https://weibo.com/u/*?tabtype=album // @grant GM_addStyle // ==/UserScript== (function() { \u0026#39;use strict\u0026#39;; // Get Weibo user\u0026#39;s ID as download filename const match = window.location.href.match(/https:\\/\\/weibo\\.com\\/u\\/(\\d+)\\?/); if (!match) { console.error(\u0026#34;Unable to get Weibo user ID\u0026#34;); return; } const userId = match[1]; const downloadFileName = `weibo_album_${userId}.zip`; // Insert button to page top and center it const button = document.createElement(\u0026#34;button\u0026#34;); button.innerText = \u0026#34;Download Images as ZIP\u0026#34;; button.style.position = \u0026#34;fixed\u0026#34;; button.style.top = \u0026#34;50%\u0026#34;; // Vertical center button.style.left = \u0026#34;50%\u0026#34;; // Horizontal center button.style.transform = \u0026#34;translate(-50%, -50%)\u0026#34;; // Make button completely centered button.style.zIndex = \u0026#34;9999\u0026#34;; // Ensure button is at page top button.style.padding = \u0026#34;10px 20px\u0026#34;; button.style.backgroundColor = \u0026#34;#007bff\u0026#34;; button.style.color = \u0026#34;#fff\u0026#34;; button.style.border = \u0026#34;none\u0026#34;; button.style.borderRadius = \u0026#34;5px\u0026#34;; button.style.cursor = \u0026#34;pointer\u0026#34;; button.style.fontSize = \u0026#34;16px\u0026#34;; // Increase font size button.style.boxShadow = \u0026#34;0 2px 10px rgba(0, 0, 0, 0.3)\u0026#34;; // Add shadow document.body.appendChild(button); // Button click event button.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { downloadAllImagesAsZip(); }); // Load JSZip library const loadJSZip = () =\u0026gt; { return new Promise((resolve, reject) =\u0026gt; { const script = document.createElement(\u0026#34;script\u0026#34;); script.src = \u0026#34;https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js\u0026#34;; script.onload = resolve; script.onerror = reject; document.head.appendChild(script); }); }; // Download all images and package as ZIP const downloadAllImagesAsZip = async () =\u0026gt; { console.log(\u0026#34;Starting to load JSZip...\u0026#34;); await loadJSZip(); // Load JSZip console.log(\u0026#34;JSZip loaded successfully.\u0026#34;); const zip = new JSZip(); // Use new JSZip() to instantiate const images = document.getElementsByClassName(\u0026#34;woo-picture-img\u0026#34;); console.log(`Found ${images.length} images in total.`); if (images.length === 0) { alert(\u0026#34;No images found!\u0026#34;); // If no images, alert user return; // Terminate function, do not execute download logic } // Download all images and store in memory const imageBlobs = await Promise.all( Array.from(images).map((img, index) =\u0026gt; { if (img.tagName === \u0026#34;IMG\u0026#34; \u0026amp;\u0026amp; img.src) { console.log(`Starting to download image ${index + 1}: ${img.src}`); return fetch(img.src) .then((response) =\u0026gt; { if (!response.ok) { throw new Error(`HTTP status code: ${response.status}`); } return response.blob(); }) .then((blob) =\u0026gt; { console.log(`Image ${index + 1} download completed.`); return { blob, index, src: img.src }; }) .catch((error) =\u0026gt; { console.error(`Unable to download image ${index + 1}: ${img.src}`, error); return null; // Return null means download failed }); } else { console.warn(`Invalid image element: ${img}`); return null; } }) ); console.log(\u0026#34;All images downloaded, starting to add to ZIP package.\u0026#34;); // Add images to ZIP package imageBlobs.forEach((image) =\u0026gt; { if (image) { const ext = image.src.split(\u0026#34;.\u0026#34;).pop().split(\u0026#34;?\u0026#34;)[0] || \u0026#34;jpg\u0026#34;; // Extract extension zip.file(`image-${image.index + 1}.${ext}`, image.blob); // Add to ZIP console.log(`Image ${image.index + 1} added to ZIP package.`); } }); // Generate ZIP and download console.log(\u0026#34;Starting to generate ZIP file.\u0026#34;); zip.generateAsync({ type: \u0026#34;blob\u0026#34; }).then((content) =\u0026gt; { console.log(\u0026#34;ZIP file generation completed, starting download.\u0026#34;); const a = document.createElement(\u0026#34;a\u0026#34;); a.href = URL.createObjectURL(content); a.download = downloadFileName; // Use user ID as download filename document.body.appendChild(a); a.click(); document.body.removeChild(a); console.log(\u0026#34;ZIP file download completed.\u0026#34;); }); }; })(); The effect.\nMight encounter CORS error.\nSolution -\u0026gt; Use Modheader to modify ResponseHeader.\nSet Access-Control-Allow-Origin to *\nDone.\nDownload content.\n","date":"2025-01-23T08:00:00Z","image":"https://liguobao.github.io/zh/p/one-line-download-weibo-img/logo_hu_d61078ab2b688570.png","permalink":"https://liguobao.github.io/p/one-line-download-weibo-img/","title":"One Line Code to Download Weibo Album"},{"content":"mini-dashboard is a small query dashboard for meilisearch,\nIt provides a cluster query page, which is just enough for my needs.\nHowever, the official image build is just a node Debug version,\nSo I built a static site image myself.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FROM node:20-alpine AS build-env WORKDIR /home/node/app RUN chown -R node:node /home/node/app USER node COPY package*.json ./ COPY --chown=node:node . . RUN yarn install ENV NODE_ENV=production # if you want to use your own meilisearch server, you can set the following env # ENV REACT_APP_MEILI_SERVER_ADDRESS=http://meilisearch:7700 # ENV REACT_APP_MEILI_API_KEY=masterKey RUN yarn build FROM nginx COPY --from=build-env /home/node/app/build/ /usr/share/nginx/html/ Docker with Nginx nginx is also available in the Docker image.\nYou can use it to serve the mini-dashboard.\n1 2 docker build --build-arg REACT_APP_MEILI_SERVER_ADDRESS=http://meilisearch:7700 -t meilisearch-mini-dashboard-nginx . -f Dockerfile.nginx docker run -p 8080:80 meilisearch-mini-dashboard-nginx You can then access the mini-dashboard at http://localhost:8080.\nThat\u0026rsquo;s about it.\nOf course, the simplest is to directly use docker-compose\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 version: \u0026#34;3.8\u0026#34; services: meilisearch-node1: image: getmeili/meilisearch:v1.12 restart: always container_name: meilisearch-node1 ports: - \u0026#34;7700:7700\u0026#34; environment: MEILI_MASTER_KEY: \u0026#34;iinti_2025_gogogo_123\u0026#34; MEILI_ENV: \u0026#34;production\u0026#34; MEILI_DB_PATH: \u0026#34;/meili_data\u0026#34; MEILI_NO_ANALYTICS: \u0026#34;true\u0026#34; volumes: - /mnt/nvme/Meilisearch/meili_data/node1:/meili_data networks: - meili-network meilisearch-dashboard: restart: always image: mini-dashboard-nginx container_name: meilisearch-dashboard ports: - \u0026#34;7707:80\u0026#34; networks: - meili-network networks: meili-network: driver: bridge ","date":"2025-01-16T08:00:00Z","image":"https://liguobao.github.io/zh/p/meili-dashboard/logo_hu_d248d5d94b3cb12b.png","permalink":"https://liguobao.github.io/p/meili-dashboard/","title":"Build meilisearch mini-dashboard static site image"},{"content":"Windows Full-Disk Backup and Migration â€” A Quick Guide A friend recently kept complaining that the laptop was running out of disk space.\nAfter checking the model and confirming with Xiaomi support, this model has only one M.2 slot, with no extra expansion bay.\nThe SSD is not soldered, though, so replacing it is straightforward.\nAccording to the teardown guides, you can swap the drive after removing the back cover screws.\nWhat You Need 4 TB M.2 SSD Precision screwdriver kit SSD enclosure (USB) Software: AOMEI Partition Assistant Workflow Install the new SSD into the USB enclosure. Use â€œPartition Assistantâ€ to clone the entire disk to the new SSD. Open the laptop, swap the original SSD with the new one. Boot up and verify the system starts and runs normally. Clone the Entire Disk This part is simple: open Partition Assistant, choose Disk Clone, select source and target disks, then start cloning.\nWith both drives being SSDs, the cloning finished in just over an hour for me.\nReference tutorials:\nNo reinstall needed â€” migrate OS to SSD easily: https://www.disktool.cn/content-center/migrate-os-to-ssd/transfer-os-to-ssd.html How to quickly migrate your system to a new drive (3 methods): https://www.disktool.cn/jiaocheng/migrate-system-drive.html The steps are very similar across guides, so I wonâ€™t repeat them.\nAfter migration, open File Explorer and spot-check that the data looks right.\nSwap the Drive This is basically unscrew, remove, replace, and screw back.\nI sent my friend a teardown video; they handled it by themselves.\nYes â€” the entire process was DIY on their side, I only provided remote guidance.\nIssue #1: â€œNo bootable deviceâ€ After the swap, the laptop showed â€œNo bootable deviceâ€.\nFirst thought: maybe the drive wasnâ€™t seated properly?\nWe rechecked â€” the drive was firmly connected.\nSwapped the old SSD back in â€” boots fine.\nSwapped the new SSD in again â€” still â€œNo bootable deviceâ€.\nThey didnâ€™t have a PE USB handy at night, so we paused and made one the next day.\nThinking about it in the morning â€” likely a bootloader issue: the new disk doesnâ€™t have valid boot records.\nWith a WinPE USB prepared, we booted into PE and confirmed the new SSD was detected. Capacity and partitions matched the old drive.\nWe first tried the builtâ€‘in boot repair tool in PE.\nRebooted â€” same error.\nIssue #2: MBR/boot records not right? Back into PE, we used DiskGenius to manually repair the MBR/boot records.\nRebooted â€” still the same error.\nStrange. What else could be missing?\nThen it hit me: after disk migration, did we forget to convert the disk to GPT?\nBack into PE, used DiskGenius to convert the new SSD to GPT.\nReboot â€” system starts normally.\nTakeaways Always have a WinPE toolkit ready when doing system migrations. After replacing the drive, pay attention to the disk partition style (GPT vs MBR). References How to repair disk boot with Laomaotao WinPE (Chinese): https://www.laomaotao.net/winpe/2020/0313/8129.html MBR (Master Boot Record) intro and repair (Chinese): https://zhuanlan.zhihu.com/p/419605886 ","date":"2024-12-27T08:00:00Z","permalink":"https://liguobao.github.io/p/windows-c-disk-move/","title":"Windows Full-Disk Backup and Migration â€” A Quick Guide"},{"content":"Recently working on GitLab migration, since the original GitLab version is too low, and it\u0026rsquo;s modified, completely impossible to upgrade from old version across several major versions to latest. So finally decided to write Python script to call GitLab API to export users, groups, and projects, then add to new GitLab via API, then copy the source repository entire folder to new server, use Python to execute shell commands, push all git bare repositories to new GitLab.\nExport Users, Groups, Projects 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 import requests import json from loguru import logger from datetime import datetime # GitLab domain gitlab_domain = \u0026#34;gitlab.com\u0026#34; # GitLab config GITLAB_URL = f\u0026#39;https://{domain}\u0026#39; # Your GitLab instance URL API_TOKEN = \u0026#39;xxx\u0026#39; # Your GitLab access token # Old version Auth Token in Header HEADERS = {\u0026#39;PRIVATE-TOKEN\u0026#39;: f\u0026#39;{API_TOKEN}\u0026#39;} # Function to get all projects def get_all_projects(): url = f\u0026#34;{GITLAB_URL}/api/v4/projects\u0026#34; params = { \u0026#34;page\u0026#34;: 1, # Default first page \u0026#34;per_page\u0026#34;: 100 # Max 100 projects per page } projects = [] while True: response = requests.get(url, headers=HEADERS, params=params) if response.status_code != 200: logger.error(f\u0026#34;Error: Unable to fetch data. Status code {response.status_code}\u0026#34;) break data = response.json() if not data: logger.info(\u0026#34;No more projects to fetch.\u0026#34;) break projects.extend(data) params[\u0026#34;page\u0026#34;] += 1 # Get next page logger.info(f\u0026#34;Fetched {len(data)} projects from page {params[\u0026#39;page\u0026#39;] - 1}.\u0026#34;) return projects # Extract required project data def extract_project_info(projects, rep_base): user_project_list = [ { \u0026#39;name\u0026#39;: p[\u0026#34;name\u0026#34;], \u0026#39;path_with_namespace\u0026#39;: p[\u0026#34;path_with_namespace\u0026#34;], \u0026#39;default_branch\u0026#39;: p[\u0026#34;default_branch\u0026#34;], \u0026#39;visibility\u0026#39;: p[\u0026#34;visibility\u0026#34;], \u0026#39;ssh_url_to_repo\u0026#39;: p[\u0026#34;ssh_url_to_repo\u0026#34;], \u0026#39;absolute_path\u0026#39;: rep_base + p[\u0026#34;ssh_url_to_repo\u0026#34;].replace(f\u0026#39;git@{gitlab_domain}:\u0026#39;, \u0026#34;\u0026#34;), \u0026#39;new_ssh_url_to_repo\u0026#39;: p[\u0026#34;ssh_url_to_repo\u0026#34;].replace(f\u0026#34;git@{gitlab_domain}:\u0026#34;, \u0026#34;git@127.0.0.1:\u0026#34;), } for p in projects ] return user_project_list # Save data to file def save_to_file(data, filename=\u0026#34;old_all_projects.json\u0026#34;): with open(filename, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: json.dump(data, f, indent=4, ensure_ascii=False) logger.info(f\u0026#34;Projects data has been saved to \u0026#39;{filename}\u0026#39;.\u0026#34;) # Main function def main(): rep_base = \u0026#34;/var/opt/gitlab/git-data/back_repositories/repositories/\u0026#34; # Your own repository path logger.info(\u0026#34;Starting the project extraction process.\u0026#34;) try: # Get all projects projects = get_all_projects() if not projects: logger.warning(\u0026#34;No projects found or there was an issue fetching data.\u0026#34;) return # Extract project info user_project_list = extract_project_info(projects, rep_base) # Save project list to file save_to_file(user_project_list) logger.info(\u0026#34;Process completed successfully.\u0026#34;) except Exception as e: logger.error(f\u0026#34;An error occurred: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Create Users, Groups, Projects 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 import requests import json import random import string from loguru import logger # New GitLab instance URL and personal access token NEW_GITLAB_URL = \u0026#39;https://new-gitlab.com\u0026#39; # New GitLab instance URL API_TOKEN = \u0026#39;xxxx\u0026#39; # New GitLab admin access token # New version auth header uses Authorization HEADERS = { \u0026#39;Authorization\u0026#39;: f\u0026#39;Bearer {API_TOKEN}\u0026#39; } # Generate random password def generate_random_password(length=12): characters = string.ascii_letters + string.digits + string.punctuation return \u0026#39;\u0026#39;.join(random.choice(characters) for i in range(length)) # Load backup data from JSON file def load_backup_data(): with open(\u0026#34;dump_gitlab_data.json\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: return json.load(f) # Create user def create_user(username, email, password): url = f\u0026#34;{NEW_GITLAB_URL}/api/v4/users\u0026#34; data = { \u0026#34;username\u0026#34;: username, \u0026#34;name\u0026#34;: username, \u0026#34;email\u0026#34;: email, \u0026#34;password\u0026#34;: password, \u0026#34;skip_confirmation\u0026#34;: True, \u0026#34;force_random_password\u0026#34;: True # Force user to change password on first login } try: response = requests.post(url, headers=HEADERS, data=data) response.raise_for_status() logger.info(f\u0026#34;User {username} created successfully.\u0026#34;) return response.json() # Return JSON with user info except requests.RequestException as e: logger.error(f\u0026#34;Error creating user {username}: {e}\u0026#34;) return None # Create project def create_project(group_id, project_name): url = f\u0026#34;{NEW_GITLAB_URL}/api/v4/projects\u0026#34; data = { \u0026#34;name\u0026#34;: project_name, \u0026#34;namespace_id\u0026#34;: group_id, # Put project in specified group \u0026#34;visibility\u0026#34;: \u0026#34;private\u0026#34;, \u0026#34;initialize_with_readme\u0026#34;: False # Create empty project by default } try: response = requests.post(url, headers=HEADERS, data=data) response.raise_for_status() logger.info(f\u0026#34;Project {project_name} created successfully in group {group_id}.\u0026#34;) return response.json() # Return created project data except requests.RequestException as e: logger.error(f\u0026#34;Error creating project {project_name} in group {group_id}: {e}\u0026#34;) return None # Create group def create_group(group_name): url = f\u0026#34;{NEW_GITLAB_URL}/api/v4/groups\u0026#34; data = { \u0026#34;name\u0026#34;: group_name, \u0026#34;path\u0026#34;: group_name.lower().replace(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;) # Auto generate path } try: response = requests.post(url, headers=HEADERS, data=data) response.raise_for_status() logger.info(f\u0026#34;Group {group_name} created successfully.\u0026#34;) return response.json() # Return JSON with group info except requests.RequestException as e: logger.error(f\u0026#34;Error creating group {group_name}: {e}\u0026#34;) return None # Add user to group and set permission to master def add_user_to_group(group_id, user_id, access_level=40): url = f\u0026#34;{NEW_GITLAB_URL}/api/v4/groups/{group_id}/members\u0026#34; data = { \u0026#34;user_id\u0026#34;: user_id, \u0026#34;access_level\u0026#34;: access_level # Set permission to master (40) } try: response = requests.post(url, headers=HEADERS, data=data) response.raise_for_status() logger.info(f\u0026#34;User {user_id} added to group {group_id} with master access.\u0026#34;) except requests.RequestException as e: logger.error(f\u0026#34;Error adding user {user_id} to group {group_id}: {e}\u0026#34;) # Import users, groups, and projects def import_data(): logger.add(\u0026#34;import_gitlab_entities.log\u0026#34;, rotation=\u0026#34;500 MB\u0026#34;, compression=\u0026#34;zip\u0026#34;) # Load backup data backup_data = load_backup_data() # Store username, email, password user_passwords = [] # Create users for user in backup_data[\u0026#34;users\u0026#34;]: username = user[\u0026#34;username\u0026#34;] email = user[\u0026#34;email\u0026#34;] password = generate_random_password() user_data = create_user(username, email, password) if user_data: user_id = user_data[\u0026#34;id\u0026#34;] user_passwords.append({\u0026#34;username\u0026#34;: username, \u0026#34;email\u0026#34;: email, \u0026#34;password\u0026#34;: password}) # Create user\u0026#39;s projects for project in user[\u0026#34;projects\u0026#34;]: project_name = project[\u0026#34;name\u0026#34;] # Find project\u0026#39;s group group_name = project[\u0026#34;namespace\u0026#34;][\u0026#34;name\u0026#34;] # When creating project, need group ID, first get group\u0026#39;s ID group_id = None for group in backup_data[\u0026#34;groups\u0026#34;]: if group[\u0026#34;name\u0026#34;] == group_name: group_id = create_group(group_name)[\u0026#34;id\u0026#34;] break if group_id: create_project(group_id, project_name) # After creating project, add user to group add_user_to_group(group_id, user_id) # Create groups and projects for group in backup_data[\u0026#34;groups\u0026#34;]: group_name = group[\u0026#34;name\u0026#34;] group_data = create_group(group_name) if group_data: group_id = group_data[\u0026#34;id\u0026#34;] # Create projects in group for project in group[\u0026#34;projects\u0026#34;]: project_name = project[\u0026#34;name\u0026#34;] create_project(group_id, project_name) # Add corresponding users to group, set to master permission for group_user in group[\u0026#34;users\u0026#34;]: add_user_to_group(group_id, group_user[\u0026#34;user_id\u0026#34;]) # Save username, email, password to JSON file with open(\u0026#34;user_passwords.json\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: json.dump(user_passwords, f, ensure_ascii=False, indent=4) logger.info(\u0026#34;User passwords have been exported to user_passwords.json\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: import_data() Push Git Bare Repositories 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 import os import subprocess import json from loguru import logger # Load config JSON file def load_config(file_path): with open(file_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: return json.load(f) # Write operation results to JSON file def write_result_to_json(results, output_file): with open(output_file, \u0026#39;w+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(results, f, indent=4, ensure_ascii=False) # Push bare repo to new remote def push_bare_repo_to_remote(project, results): absolute_path = project[\u0026#34;absolute_path\u0026#34;] new_ssh_url_to_repo = project[\u0026#34;new_ssh_url_to_repo\u0026#34;] # Check if repo path is valid if not os.path.isdir(absolute_path) or not os.path.exists(os.path.join(absolute_path, \u0026#34;HEAD\u0026#34;)): logger.error(f\u0026#34;Invalid bare repository path: {absolute_path}\u0026#34;) results.append({ \u0026#34;project\u0026#34;: project, \u0026#34;status\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;Invalid bare repository path: {absolute_path}\u0026#34; }) return try: # Enter original repo directory logger.info(f\u0026#34;Accessing local bare repository: {absolute_path}\u0026#34;) os.chdir(absolute_path) # Ensure it\u0026#39;s bare repo subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;rev-parse\u0026#34;, \u0026#34;--is-bare-repository\u0026#34;], check=True) # Set new remote URL logger.info(f\u0026#34;Setting new remote URL: {new_ssh_url_to_repo}\u0026#34;) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;remote\u0026#34;, \u0026#34;add\u0026#34;, \u0026#34;new-origin\u0026#34;, new_ssh_url_to_repo], check=True) # Push all branches to new remote logger.info(\u0026#34;Pushing all branches to the new remote repository\u0026#34;) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;new-origin\u0026#34;, \u0026#34;--all\u0026#34;,\u0026#34;-f\u0026#34;], check=True) # Push all tags to new remote logger.info(\u0026#34;Pushing all tags to the new remote repository\u0026#34;) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;new-origin\u0026#34;, \u0026#34;--tags\u0026#34;], check=True) results.append({ \u0026#34;project\u0026#34;: project, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;Successfully pushed to {new_ssh_url_to_repo}\u0026#34; }) except subprocess.CalledProcessError as e: logger.error(f\u0026#34;Error during git operations: {e}\u0026#34;) results.append({ \u0026#34;project\u0026#34;: project, \u0026#34;status\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;Error during git operations: {str(e)}\u0026#34; }) except Exception as e: logger.exception(\u0026#34;Unexpected error occurred\u0026#34;) results.append({ \u0026#34;project\u0026#34;: project, \u0026#34;status\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;Unexpected error: {str(e)}\u0026#34; }) finally: # Remove new remote config to keep original repo clean try: subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;remote\u0026#34;, \u0026#34;remove\u0026#34;, \u0026#34;new-origin\u0026#34;], check=True) except subprocess.CalledProcessError: logger.warning(\u0026#34;Failed to remove new-origin remote, please check manually.\u0026#34;) # Main function def main(): config_file = \u0026#39;old_all_projects.json\u0026#39; output_file = \u0026#39;old_all_projects_result_2024.json\u0026#39; projects = load_config(config_file) logger.add(\u0026#34;all_projects_operation_log_2024.log\u0026#34;, rotation=\u0026#34;500 MB\u0026#34;, level=\u0026#34;INFO\u0026#34;, compression=\u0026#34;zip\u0026#34;) results = [] for project in projects: logger.info(f\u0026#34;Processing project: {project[\u0026#39;name\u0026#39;]}\u0026#34;) push_bare_repo_to_remote(project, results) write_result_to_json(results, output_file) logger.info(f\u0026#34;Operation results saved to {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() ","date":"2024-11-25T08:00:00Z","permalink":"https://liguobao.github.io/p/gitlab-migration-low-version/","title":"GitLab Low Version Migration Simple Tutorial"},{"content":"Reposted from Zhihu; does not represent personal views.\nOriginal: https://henchat.net/%e5%9c%a8%e6%9c%89%e7%83%ad%e6%ad%a6%e5%99%a8%e5%8e%8b%e5%88%b6%e6%9a%b4%e6%b0%91%e7%9a%84%e7%8e%b0%e4%bb%a3%e5%9b%bd%e5%ae%b6%e7%bb%93%e6%9d%9f%e5%a5%b4%e9%9a%b6%e5%88%b6/\nSummary translation:\nModern slavery would end under atomized rebellion. Atomized rebellion means no program, no organization, no demands â€” just individuals taking random retaliatory action against society, attacking anyone they deem an â€œenemyâ€, even themselves. Each person becomes a oneâ€‘person insurgency, unconcerned with outcomes, focused on maximizing disruption.\nFirepower is designed to suppress organized uprisings, not scattered individuals. Atomized rebellion is far costlier to contain than a collective one of the same scale. Ten thousand people as one army can be crushed by a modern military; ten thousand individuals acting at random in ten thousand places at random times against random targets overwhelm policing capacity. This is why modern rulers fear security wars more than frontal wars.\nInformation control (mass surveillance) preempts organized plots, but it struggles to predict spontaneous loneâ€‘actor violence. Cameras canâ€™t read intentions. Ban guns and they use knives; ban knives and they use cars; remove stones and they use bare hands. The insurgent is always on offense; the ruler is always reactive.\nMoral condemnation also fails: atomized rebels may lack ideology and ethics, harming innocents without seeking to help or liberate anyone. Thereâ€™s no faction to divide or negotiate with. You look around â€” everyone seems ordinary; you hold the sword but donâ€™t know whom to punish or protect.\nThe principle stands: as rulers centralize slaves for exploitation, they fall to collective revolt; as they atomize slaves, they fall to atomized revolt. Like microbes that thrive as society grows pathological, such rebels spread with entropy that artillery canâ€™t kill.\nIn the new era, â€œrebellionâ€ is about disorder, not ideological opposition. Rebels need not target rulers; targeting reachable bystanders already undermines the systemâ€™s foundation because both slavery and contract societies rest on shared belief. Each â€œsocialâ€‘revengeâ€ incident increases disorder, pushing society toward the law of the jungle. As random aggression grows and individuals move like Brownian motion, the rules can no longer govern an everâ€‘entropic world â€” marking the end of slavery.\n","date":"2024-11-14T23:09:00Z","permalink":"https://liguobao.github.io/p/how-to-end-sl-avery-in-a-modern-country-where-there-are/","title":"In Modern States with Firepower to Suppress Mobs, What Conditions End Slavery?"},{"content":"On Linux or macOS, use du to get the size of each first-level subdirectory in the current folder:\n1 du -h --max-depth=1 -h: human-readable units (KB, MB, GB) --max-depth=1: only list first-level subdirectories To view only the total size of the current directory:\n1 du -sh . -s: show only the total summary These commands help you quickly understand sizes of subfolders in the current directory.\n","date":"2024-08-07T08:00:00Z","permalink":"https://liguobao.github.io/p/linux-du/","title":"Linux du Command"},{"content":"npm.taobao.org https Certificate Expiration This thing has been expired for more than a day or two, all who used this mirror source will directly report error. Recently because of checking build failure issue, tinkered for several hours before realizing it\u0026rsquo;s certificate expired. At first thought it was my network problem, later found it\u0026rsquo;s the source problem. Although this domain has been CNAME to registry.npmmirror.com, however when using npm, it won\u0026rsquo;t automatically jump to this new domain.\nSo, still build failure.\nSolution 1 2 npm config set registry registry.npmmirror.com npm config set registry registry.npmjs.org If project root has .npmrc file, remember to change this file.\n1 registry=https://registry.npmmirror.com Remember Don\u0026rsquo;t overly rely on certain vendor\u0026rsquo;s services, especially domestic ones. Sometimes, better to directly use official source. ","date":"2024-08-06T08:00:00Z","image":"https://liguobao.github.io/zh/p/npm-taobao-org-exp/npm_hu_b554f7cef0ad70f3.png","permalink":"https://liguobao.github.io/p/npm-taobao-org-exp/","title":"npm.taobao.org Taobao Mirror Source Certificate Expiration Solution"},{"content":"Overall Strategy Answers first Ops team at the core; product+engineering driven Creatorâ€‘centric; creation is king Zhihu positions as a deepâ€‘content community Product/Engineering Focus Treat moderators/answerers like community owners Recommendation Algorithm Updates Coldâ€‘start optimization Objective shift for ranking Community governance (reduce flamebait/opposition; ~2% comments removed) Health of the ecosystem; encourage professional verticals Past: PV/UV and traffic were king. Now: depth and interactions matter more.\nHot List Experience Reduce confrontational topics Content ops strategy tweaks Product: remove â€œcollapseâ€, add â€œfeaturedâ€ Question page ranking: suppress low quality; optimize TPR True Positive Rate True Positive Rate (TPR, recall/sensitivity) measures how well a classifier identifies positives: TPR = TP / (TP + FN). TPR ranking compares configurations/models by their TPR to choose better settings. In contexts like screening or fraud detection, high TPR is crucial.\nUser Communication Feedback loops AI Content Stance Opinion extraction, similarity, quality, trust Plagiarism detection and enforcement Antiâ€‘scraping (SEO shielding?) AIGC/Spam detection Actions: restrict distribution; penalize malicious accounts. For Zhihuâ€™s own â€œAI direct answersâ€: what sources were quoted to form the answer?\nAuthor Experience Good Questions Asking is somewhat deâ€‘emphasized How are duplicates handled? Antiâ€‘cheat for mass question spammers? Notifications Improvements to reminders Columns Upgrades, exposure, circulation, paid support What about Zhihu Daily? High SEO and content quality (reopen when?) AI Search / Direct Answers TBD ","date":"2024-07-28T08:00:00Z","permalink":"https://liguobao.github.io/p/zhihu-salon-0728/","title":"Zhihu Salon â€” 2024-07-28 Notes"},{"content":"Configuring Git Bash in Visual Studio 2022\nNo need for more words Reference: https://stackoverflow.com/questions/62109412/how-can-i-use-a-git-bash-terminal-profile-in-visual-studio-2019\nShell Location: C:\\Program Files\\Git\\bin\\sh.exe\nPath:\nC:\\Program Files\\Git\\bin\\sh.exe\nIf your installation directory is different, adjust accordingly.\nArguments:\n\u0026ndash;login -i -l\nFinal result.\n","date":"2024-07-27T08:00:00Z","image":"https://liguobao.github.io/zh/p/vs-git-bash/bash_hu_2b7c3f3d3912770d.jpg","permalink":"https://liguobao.github.io/p/vs-git-bash/","title":"Add Git Bash to Terminal in Visual Studio"},{"content":"Requirements Previously implemented a hardware info interface in C++, using hwinfo, roughly returns the following hardware info.\n1 { \u0026#34;cpuModel\u0026#34;: \u0026#34;12th Gen Intel(R) Core(TM) i7-12700K\u0026#34;, \u0026#34;gpuName\u0026#34;: \u0026#34;Microsoft Remote Display Adapter | NVIDIA GeForce RTX 3070\u0026#34;, \u0026#34;ramVendor\u0026#34;: \u0026#34;A-DATA Technology\u0026#34;, \u0026#34;ramSerialNumber\u0026#34;: \u0026#34;CC500\u0026#34;, \u0026#34;ramTotalSizeBytes\u0026#34;: 34359740000, \u0026#34;ramFreeSizeBytes\u0026#34;: -1, \u0026#34;mainBoardVendor\u0026#34;: \u0026#34;Rev 1.xx\u0026#34;, \u0026#34;mainBoardName\u0026#34;: \u0026#34;PRIME Z690-P D4\u0026#34;, \u0026#34;mainBoardSerialNumber\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;disks\u0026#34;: [ { \u0026#34;model\u0026#34;: \u0026#34;ST12000VN0008-2YS101\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34; ZRT0DJ\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;12000132426240\u0026#34;, \u0026#34;freeSize\u0026#34;: \u0026#34;-1\u0026#34; }, { \u0026#34;model\u0026#34;: \u0026#34;Samsung SSD 980 1TB\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34;0025.\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;1000202273280\u0026#34;, \u0026#34;freeSize\u0026#34;: \u0026#34;-1\u0026#34; }, { \u0026#34;model\u0026#34;: \u0026#34;WDC WD10EZEX-00BBHA0\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34; WD-WATM2266\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;1000202273280\u0026#34;, \u0026#34;freeSize\u0026#34;: \u0026#34;-1\u0026#34; } ], \u0026#34;osOperatingSystem\u0026#34;: \u0026#34;19044\u0026#34;, \u0026#34;osShortName\u0026#34;: \u0026#34;Windows\u0026#34;, \u0026#34;osArchitecture\u0026#34;: \u0026#34;32 bit\u0026#34;, \u0026#34;diskTotalSize\u0026#34;: 14000536972800, \u0026#34;logicDiskTotalFreeSpace\u0026#34;: 0 } The code is nothing special, basically calling libraries, hwinfo\u0026rsquo;s examples are well written~\nBut a few days ago found this thing occasionally crashes, error 0000005, should be memory access error, probably null reference triggered.\nLong ago fixed a string encoding bug for this library, later the maintainer upgraded several versions, I was too busy to tinker more.\nThought about it, forget it, let\u0026rsquo;s implement one in Python first.\nAsk GPT? Q:\nI want to implement computer hardware info reading in Python3, return the above JSON, how to handle. Help make it compatible, if not on Windows, give as much info as possible, for missing info give default values. What are the corresponding library versions? Give me requirements A:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 import psutil import cpuinfo import GPUtil import platform import json def get_cpu_info(): try: cpu = cpuinfo.get_cpu_info() return cpu[\u0026#39;brand_raw\u0026#39;] except Exception as e: return f\u0026#34;Unknown CPU ({e})\u0026#34; def get_gpu_info(): try: gpus = GPUtil.getGPUs() return \u0026#34; | \u0026#34;.join([gpu.name for gpu in gpus]) except Exception as e: return f\u0026#34;Unknown GPU ({e})\u0026#34; def get_ram_info(): try: if platform.system() == \u0026#34;Windows\u0026#34;: import wmi c = wmi.WMI() for memory in c.Win32_PhysicalMemory(): ram_vendor = memory.Manufacturer ram_serial_number = memory.SerialNumber ram_total_size = int(memory.Capacity) ram_free_size = psutil.virtual_memory().available return ram_vendor, ram_serial_number, ram_total_size, ram_free_size else: vm = psutil.virtual_memory() return \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Serial\u0026#34;, vm.total, vm.available except Exception as e: return \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Serial\u0026#34;, -1, -1 def get_mainboard_info(): try: if platform.system() == \u0026#34;Windows\u0026#34;: import wmi c = wmi.WMI() for board in c.Win32_BaseBoard(): mainboard_vendor = board.Manufacturer mainboard_name = board.Product mainboard_serial = board.SerialNumber return mainboard_vendor, mainboard_name, mainboard_serial else: return \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Board\u0026#34;, \u0026#34;Unknown Serial\u0026#34; except Exception as e: return \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Board\u0026#34;, f\u0026#34;Unknown Serial ({e})\u0026#34; def get_disk_info(): try: if platform.system() == \u0026#34;Windows\u0026#34;: import wmi c = wmi.WMI() disks = [] for disk in c.Win32_DiskDrive(): model = disk.Model serial_number = disk.SerialNumber.strip() size = int(disk.Size) disks.append({ \u0026#34;model\u0026#34;: model, \u0026#34;serialNumber\u0026#34;: serial_number, \u0026#34;size\u0026#34;: size, \u0026#34;freeSize\u0026#34;: -1 }) return disks else: disks = [] for disk in psutil.disk_partitions(): usage = psutil.disk_usage(disk.mountpoint) disks.append({ \u0026#34;model\u0026#34;: \u0026#34;Unknown Model\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34;Unknown Serial\u0026#34;, \u0026#34;size\u0026#34;: usage.total, \u0026#34;freeSize\u0026#34;: usage.free }) return disks except Exception as e: return [{\u0026#34;model\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;size\u0026#34;: -1, \u0026#34;freeSize\u0026#34;: -1}] def get_os_info(): try: os_version = platform.version() os_short_name = platform.system() os_arch = platform.architecture()[0] return os_version, os_short_name, os_arch except Exception as e: return \u0026#34;Unknown\u0026#34;, \u0026#34;Unknown\u0026#34;, \u0026#34;Unknown\u0026#34; def get_total_disk_size_and_free_space(): try: total_size = 0 total_free_space = 0 for partition in psutil.disk_partitions(): try: usage = psutil.disk_usage(partition.mountpoint) total_size += usage.total total_free_space += usage.free except PermissionError: continue # Avoid errors from disks without access permission return total_size, total_free_space except Exception as e: return -1, -1 def get_hardware_info(): cpu_model = get_cpu_info() gpu_name = get_gpu_info() ram_vendor, ram_serial_number, ram_total_size, ram_free_size = get_ram_info() mainboard_vendor, mainboard_name, mainboard_serial_number = get_mainboard_info() disks = get_disk_info() os_version, os_short_name, os_arch = get_os_info() total_disk_size, total_free_space = get_total_disk_size_and_free_space() return { \u0026#34;cpuModel\u0026#34;: cpu_model, \u0026#34;gpuName\u0026#34;: gpu_name, \u0026#34;ramVendor\u0026#34;: ram_vendor, \u0026#34;ramSerialNumber\u0026#34;: ram_serial_number, \u0026#34;ramTotalSizeBytes\u0026#34;: ram_total_size, \u0026#34;ramFreeSizeBytes\u0026#34;: ram_free_size, \u0026#34;mainBoardVendor\u0026#34;: mainboard_vendor, \u0026#34;mainBoardName\u0026#34;: mainboard_name, \u0026#34;mainBoardSerialNumber\u0026#34;: mainboard_serial_number, \u0026#34;disks\u0026#34;: disks, \u0026#34;osOperatingSystem\u0026#34;: os_version, \u0026#34;osShortName\u0026#34;: os_short_name, \u0026#34;osArchitecture\u0026#34;: os_arch, \u0026#34;diskTotalSize\u0026#34;: total_disk_size, \u0026#34;logicDiskTotalFreeSpace\u0026#34;: total_free_space } if __name__ == \u0026#34;__main__\u0026#34;: hardware_info = get_hardware_info() print(json.dumps(hardware_info, indent=2)) Optimization get_hardware_info function is slow, can optimize? Some info after getting should be cacheable I use loguru library, please add some logs to confirm call time My logs use stdout, no need to write file; can lru_cache set expiration time? get_cpu_info is too slow 2024-07-05 16:24:16.370 | INFO | main:get_cpu_info:54 - get_cpu_info() took 1.1144 seconds\nWhy is cpu = cpuinfo.get_cpu_info() so slow, are there other implementations?\nFinal code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import psutil import GPUtil import platform import json import time import subprocess from functools import wraps from loguru import logger from datetime import datetime, timedelta # Configure loguru to output logs to stdout logger.remove() logger.add(lambda msg: print(msg, end=\u0026#39;\u0026#39;)) # Custom cache decorator def timed_lru_cache(seconds: int, maxsize: int = 128): def wrapper(func): cache = {} cache_expiration = {} @wraps(func) def inner(*args): now = datetime.now() # Clear expired cache for key in list(cache_expiration): if cache_expiration[key] \u0026lt; now: cache.pop(key, None) cache_expiration.pop(key, None) if args in cache: return cache[args] result = func(*args) cache[args] = result cache_expiration[args] = now + timedelta(seconds=seconds) if len(cache) \u0026gt; maxsize: oldest = min(cache_expiration, key=cache_expiration.get) cache.pop(oldest, None) cache_expiration.pop(oldest, None) return result return inner return wrapper @timed_lru_cache(60) def get_cpu_info(): start_time = time.time() try: if platform.system() == \u0026#34;Windows\u0026#34;: result = subprocess.run([\u0026#39;wmic\u0026#39;, \u0026#39;cpu\u0026#39;, \u0026#39;get\u0026#39;, \u0026#39;Name\u0026#39;], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) cpu_model = result.stdout.split(\u0026#39;\\n\u0026#39;)[1].strip() elif platform.system() == \u0026#34;Linux\u0026#34;: result = subprocess.run([\u0026#39;lscpu\u0026#39;], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) for line in result.stdout.split(\u0026#39;\\n\u0026#39;): if \u0026#39;Model name\u0026#39; in line: cpu_model = line.split(\u0026#39;:\u0026#39;)[1].strip() break elif platform.system() == \u0026#34;Darwin\u0026#34;: result = subprocess.run([\u0026#39;sysctl\u0026#39;, \u0026#39;-n\u0026#39;, \u0026#39;machdep.cpu.brand_string\u0026#39;], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) cpu_model = result.stdout.strip() else: cpu_model = \u0026#34;Unsupported platform\u0026#34; except Exception as e: cpu_model = f\u0026#34;Unknown CPU ({e})\u0026#34; end_time = time.time() logger.info(f\u0026#34;get_cpu_info() took {end_time - start_time:.3f} seconds\u0026#34;) return cpu_model @timed_lru_cache(60) def get_gpu_info(): start_time = time.time() try: gpus = GPUtil.getGPUs() gpu_name = \u0026#34; | \u0026#34;.join([gpu.name for gpu in gpus]) except Exception as e: gpu_name = f\u0026#34;Unknown GPU ({e})\u0026#34; end_time = time.time() logger.info(f\u0026#34;get_gpu_info() took {end_time - start_time:.3f} seconds\u0026#34;) return gpu_name @timed_lru_cache(60) def get_ram_info(): start_time = time.time() try: if platform.system() == \u0026#34;Windows\u0026#34;: import wmi c = wmi.WMI() for memory in c.Win32_PhysicalMemory(): ram_vendor = memory.Manufacturer ram_serial_number = memory.SerialNumber ram_total_size = int(memory.Capacity) ram_free_size = psutil.virtual_memory().available break else: vm = psutil.virtual_memory() ram_vendor, ram_serial_number, ram_total_size, ram_free_size = \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Serial\u0026#34;, vm.total, vm.available except Exception as e: ram_vendor, ram_serial_number, ram_total_size, ram_free_size = \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Serial\u0026#34;, -1, -1 end_time = time.time() logger.info(f\u0026#34;get_ram_info() took {end_time - start_time:.3f} seconds\u0026#34;) return ram_vendor, ram_serial_number, ram_total_size, ram_free_size @timed_lru_cache(60) def get_mainboard_info(): start_time = time.time() try: if platform.system() == \u0026#34;Windows\u0026#34;: import wmi c = wmi.WMI() for board in c.Win32_BaseBoard(): mainboard_vendor = board.Manufacturer mainboard_name = board.Product mainboard_serial = board.SerialNumber break else: mainboard_vendor, mainboard_name, mainboard_serial = \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Board\u0026#34;, \u0026#34;Unknown Serial\u0026#34; except Exception as e: mainboard_vendor, mainboard_name, mainboard_serial = \u0026#34;Unknown Vendor\u0026#34;, \u0026#34;Unknown Board\u0026#34;, f\u0026#34;Unknown Serial ({e})\u0026#34; end_time = time.time() logger.info(f\u0026#34;get_mainboard_info() took {end_time - start_time:.3f} seconds\u0026#34;) return mainboard_vendor, mainboard_name, mainboard_serial @timed_lru_cache(60) def get_disk_info(): start_time = time.time() try: if platform.system() == \u0026#34;Windows\u0026#34;: import wmi c = wmi.WMI() disks = [] for disk in c.Win32_DiskDrive(): model = disk.Model serial_number = disk.SerialNumber.strip() size = int(disk.Size) disks.append({ \u0026#34;model\u0026#34;: model, \u0026#34;serialNumber\u0026#34;: serial_number, \u0026#34;size\u0026#34;: size, \u0026#34;freeSize\u0026#34;: -1 }) else: disks = [] for disk in psutil.disk_partitions(): usage = psutil.disk_usage(disk.mountpoint) disks.append({ \u0026#34;model\u0026#34;: \u0026#34;Unknown Model\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34;Unknown Serial\u0026#34;, \u0026#34;size\u0026#34;: usage.total, \u0026#34;freeSize\u0026#34;: usage.free }) except Exception as e: disks = [{\u0026#34;model\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;serialNumber\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;size\u0026#34;: -1, \u0026#34;freeSize\u0026#34;: -1}] end_time = time.time() logger.info(f\u0026#34;get_disk_info() took {end_time - start_time:.3f} seconds\u0026#34;) return disks @timed_lru_cache(60) def get_os_info(): start_time = time.time() try: os_version = platform.version() os_short_name = platform.system() os_arch = platform.architecture()[0] except Exception as e: os_version, os_short_name, os_arch = \u0026#34;Unknown\u0026#34;, \u0026#34;Unknown\u0026#34;, \u0026#34;Unknown\u0026#34; end_time = time.time() logger.info(f\u0026#34;get_os_info() took {end_time - start_time:.3f} seconds\u0026#34;) return os_version, os_short_name, os_arch @timed_lru_cache(60) def get_total_disk_size_and_free_space(): start_time = time.time() try: total_size = 0 total_free_space = 0 for partition in psutil.disk_partitions(): try: usage = psutil.disk_usage(partition.mountpoint) total_size += usage.total total_free_space += usage.free except PermissionError: continue # Avoid errors from disks without access permission except Exception as e: total_size, total_free_space = -1, -1 end_time = time.time() logger.info(f\u0026#34;get_total_disk_size_and_free_space() took {end_time - start_time:.3f} seconds\u0026#34;) return total_size, total_free_space def get_hardware_info(): ","date":"2024-07-08T08:00:00Z","image":"https://liguobao.github.io/zh/p/how-to-get-hwinfo-by-python/logo_hu_2b170d60a8edc426.png","permalink":"https://liguobao.github.io/p/how-to-get-hwinfo-by-python/","title":"Get Computer Hardware Information via Python, Including CPU, GPU, Memory, Motherboard, Hard Disk, OS, etc."},{"content":"Preface Don\u0026rsquo;t ask why to do this. If you\u0026rsquo;re reading this article and interested in continuing, then you have this need too.\nWhat is Magisk Introduction Magisk is a suite of open source software for customizing Android, supporting devices higher than Android 6.0.\nSome highlight features:\nMagiskSU: Provide root access for applications Magisk Modules: Modify read-only partitions by installing modules MagiskBoot: The most complete tool for unpacking and repacking Android boot images Zygisk: Run code in every Android applications\u0026rsquo; processes Powered by Magisk\nOperation Steps Preview Unlock BL Install Magisk App Download corresponding system version ROM package and extract boot.img Use Magisk to modify boot.img Flash the modified boot.img Install Magisk modules Enjoy the \u0026ldquo;free new world\u0026rdquo; Unlock BL Each has its own way, handle it yourself. Xiaomi devices can refer to Xiaomi Unlock BL OnePlus devices can refer to OnePlus Amu Other devices please search yourself In 2024, the best device to unlock is OnePlus. Others are inferior.\n(Originally considered buying Xiaomi 14, but BL lock deterred a 10-year Xiaomi fan)\nInstall Magisk App Download Magisk/releases After downloading, install on your phone.\n1 adb install Magisk-v26.4.apk Extract boot.img Download corresponding system version ROM package and extract boot.img If it\u0026rsquo;s a flash package, directly unzip and find boot.img\nIf it\u0026rsquo;s a line brush package, unzip to get payload.bin, need to use tools to extract.\nOn Windows, use FastbootEnhance tool; on Mac/Linux, use payload_dumper tool.\nBasically similar.\nUse Magisk to Generate Patched boot.img Magisk App -\u0026gt; Install -\u0026gt; Install -\u0026gt; Select and Patch a File -\u0026gt; Select boot.img Wait for patched boot.img to be generated, automatically saved to /sdcard/Download/magisk_patched-xxx.img Flash the Modified boot.img Phone enters Fastboot mode, connect to computer 1 2 3 4 5 # Check if device is connected successfully fastboot devices # Flash the modified boot.img fastboot flash boot /sdcard/Download/magisk_patched-xxx.img After flashing, reboot phone.\nActivate Magisk After rebooting, reopen Magisk App, first time should prompt to start repair.\nNormally, after repair completes, phone will reboot, and after reboot, it\u0026rsquo;s done.\nInstall Magisk Modules Download Magisk modules, install locally. The general process is basically this.\nCommon Commands List 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Enter Fastboot mode adb reboot bootloader # Check if fastboot device is connected successfully fastboot devices # Unlock fastboot oem unlock # Unlock BL for other devices fastboot flashing unlock # Flash boot.img fastboot flash boot ./Download/magisk_patched-xxx.img # Reboot fastboot reboot LineageOS Build Archive Download LineageOS Old Versions LineageOS Latest Versions LineageOS Set Network Detection Address Change captive connection verification server\n1 2 3 4 adb shell settings delete global captive_portal_https_url adb shell settings delete global captive_portal_http_url adb shell settings put global captive_portal_https_url https://connect.rom.miui.com/generate_204 adb shell settings put global captive_portal_http_url http://connect.rom.miui.com/generate_204 ","date":"2024-07-05T08:00:00Z","image":"https://liguobao.github.io/zh/p/android-magisk-install/logo_hu_4faf4a02431817ba.png","permalink":"https://liguobao.github.io/p/android-magisk-install/","title":"Start Your Android Tinkering Journey with Magisk"},{"content":"Preface Around last year or the year before, after switching phone to iOS, no Xiaoai classmate anymore, using Mi Home App to control Mi Home devices was really painful. So at that time, I tinkered with HomeAssistant solution, and wrote such an article.\nZhihu Column: Home Assistant Smart Home System Setup Guide (Mi Home + Siri)\nThis solution with HomePod speaker basically met my needs. Later, home basically had various devices, HomePod was basically the whole house central control.\nHome Assistant running on home host 24/7, basically no issues, so didn\u0026rsquo;t tinker for a long time.\nAnother \u0026ldquo;New Requirement\u0026rdquo; At the beginning of the year, a big shot contacted me, asking if based on current Mi Home or other hardware devices, can do apartment smart home solution.\nBased on the above experience, I gave some suggestions, but didn\u0026rsquo;t research deeply.\nAt that time, let a certain 20-year teacher tinker with DIY control Mi Home devices solution, this friend\u0026rsquo;s progress not ideal, finally no results.\nJust at that time knew there is a github.com/rytilahti/python-miio good tool.\nPython library \u0026amp; console tool for controlling Xiaomi smart appliances A Python library and console tool for controlling Xiaomi smart home devices Interesting, Mark it.\nRecent Practice Recently, again because of some strange requirements, need to \u0026ldquo;seriously\u0026rdquo; tinker with Mi Home device control solution.\nEarliest when doing solution verification, even used Appium to simulate operating Mi Home App, but lag, slow, unstable\u0026hellip;\nTo these two weeks entering formal practice stage, decided to use python-miio, although Hack, but definitely more stable than Appium.\nSo, let\u0026rsquo;s start.\nGetting Started with python-miio First, install python-miio locally\n1 2 3 4 5 6 7 # Remember, do not directly install pip install python-miio # Default Release version is 0.5.12, 2022 version pip install python-miio==0.6.0.dev0 # Or directly install latest version from Github pip install git+https://github.com/rytilahti/python-miio.git After installing, local command line should have miiocli command line tool.\n1 2 3 4 5 6 7 8 âœ ~ miiocli --help Usage: miiocli [OPTIONS] COMMAND [ARGS]... Options: -d, --debug -o, --output [default|json|json_pretty] --version Show the version and exit. --help Show this message and exit. Normally, we should be able to control our Mi Home devices through miiocli.\nFirst, we need to find our device\u0026rsquo;s IP address and Token.\nAll subsequent operations are based on these two pieces of information.\nGet Device IP Address and Token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 âœ ~ miiocli cloud Username: your Xiaomi account Password: your password == Xiaomi Smart Camera C400 (Device online ) == Model: chuangmi.camera.039a01 Token: XXX IP: 192.168.31.170 (mac: 94:F8:27:05:5B:AA) DID: 1029838640 Locale: cn == Xiaomi Smart Camera (Device online ) == Model: chuangmi.camera.029a02 Token: XXX IP: 192.168.31.233 (mac: 60:7E:A4:C0:E6:26) DID: 525571884 Locale: cn == Smart Plug-Ubuntu (Device online ) == Model: chuangmi.plug.212a01 Token: XXX IP: 192.168.31.141 (mac: 58:B6:23:EB:11:EF) DID: 433609370 Locale: cn Here basically can see devices under your account, then we can control these devices through miiocli commands.\nControl Devices 1 2 3 4 5 6 7 8 9 # This is a camera device âœ ~ miiocli device --ip 192.168.31.170 --token xxxx info Running command info Model: chuangmi.camera.039a01 Hardware version: Linux Firmware version: 5.1.6_0420 Supported using: GenericMiot Command: miiocli genericmiot --ip 192.168.31.170 --token xxxx Supported by genericmiot: True Here need to pay attention to Supported by genericmiot, if have this, basically can control this device through miiocli genericmiot.\n1 2 3 4 5 6 # Supported by genericmiot: True Note that the command field which gives you the direct command to use for controlling the device. If the device is supported by the genericmiot integration as stated in the output, you can also use miiocli genericmiot for controlling it. The following documentation excerpted and translated from official docs.\nControl Modern (MIoT) Devices Most modern (MIoT) devices will automatically be supported by genericmiot integration.\nInternally, it uses (\u0026ldquo;miot spec\u0026rdquo;) files to understand supported features like sensors, settings, and actions.\nhttps://home.miot-spec.com/spec/xiaomi.controller.86v1 Mi Home Product Library (Unofficial) This device model specific file will be downloaded (and cached locally) when you first use genericmiot integration.\nAll supported device features can be controlled using common commands: status (show device status), set (change settings), actions list available actions, and call execute actions.\nDevice Status Executing status will show current device status, and acceptable values for settings (marked with access RW):\n1 2 3 4 5 6 miiocli genericmiot --ip 127.0.0.1 --token 00000000000000000000000000000000 status Service Light (light) Switch Status (light:on, access: RW): False (\u0026lt;class \u0026#39;bool\u0026#39;\u0026gt;, ) Brightness (light:brightness, access: RW): 60 % (\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, min: 1, max: 100, step: 1) Power Off Delay Time (light:off-delay-time, access: RW): 1:47:00 (\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, min: 0, max: 120, step: 1) Change Settings To change settings, you need to provide the setting name (e.g., in the above example light:brightness):\n1 2 3 miiocli genericmiot --ip 127.0.0.1 --token 00000000000000000000000000000000 set light:brightness 0 [{\u0026#39;did\u0026#39;: \u0026#39;light:brightness\u0026#39;, \u0026#39;siid\u0026#39;: 2, \u0026#39;piid\u0026#39;: 3, \u0026#39;code\u0026#39;: 0}] Use Actions Most devices will also provide actions:\n1 2 3 4 5 6 miiocli genericmiot --ip 127.0.0.1 --token 00000000000000000000000000000000 actions Light (light) light:toggle Toggle light:brightness-down Brightness Down light:brightness-up Brightness Up These can be executed using the call command:\n1 2 3 miiocli genericmiot --ip 127.0.0.1 --token 00000000000000000000000000000000 call light:toggle {\u0026#39;code\u0026#39;: 0, \u0026#39;out\u0026#39;: []} Use miiocli genericmiot \u0026ndash;help for more available commands.\nDetailed docs here:\nhttps://github.com/rytilahti/python-miio?tab=readme-ov-file#controlling-modern-miot-devices Control Your Mi Home Devices with One Line of Code Based on the above operations, this one line of code came out.\n1 2 3 4 5 6 7 8 # Get executable commands miiocli genericmiot --ip 192.168.31.170 --token xxxx actions ## Execute action miiocli genericmiot --ip 192.168.31.170 --token xxxx call camera:record-start ## Change property miiocli genericmiot --ip 192.168.31.170 --token xxxx set camera:record-mode 1 Of course if you need to write some code yourself, it\u0026rsquo;s like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from miio import Device, DeviceFactory, DeviceStatus from loguru import logger class MiIotDevice(): def __init__(self, ip, token) -\u0026gt; None: self.device: Device = DeviceFactory.create(ip, token) self.device_info = self.get_info() logger.info(f\u0026#34;Device: {ip}-{token} created\u0026#34;) def load_status(self): if self.device is None: return None d_status: DeviceStatus = self.device.status() return d_status.data def get_info(self): return { \u0026#34;ip\u0026#34;: self.device.ip, \u0026#34;model\u0026#34;: self.device.model, \u0026#34;device_id\u0026#34;: \u0026#34;nx_\u0026#34;+str(self.device.device_id), \u0026#34;mi_device_id\u0026#34;: self.device.device_id } def call_action(self, name, args): if self.device is None: raise Exception(\u0026#34;Device not ready\u0026#34;) call_result = self.device.call_action(name, args) logger.info( f\u0026#34;call_action: {name}-{args} result: {call_result}\u0026#34;) return call_result def set_properties(self, properties): if self.device is None: raise Exception(\u0026#34;Device not ready\u0026#34;) call_result = self.device.send(\u0026#34;set_properties\u0026#34;, properties) logger.info( f\u0026#34;set_properties: {properties} finish, result: {call_result}\u0026#34;) return call_result Basically, as above.\nSummary python-miio is a good tool for controlling Mi Home devices\nThrough miiocli can conveniently control devices Through miiocli genericmiot can control modern devices Through miiocli genericmiot actions can view device supported actions Through miiocli genericmiot status can view device status Through miiocli genericmiot set can change device properties Through miiocli genericmiot call can execute device actions Through miiocli genericmiot \u0026ndash;help can view more commands At the same time can write code to tinker with specific devices, making some automation things also simple.\nWish everyone have fun~\nReferences python-miio miot-spec Mi Home Device Properties Query (Unofficial) ","date":"2024-06-27T08:00:00Z","image":"https://liguobao.github.io/zh/p/one-line-code-control-mi-iot-diy/mi-iot-diy_hu_d4ccb214c5fa2dc8.png","permalink":"https://liguobao.github.io/p/one-line-code-control-mi-iot-diy/","title":"Control Your Mi Home Devices with One Line of Code"},{"content":"WebCamera â€” A WebRTC-Based Peer-to-Peer Webcam Saw a neat open-source project on Twitter/X â€” a P2P webcam built on WebRTC.\nRepo: https://github.com/ShouChenICU/WebCamera Looked through the code and docs â€” interesting enough to deploy and try. Sharing notes below.\nProject Overview WebCamera is a network webcam tool built on WebRTC, developed with Nuxt.js and managed with Yarn. Features Real-time video: WebRTC-based, efficient streams. Cross-platform: Works across browsers and devices. Dev-friendly: Built on Nuxt.js, easy to extend and maintain. Modular design: Easy to integrate and expand. Privacy \u0026amp; security: Peer-to-peer encrypted connections. Docker Deployment Image: ccr.ccs.tencentyun.com/liguobao/tools:web-camera This is my built image hosted on Tencent Cloud Container Registry. Build it yourself:\n1 2 docker build -t webcamera . docker run -d -p 3000:3000 webcamera k8s Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: web-camera labels: app: web-camera spec: replicas: 1 selector: matchLabels: app: web-camera template: metadata: labels: app: web-camera spec: containers: - name: web-camera image: ccr.ccs.tencentyun.com/liguobao/tools:web-camera ports: - containerPort: 3000 resources: requests: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;2048Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; readinessProbe: httpGet: path: / port: 3000 initialDelaySeconds: 120 periodSeconds: 60 env: - name: DOCKER_ENABLE_SECURITY value: \u0026#34;false\u0026#34; dnsPolicy: ClusterFirst restartPolicy: Always terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: name: web-camera spec: selector: app: web-camera ports: - protocol: TCP port: 3000 targetPort: 3000 type: ClusterIP --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: cert-manager.io/cluster-issuer: letsencrypt-prod kubernetes.io/tls-acme: \u0026#34;true\u0026#34; generation: 1 name: ingress-web-camera spec: ingressClassName: traefik rules: - host: xxx.yyy.cn http: paths: - backend: service: name: web-camera port: number: 3000 path: / pathType: Prefix tls: - hosts: - xxx.yyy.cn secretName: xxx-yyy-cn-tls Whatâ€™s Next Chatted with the author â€” there might be a simple online meeting app coming next, worth keeping an eye on.\nReferences WebCamera: https://github.com/ShouChenICU/WebCamera WebRTC: https://webrtc.org/?hl=zh-cn Ruanyifengâ€™s WebRTC docs (Chinese): https://javascript.ruanyifeng.com/htmlapi/webrtc.html WebRTC is quite interesting â€” worth exploring if youâ€™re curious. The author said they only studied it for a couple of days before shipping this.\n","date":"2024-06-12T08:00:00Z","image":"https://liguobao.github.io/zh/p/open-source-web-camera/web-c_hu_4da7fac45bdbd69a.png","permalink":"https://liguobao.github.io/p/open-source-web-camera/","title":"WebCamera â€” A WebRTC-Based Peer-to-Peer Webcam"},{"content":"The C# plugin in VS Code seems abnormal Recently started a new C# project, thinking of dividing multiple projects by folders, putting a solution in the root directory. But when opening in VS Code, the reference information and hints in the project source code are not normal. Confused, I\u0026rsquo;ve always used it this way, shouldn\u0026rsquo;t there be any problem.\nSearched and found that some friends also encountered similar problems.\n[VS Code] Fix C# code F12 invalid VSCode C# Go to Definition (F12) and Ctrl+Left Click Invalid Solution Basically, the OmniSharp plugin is not working properly, need to specify manually. But\u0026hellip;\nomnisharp-vscode: This repository has been archived by the owner on Jun 22, 2023. It is now read-only.\nNaturally, this method is useless.\nHowever.\ndotnet/vscode-csharp is now officially supported.\nA Visual Studio Code extension that provides rich language support for C# and is shipped along with C# Dev Kit. Powered by a Language Server Protocol (LSP) server, this extension integrates with open source components like Roslyn and Razor to provide rich type information and a faster, more reliable C# experience.\nSo, does that mean I should install this thing?\nC# Dev Kit\nFinally verified, it\u0026rsquo;s indeed like this. The C# Dev Kit plugin supports loading sln, after installing, manually confirm which sln file. Everything back to normal.\nDone.\n","date":"2024-06-10T08:00:00Z","image":"https://liguobao.github.io/zh/p/vscode-support-csharp-sln/code_hu_8262bb9eaf7719fa.png","permalink":"https://liguobao.github.io/p/vscode-support-csharp-sln/","title":"VS Code Support for C# Solution Files"},{"content":"Background: API Gateway Is Going Away Heard Tencent Cloudâ€™s API Gateway is being sunset. Some of our projects were (unfortunately) built on it. What to replace it with now?\nImportant: API Gateway product end-of-sale announcement\nBack when I was still using Tencent Cloudâ€™s managed k8s masters for free, after one upgrade they switched our Services to API Gateway behind the scenes and started billing by usage.\nSo I began looking into how to remove and replace it â€” and today that work finally pays off.\nOption 1: Tencent Cloud k8s + NodePort Keep the same cluster; join your own machines as worker nodes. Remove all Services bound to internal IPs; expose everything via NodePort. Install NGINX on worker nodes and reverse-proxy traffic to NodePort. This ran for about a year. Then the managed cluster itself started charging a â€œmanagement feeâ€. A few dozen RMB per month, hundreds per year â€” not acceptable for us. Time to migrate again.\nOption 2: SuperEdge self-hosted cluster + NodePort + NGINX For a self-hosted cluster, use whatever is convenient.\nI wrote these back then:\nSuperEdge first impressions (Chinese): https://zhuanlan.zhihu.com/p/346084530 SuperEdge + NFS (Chinese): https://zhuanlan.zhihu.com/p/514678542 I was basically an organic promoter for the project.\nHoweverâ€¦ last year the old SuperEdge WeChat group was migrated; the new group seemed unmaintained. People I knew said the project moved into maintenance mode.\nThe real problem: the certs expired and wouldnâ€™t renew; my cluster broke.\nHonestly, itâ€™s tiring â€” many big-company open-source projects in CN end up tied to KPIs. Hard to get in, harder to keep running, and easy to get dropped.\nSo I started looking again, with these goals:\nStill allâ€‘in on k8s Still selfâ€‘hosted Still zero cost Then I found k3s â€” itâ€™s great.\nFinal Approach: k3s + ingress-nginx Deployment guide: https://zhuanlan.zhihu.com/p/696737105 (Chinese)\nExample node list:\n1 2 3 4 5 6 7 NAME STATUS ROLES AGE VERSION aliyun-bj-199 Ready \u0026lt;none\u0026gt; 34d v1.29.4+k3s1 haru Ready \u0026lt;none\u0026gt; 6d16h v1.29.4+k3s1 t7610 Ready \u0026lt;none\u0026gt; 6d2h v1.29.4+k3s1 vm-16-12-ubuntu Ready control-plane,etcd,master 75d v1.28.7+k3s1 vm-28-17-ubuntu Ready,SchedulingDisabled control-plane,etcd,master 75d v1.28.7+k3s1 zj-hc1 Ready \u0026lt;none\u0026gt; 33d v1.29.4+k3s1 As you can see, there are two masters (vm-16-12-ubuntu and vm-28-17-ubuntu). Theyâ€™re on cloud VMs, mutually reachable on the private network, and both have public IPs (important).\nOther nodes are a mix of local, other clouds, and edge devices. They can talk over private network, but not all have public IPs.\nPreviously, we needed an API Gateway to forward traffic to worker nodes. Thatâ€™s what Tencent Cloudâ€™s API Gateway did, but we donâ€™t have it anymore. Instead, we can use ingress-nginx.\nIn a k3s cluster, any node with a public IP can serve as an entry point.\nWe can configure ingress-nginx to forward traffic to services living on worker nodes.\nAll you need is to pass --node-external-ip when starting the server/agent to set the public IP.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # server ExecStart=/usr/local/bin/k3s \\ server \\ --node-external-ip PUBLIC_IP \\ --flannel-backend wireguard-native \\ \u0026#39;--tls-san\u0026#39; \\ \u0026#39;PUBLIC_IP\u0026#39; \\ \u0026#39;--node-name\u0026#39; \\ \u0026#39;vm-16-12-ubuntu\u0026#39; \\ \u0026#39;--server\u0026#39; \\ \u0026#39;https://10.0.28.17:6443\u0026#39; \\ # agent ExecStart=/usr/local/bin/k3s \\ agent --node-name zj-hc1 \\ --lb-server-port 5443 \\ --node-ip 10.42.4.1 \\ -node-external-ip PUBLIC_IP \\ --server https://server:6443 \\ --token=\u0026#34;YOUR_TOKEN\u0026#34; \\ Now ports 80/443 on that node can accept external traffic (make sure your firewall allows it). Point your domainâ€™s DNS A record(s) to that nodeâ€™s public IP, and youâ€™re good to go.\nA Minimal Working Example Self-signed or ACME certs supported Any node with a public IP can act as an entry point For LB, add multiple A records or use DNS load balancing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 apiVersion: v1 kind: PersistentVolume metadata: name: stirling-pdf spec: capacity: storage: 30Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/cfs-a4nopkhh/stirling-pdf nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - zj-hc1 --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: stirling-pdf spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi storageClassName: local-storage --- apiVersion: apps/v1 kind: Deployment metadata: name: stirling-pdf labels: app: stirling-pdf spec: replicas: 1 selector: matchLabels: app: stirling-pdf template: metadata: labels: app: stirling-pdf spec: containers: - name: stirling-pdf image: ccr.ccs.tencentyun.com/liguobao/s-pdf:ustc ports: - containerPort: 8080 resources: requests: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;2048Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 120 periodSeconds: 60 env: - name: DOCKER_ENABLE_SECURITY value: \u0026#34;false\u0026#34; - name: INSTALL_BOOK_AND_ADVANCED_HTML_OPS value: \u0026#34;false\u0026#34; - name: UI_APP_NAME value: \u0026#34;R2049 PDF\u0026#34; - name: ALLOW_GOOGLE_VISIBILITY value: \u0026#34;true\u0026#34; - name: LANGS value: \u0026#34;zh_CN\u0026#34; - name: APP_LOCALE value: \u0026#34;zh_CN\u0026#34; volumeMounts: - mountPath: /configs subPath: configs name: data - mountPath: /usr/share/tessdata subPath: tessdata name: data - mountPath: /customFiles/ subPath: customFiles name: data volumes: - name: data persistentVolumeClaim: claimName: stirling-pdf imagePullSecrets: - name: regcred dnsPolicy: ClusterFirst restartPolicy: Always terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: name: stirling-pdf-service spec: selector: app: stirling-pdf ports: - protocol: TCP port: 8080 targetPort: 8080 type: ClusterIP --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: cert-manager.io/cluster-issuer: letsencrypt-prod kubernetes.io/tls-acme: \u0026#34;true\u0026#34; generation: 1 name: ingress-pdf-house2048 spec: ingressClassName: traefik rules: - host: pdf.house2048.cn http: paths: - backend: service: name: stirling-pdf-service port: number: 8080 path: / pathType: Prefix tls: - hosts: - pdf.house2048.cn secretName: pdf-house2048-cn-tls ","date":"2024-06-03T08:00:00Z","image":"https://liguobao.github.io/zh/p/q-cloud-api-down-k3s-up/qcloud_hu_35618731670e260d.png","permalink":"https://liguobao.github.io/p/q-cloud-api-down-k3s-up/","title":"Tencent Cloud API Gateway Sunset? Open-Source Cluster Replacement"},{"content":"GPT2077 News~\nGPT2077 Free GPT4 Site~ Domestic high speed, no VPN, safe and efficient.\nSupports GPT3.5, GPT4, paper polishing, pay per use.\nPowered by GPT 2077~\nDomestic network, register with email and activate immediately.\nGPT4, GPT3.5, GPT-4o, Google Gemini-Pro, DALL-E3-Image text to image model all available~\nPaper interpretation, SiriGPT, paper polishing, API calls and more~\nPay per use, as needed.\nDaily check-in gives 100K Token, invite friends each gives 300K Token. Basically free~\nScan QR code to register\nInvite code registration. Welcome to use my invite code [9NBE3XJ9] to register GPT2077, immediately get 500K Token reward (universal for GPT4, 3.5 models).\nClick link to register: https://chat.r2049.cn/user/login?inviteCode=9NBE3XJ9\nGPT4 classic question Q: Lu Xun hit Zhou Shuren, is this normal? Answers from three different models~\n\u0026ldquo;Daily Check-in\u0026rdquo; Entry: Personal info page Check-in Token reward 100K \u0026ldquo;Invite Code\u0026rdquo; Function GPT 2077 Invite Code Current inviter reward 300K Token, new invited user reward 500K Token. Siri GPT How to use GPT conversation in Siri~ GPT3.5 vs GPT4 GPT3.5, GPT4 comparison ","date":"2024-05-29T08:00:00Z","image":"https://liguobao.github.io/zh/p/gpt2077-free-gpt4/girl_hu_b042573a578e1c7e.png","permalink":"https://liguobao.github.io/p/gpt2077-free-gpt4/","title":"GPT2077 Free GPT4 Site~"},{"content":"Spotted an interesting browser extension recently â€” â€œSlax Readerâ€. Worth a quick recommendation.\nAn AI-powered browser extension that generates outlines and highlights key points to enhance your web reading experience.\nChrome Web Store:\nhttps://chromewebstore.google.com/detail/slax-reader/gdnhaajlomjkhahnmiijphnodkcfikfd\nNote: Itâ€™s a Chrome extension â€” handle installation as you see fit. Original post: https://x.com/wulujia/status/1791680166382727541\nSome screenshots and examples below:\nOriginal article (Chinese): â€œWhy we fall in love, and why we partâ€ https://mp.weixin.qq.com/s/r9wSD-Yâ€81My7â€6J_74A5d4g\nAnother Zhihu thread (debate/argument)\nA Python learning article I wrote earlier.\nAt the time of writing, the extension is free; the author mentioned typical usage isnâ€™t heavy. It reportedly uses the OpenAI API, so itâ€™s not exactly â€œcheapâ€ behind the scenes.\nFor quick reads and skimming, the overall experience is solid â€” nice for building an overview.\nPretty neat â€” another way to save time while reading.\n","date":"2024-05-21T08:00:00Z","image":"https://liguobao.github.io/zh/p/slax-reader-ai-mind-map/ai_hu_1c6e9ef2b4f06164.png","permalink":"https://liguobao.github.io/p/slax-reader-ai-mind-map/","title":"Slax Reader An AI-Powered Mind Map Extension"},{"content":"PS: Not working on Labor Day â€” just kidding!\nManual dog head ~\n[Disclaimer on programming technical area]\nOne of our servers was about to expire and the IDC quoted 2500 for renewal. They said the price was the same last year. Fair enough â€” we did pay last year. But this year isnâ€™t easy.\nSo I looked for cloud deals and bought machines quickly. I decided to build a cluster. I chose k3s.\nI had used other edge solutions like SuperEdge before, but those projects stalled. Many open-source projects from large companies in China seem to be KPI-driven and get abandoned. K3s has been more actively maintained by the Rancher team.\nWhy is it called K3s?\nThe joke says we want Kubernetes that uses half the memory. Kubernetes is abbreviated to K8s. Half of 10 letters is 5, so the joke abbreviates it to K3s. K3s doesnâ€™t have an official long name or pronunciation. See the docs: https://docs.k3s.io/zh/.\nSmall but powerful.\nI handed the task to Haige; he spent an afternoon getting the cluster up. We migrated services that night and it took about three and a half hours. We accidentally lost a Redis cache (my mistake), but the cluster went online.\nTwo cheap nodes running:\nvm-16-12-ubuntu Ready 42d v1.28.7+k3s1 vm-28-17-ubuntu Ready 42d v1.28.7+k3s1\nIâ€™ve been busy with GPT2077 recently and left little time for the machines. Now that things are calm and a server renewal came up, I started tinkering again.\nInstallation follows the official docs: https://docs.k3s.io/zh/quick-start.\nExample (China mirror):\n1 2 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\ INSTALL_K3S_MIRROR=cn sh - Notes and gotchas\nAfter running the script, the installer creates a k3s.service and installs the k3s CLI. Whether a node runs as a server or an agent depends on k3s.service configuration. By default the system starts as a server; adjust the service arguments to run as an agent or use other options.\n2) Edge networking My servers are two cloud hosts in the same LAN, so configuration is simple. But edge nodes may sit behind NAT or in other clouds, making direct internal networking impossible.\nWireGuard is a good option for cross-location networking.\nWhat is WireGuard?\nWireGuard is a simple, fast, and secure VPN that uses modern cryptography. It aims to be easier and slimmer than traditional VPNs and runs on many platforms including Linux, Windows, macOS, BSD, iOS, and Android. Reference: https://zhuanlan.zhihu.com/p/108365587\nI first looked at Netmaker (but itâ€™s a paid project with conceptual complexity). Then I found Yang Binâ€™s article about using WireGuard to build K3s across clouds and adopted the approach.\nThe plan:\nUse WireGuard for site-to-site networking Configure k3s Flannel to use wireguard-native Restart and verify the cluster Install WireGuard Debian / Ubuntu 1 2 3 4 5 apt install wireguard resolvconf -y # Enable IPv4 forwarding echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p CentOS 7 1 2 3 4 5 6 7 sudo yum install epel-release elrepo-release -y sudo yum install yum-plugin-elrepo -y sudo yum install kmod-wireguard wireguard-tools systemd-resolved -y # Enable IPv4 forwarding echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p CentOS may need a reboot after kernel module updates.\nk3s Flannel -\u0026gt; wireguard-native Edit the k3s systemd service and restart k3s:\n1 2 3 sudo vi /etc/systemd/system/k3s.service sudo systemctl daemon-reload sudo systemctl restart k3s Example ExecStart snippet for /etc/systemd/system/k3s.service:\n1 2 3 4 5 6 7 8 9 10 11 ExecStart=/usr/local/bin/k3s \\ server \\ --node-external-ip \u0026lt;PUBLIC_IP\u0026gt; \\ --flannel-backend wireguard-native \\ --flannel-external-ip \u0026lt;PUBLIC_IP\u0026gt; \\ \u0026#39;--tls-san\u0026#39; \\ \u0026#39;\u0026lt;PUBLIC_IP\u0026gt;\u0026#39; \\ \u0026#39;--node-name\u0026#39; \\ \u0026#39;vm-16-12-ubuntu\u0026#39; \\ \u0026#39;--server\u0026#39; \\ \u0026#39;https://\u0026lt;INTERNAL_IP\u0026gt;:6443\u0026#39; \\ If there are no errors, restart the service and check logs.\nNext, configure agent nodes (edge nodes).\n1 2 3 4 5 # Agent example k3s agent --node-name zj-hc1 \\ --lb-server-port 5443 \\ --server https://\u0026lt;SERVER_PUBLIC_IP\u0026gt;:6443 \\ --token=\u0026#34;\u0026lt;YOUR_TOKEN\u0026gt;\u0026#34; I used --lb-server-port 5443 because 6443 was taken. This is optional.\nMore agent options: https://docs.rancher.cn/docs/k3s/installation/install-options/agent-config/_index/\nAfter the agent joins, modify /etc/systemd/system/k3s.service on the agent and restart k3s:\n1 2 sudo systemctl daemon-reload sudo systemctl restart k3s Example agent ExecStart core options:\n1 2 3 4 5 6 7 ExecStart=/usr/local/bin/k3s \\ agent --node-name zj-hc2 \\ --lb-server-port 5443 \\ --node-ip 10.42.5.1 \\ --node-external-ip \u0026lt;PUBLIC_IP\u0026gt; \\ --server https://\u0026lt;YOUR_SERVER_IP\u0026gt;:6443 \\ --token=\u0026#34;\u0026lt;YOUR_TOKEN\u0026gt;\u0026#34; Thatâ€™s it â€” the cluster should be up.\nI tested basic services and the latency is acceptable for non-latency-sensitive workloads.\nWritten on a speeding train â€” still worth a thumbs up.\nManual dog head ~\nHappy holidays!\n","date":"2024-05-01T08:00:00Z","image":"https://liguobao.github.io/zh/p/k3s-edge-computing-cluster/k3s_hu_14ccdc70feded138.png","permalink":"https://liguobao.github.io/p/k3s-edge-computing-cluster/","title":"[k3s] Best Edge Computing Cluster Solution of the Year"},{"content":"Instead of bothering others for help (for free), letâ€™s solve this with a tiny bit of code.\nOpen the target Zhihu answer page, e.g.:\nhttps://www.zhihu.com/question/647193565/answer/3469007201\nOpen DevTools (F12), go to the console, and run:\n1 document.getElementsByClassName(\u0026#34;RichContent RichContent--unescapable\u0026#34;)[0].innerText You should see the text content printed.\nRightâ€‘click the text output â†’ â€œCopy string contentsâ€, then paste wherever you need.\nDone.\n","date":"2024-04-27T08:00:00Z","image":"https://liguobao.github.io/zh/p/how-to-copy-zhihu-content/gqj_hu_bf9acab92d4ae1bd.png","permalink":"https://liguobao.github.io/p/how-to-copy-zhihu-content/","title":"How to Copy Some Hard-to-Copy Content on Zhihu"},{"content":"By Chen Yeliang â€” source link in the Chinese post\nRecently, writing on Zhihu has felt burdensome to me due to mental health. But in the theater, Miyazaki gave me two hours free of worries. Reading answers afterward, I felt I should contribute my aesthetics and knowledge to the world, even if it isnâ€™t pleasant.\nFor Miyazaki, â€œThe Boy and the Heronâ€ is an â€œunpleasantâ€ film â€” you can feel it. It still has a boy, a girl, a fantastical world, and a grand adventure, like Spirited Away or Laputa â€” but those were largely light and joyful. Their heroes had no inner knot; eyes were firm toward the future. This one is different: the boyâ€™s eyes are still firm, the goal clear, but he always carries a knot.\nHis enemy is no longer a concrete thing â€” not the stepmother, not the heron, not ghosts, not the castellan, not the parrot. The troubles canâ€™t be solved by slashing or shooting anyone. He can neither leave nor fight; seeing his mother or meeting the castellan changes nothing. The worldâ€™s collapse is inevitable; he is merely a witness.\nWhy make an â€œunpleasantâ€ film? Because the blade is pointed at himself. Dissecting oneself is never easy.\nThe story is actually straightforward if you grasp a few key metaphors. I pick six to form the spine:\nThe Tower There are two worlds: wartime Japan (reality) and the world inside the tower (fantasy). The bridge is the tower. Two explanations are offered: the stepmother says a clever greatâ€‘uncle built it; the maids say it fell from the sky and the uncle merely walled it in, and many died during construction.\nTo read the metaphor, recall modern Japanese history. After centuries of isolation under the Tokugawa shogunate, the â€œBlack Shipsâ€ forced Japan to open in 1854. Reformers aligned with southern domains led the Meiji Restoration and modernization.\nThe tower â€œfell from the skyâ€ and is black â€” the Black Ships; more broadly, Western modernity. The â€œclever greatâ€‘uncleâ€ stands for the reformers; Meiji Restoration wrapped Western ideas in Japanese dress, yielding a hybrid tower.\nBut the tower as the bridge between worlds goes deeper: it symbolizes the entire postâ€‘Meiji institutional system â€” politics, history, culture â€” a maze beneath. Inside, politics becomes visible; every creature stands for something. I view the tower as Japanâ€™s whole system.\nThe Tomb Owner Many reviews ignore this key metaphor â€” the root of the towerâ€™s decay, the source of pollution, the beginning. When the boy first arrives, he sees a tomb: walls outside, a golden door, a crude stone chamber within, inscribed â€œThose who emulate me shall die.â€ Even powerful Himi treats it warily; pelicans worship it, but only from afar.\nThe â€œtomb ownerâ€ is the archetype who brought the â€œlawâ€ of expansion and greed â€” a critique of the imported social order that rewards capital accumulation. It is the origin of the rot that pervades the tower.\nPelicans They are pitiful: greedy, timid, and they learned â€œmanâ€‘eatingâ€ from the tomb owner. Unable to catch fish at sea, they prey on the wahâ€‘laâ€‘wahâ€‘la (the little warawara souls).\nPelicans symbolize the new domestic forces after Meiji â€” the class of the boyâ€™s father: private capital. They learned expansion and compounding from old imperial powers but â€œcanâ€™t catch most fish at seaâ€ â€” they lack the capacity.\nFish never speak; they likely donâ€™t symbolize anything concrete â€” think â€œtraditional natural economy.â€ The preâ€‘imperialist cycle was: fish at sea â†’ we eat the meat â†’ oil goes to warawara â†’ warawara, wellâ€‘fed, ascend to become the future. People are the warawara; only they can sustain the world.\nPelicans were jammed into the system; like Japanese capitalism, they skipped the long gestation of Western capital. They canâ€™t run like the natural economy, and they canâ€™t touch the tomb ownerâ€™s cake; so they turn inward and squeeze the warawara. They know eating warawara ends the future, but the rule â€” â€œexpand or dieâ€ â€” forces blind expansion. New pelicans can no longer fly, and thus they becomeâ€¦\nParrots The clearest metaphor: militarism. Miyazaki all but draws the mustache on their faces.\nHow did militarism arise? Pelicans canâ€™t fish; the tomb owner canâ€™t be challenged. So Japan turned to direct violence and outward expansion: 1874 Taiwan expedition; 1875 Korea (Ganghwa); 1894 Sinoâ€‘Japanese War; 1900 the Eightâ€‘Nation Alliance; 1904 Russoâ€‘Japanese War; 1914 WWI (Qingdao, Jiaozhou, Jinan; the â€œTwentyâ€‘One Demandsâ€); 1918 Siberia; 1927 Manchuria; 1939 the Pacific War. Each victory fed the parrots until appetite and arrogance doomed the world.\nThe boyâ€™s father building Zero fighter shells is the collusion of pelican (capital) and parrot (militarism).\nThe Castellan He is Miyazaki himself. The thirteen blocks are his thirteen films. He searches for â€œclean stonesâ€ to stack â€” childhood innocence, dreams, and fantasy untainted by social rules. But he is pessimistic: clean stones grow scarce. Pollution â€” the law of greed and capital as success â€” is endless. Even the purest child must bow to it when grown. Miyazakiâ€™s own works are not fully untainted either; they must obey commerce to fund the next film.\nHe wants to build a stable tower that wonâ€™t collapse â€” but that is doomed. To the Parrot King, itâ€™s absurd to think your tower changes the world. The Parrot King casually piles a tower, finds it wrong, and chops it down; the world ends. Two atomic bombs fall; WWII ends; a new Japan is born. As all creatures flee the tower, parrots emerge too, instantly turning into harmless little birds that cuddle up to people â€” and then defecate on their faces as they fly off.\n(Further sections follow the same logic in the Chinese article.)\nMahito (the Boy) Â· The Wound Back to the start: the most â€œunpleasantâ€ part is the montage where Mahito fights at school and smashes his own head with a stone. Thereâ€™s no production sound, no monologue, the scuffle is elided under light music â€” a strange treatment for an important beat.\nIt feels like a storyteller glossing over his wrongdoing. Is Miyazaki hiding something? No â€” or he wouldnâ€™t include it at all. The bald patch remains for the entire film; his asymmetric hair is a constant reminder: Mahito did something wrong.\nHe injured himself, then lied to win parental concern and let his father trouble the classmate. To us itâ€™s childish; to Miyazaki it is base.\nAt the end, when the Castellan offers the blocks, Mahito refuses: â€œI am not pure. This wound proves my baseness. If I take the stone, I will pollute it.â€\nMeaning: his father profited from war; he grew up in that home. He too harbors base thoughts. He is not the pure, upright hero of classic Miyazaki, but a â€œreal personâ€. Building blocks is creating art; blocks are bits of life and a worldview. But if the self is base, the gaze pollutes, and the work distorts.\nThis is Miyazakiâ€™s worry about Japan. Mahito is the next generation. Japanese today live with original sin, nourished â€” however indirectly â€” by violence and war. When disasters are forgotten, can they remain pure? Can they build stable towers?\nAs a citizen of a country once invaded by Japan, I do not presume to untie their knots. My grandparentsâ€™ generation suffered; even today, our cityâ€™s airâ€‘raid sirens sound in September to mark the day it fell â€” a reminder not to forget national humiliation.\nBut victims of violence are not immune from violence. Polluted rules still â€œeat peopleâ€ in many societies. Our works also worship money, status, power, and violence; kneel to consumerism.\nMiyazakiâ€™s ideals â€” I genuinely admire. His questions â€” I cannot answer.\n","date":"2024-04-17T09:19:00Z","permalink":"https://liguobao.github.io/p/how-to-evaluate-the-new-work-of-hayao-miyazaki/","title":"How to Evaluate Miyazakiâ€™s â€œThe Boy and the Heronâ€?"},{"content":"Source: https://www.zhihu.com/question/265203478/answer/3415024932\nBackup. For when you need it.\nJavaScript snippet to batchâ€‘delete short answers on a page:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // Delete a single answer function deleteAnswer(answerId) { var xhr = new XMLHttpRequest(); xhr.withCredentials = true; xhr.addEventListener(\u0026#34;readystatechange\u0026#34;, function () { if (this.readyState === 4) { console.log(this.responseText); } }); xhr.open(\u0026#34;DELETE\u0026#34;, \u0026#34;https://www.zhihu.com/api/v4/answers/\u0026#34; + answerId); xhr.setRequestHeader(\u0026#34;authority\u0026#34;, \u0026#34;www.zhihu.com\u0026#34;); xhr.setRequestHeader(\u0026#34;accept\u0026#34;, \u0026#34;*/*\u0026#34;); xhr.setRequestHeader(\u0026#34;accept-language\u0026#34;, \u0026#34;zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7\u0026#34;); xhr.send(); console.log(\u0026#39;delete answer: \u0026#39; + answerId + \u0026#39; success\u0026#39;); } // Clean all answers on the current page (shorter than N characters) function clearAllAnswers(longer_than = 100) { let answerItemList = document.querySelectorAll(\u0026#39;.AnswerItem\u0026#39;); for (let answerItem of answerItemList) { let answerContentDev = answerItem.querySelector(\u0026#39;.RichContent-inner\u0026#39;); if (!answerContentDev) continue; let ans_id = answerItem.getAttribute(\u0026#39;name\u0026#39;); let answerContent = answerContentDev.innerText; if (answerContent.length \u0026gt;= longer_than) { console.log(`answer:${ans_id} is too long, skip`); continue } console.log(`delete answerContent: ${answerContent}`); deleteAnswer(ans_id); } } Steps\nGo to your answers list page, open the browser DevTools console (F12).\nPaste the code above into the Console and press Enter.\nRun clearAllAnswers(50) and press Enter.\nNote: 50 means answers longer than 50 characters are skipped.\nKeep paging through your answers and repeat: clearAllAnswers(50)\nDone.\n","date":"2024-03-01T08:00:00Z","permalink":"https://liguobao.github.io/p/how-to-delete-all-answers-on-zhihu/","title":"How To Delete All Your Zhihu Answers and Ideas At Once?"},{"content":"Source (Chinese): https://www.zhihu.com/question/640589218/answer/3414109229\nWelcome to the workplace theme park. Please follow the safety rules:\nPower runs 24/7; diesel generators autoâ€‘start during outages. Do not approach the power room if you hear loud bangs. Report any brown liquid seeping out to admin.\nPower buttons are on top of cases; if yours is on the front, do not power on â€” leave early.\nWe provide 19â€‘inch MacBook Pros (silver only). If you get a black one, do not turn it on; return it to IT.\nPantry stocks Pepsi Zero only. If you see Cocaâ€‘Cola on sale, donâ€™t try it â€” call admin. If you drank it, go to the clinic.\nTurn off your monitor when leaving â€” save energy.\nDo not shut down your PC â€” improve efficiency.\nIf the PC is off in the morning, hold power to boot. If it wonâ€™t boot, call IT. Donâ€™t open the case or unplug it yourself.\nIf you see other monitors lit in the morning, note the number and leave; report if needed.\nThere are no robot dogs. If you see one, avoid and report.\nMost programmers are introverts; avoid unnecessary small talk. If one initiates unusually, note their number and report.\nAccess control and face recognition are developed inâ€‘house; only staff cards work. No outsiders can enter.\nâ€¦\nWhy leave PCs on? Builds, background jobs, remote sessions, scheduled tasks, and longâ€‘running dev tools keep working overnight. Monitors off saves power; machines on preserves context and avoids morning setup cost.\n","date":"2024-02-29T08:00:00Z","permalink":"https://liguobao.github.io/p/why-dont-programmers-turn-off-their-computers-after-work/","title":"Why Do Programmers Turn Off Only the Monitor, Not the PC?"},{"content":"Source: https://www.zhihu.com/question/639020341\nQ: Whatâ€™s the shortest I can rent a server? Can I rent for only a week?\nA:\nIs there such a service? Yes. Choose payâ€‘asâ€‘youâ€‘go/onâ€‘demand. Most cloud products support it. In China, many providers require realâ€‘name verification and a prepaid balance (e.g., \u0026gt;100 CNY) to enable it â€” a bit sneaky.\nAny â€œcheaperâ€ options? Sure: for static sites, GitHub Pages is simplest. I wrote this earlier:\nHow to configure a GitHub Pages site: https://liguobao.github.io/p/how-to-set-github-page/\nI want to expose my local site to the internet â€” any simple solution? Yes. Easiest is FRP (intranet penetration), but you still need a public server for the FRP server side.\nHow to configure FRP: https://liguobao.github.io/p/how-to-set-frp/\nAlternatively, use a tunneling service. When I played with stable-diffusion-webui, it provided a temporary public URL (via https://www.gradio.app/). You can use similar services to expose a local service as a temporary public address.\nDocs: Sharing Your App â€” https://www.gradio.app/3.50.2/guides/sharing-your-app\nHave fun!\n","date":"2024-01-29T08:00:00Z","permalink":"https://liguobao.github.io/p/how-long-can-i-buy-a-server/","title":"Whatâ€™s the Shortest Server Rental? Can I Rent for Just a Week?"},{"content":"The last update was around late 2022. No strong new ideas since then; I kept occasional data maintenance going (powered by love, as always).\nLast month, a certain site removed an old encryption routine, and data volume dropped a lot. A few days ago, we restored scraping for that siteâ€™s group data.\nRecently, I saw user feedback in the Mini Program console saying â€œCopy linkâ€ stopped working.\nChecked with a front-end friend â€” turns out the Mini Program framework we used no longer has docs. Another framework sunsetâ€¦ classic. Generally, avoid KPIâ€‘driven bigâ€‘company frameworks when you can.\nAfter some effort we got the project running. It looks like the privacy terms changed, and the clipboard API we used was disabled. So a light update should do.\nAlso fixed: auto-refresh when entering the list page; cache hit logic, which improved response speed. Multi-layer caching is â€œmaxed outâ€ here â€” even I got confused at times.\nIf time permits in a few weeks, Iâ€™ll start a framework migration and maybe a redesign. Still thinking about product direction.\nI also hope to spin up an NLP text classifier: first pass to detect ad posts, then a scoring model. Might take a while â€” target is before midâ€‘year.\nI tested pulling some xhs data today; the pipeline works â€” though whether thatâ€™s â€œsafeâ€ is another story :-)\nThatâ€™s it for now. See you in the next updateâ€¦ or a farewell. Such is life. Bye.\nOriginal (Chinese): https://mp.weixin.qq.com/s/4nC2zZm2a6Tn4LL7H-oC8w?poc_token=HIyRtmWjVEi3VvkbLmrA_rAIxqQfWOzfTl-rWZo6\n","date":"2024-01-13T23:09:00Z","permalink":"https://liguobao.github.io/p/house-map-bug-fix/","title":"House-Map: Bug Fixes After a Long Hiatus"},{"content":"Source: https://www.zhihu.com/question/638539558\nFirst, a general path.\nEducation With a standard bachelorâ€™s, in principle you shouldnâ€™t be autoâ€‘screened â€” but lately some HRs hardâ€‘filter by degree/school; it comes down to luck. Try anyway.\nStepwise moves Small company â†’ leading small company Leading small company â†’ midâ€‘size company Midâ€‘size company â†’ big tech This is the safer route, but itâ€™s long and you might miss windows of opportunity.\nResume Write it well. Have friends review; tailor for each role. Otherwise you may fail the resume screen outright. Prepare a couple of projects you can speak about in depth â€” they should look interesting and solid at a glance, and hold up in conversation.\nReferrals and advocates Get people to vouch for you. Hiring folks donâ€™t mind fishing for resumes. If a friend forwards yours, I (for example) will at least look and reply honestly about fit. If it looks good, Iâ€™ll put in a good word to get you a shot.\nBe thickâ€‘skinned and meet people online; there are always ways. Avoid â€œreferral brokersâ€; youâ€™re just a transaction to them. Friends are more likely to care and follow up. Be polite widely and offend few; someone will help.\nLeetCode + â€œeightâ€‘legged essaysâ€ (fundamentals Q\u0026amp;A) No way around it â€” some companies like them. Practice; muscle memory helps. As for rote theory grilling, it depends on the company. Some places overdo it; be prepared.\nEaster egg: so he didnâ€™t even call me â€œBaoâ€‘geâ€â€¦\n","date":"2024-01-08T15:00:00Z","permalink":"https://liguobao.github.io/p/how-to-jump-from-small-company-to-big-company/","title":"2ndâ€‘Tier Public University CS Grad, 1 Year In â€” How To Jump From Small Company to Big Tech?"},{"content":"Source: https://www.zhihu.com/question/638134615/\nSome quick thoughts I jotted down about â€œscenariosâ€.\nA â€œfault lineâ€ effect How many people working away from home have the conditions to watch TV? Most rental apartments donâ€™t even have a living room; if thereâ€™s a living room, it may not have a TV; if thereâ€™s a TV, you need internet; with internet youâ€™ll likely watch OTT/streaming; cable requires a separate subscription. After each layer of filtering, a huge population shrinks to a sliver. This doesnâ€™t fully explain the postâ€‘2016 cliff, but itâ€™s indicative.\nContent supply Letâ€™s be honest: TV programming quality has dropped. If you switch to the channels you watched years ago, youâ€™ll notice how weak the content is. Ads are even more homogenized. Compared to my childhood, todayâ€™s TV ads areâ€¦ well, terrible. Why talk ads? Because thatâ€™s the main revenue for TV shows. Less money â†’ canâ€™t buy/make good content â†’ fewer viewers â†’ less ad revenue â€” a vicious cycle.\nInternet TV isnâ€™t great either. Aside from Bilibiliâ€™s TV app, most others feel like adware. Cloud TV clients arenâ€™t â€œgoodâ€, they just look better by comparison. If you try Netflixâ€™s TV client, youâ€™ll see what a proper TVâ€‘first experience looks like. Not targeting anyone: all the content providers here are â€œgreatâ€ (you know what I mean).\nHabits The younger generation didnâ€™t inherit a TVâ€‘watching habit. Lack of conditions is one reason; lack of content is another. Some potential â€œnew scenariosâ€ were also bypassed. For example, as modernization progressed, gaming could have driven a TV resurgence â€” big screen + gamepad = compelling. But in China, we went from web games to online games to mobile games. Console gamers are mostly a small, slowâ€‘growing stock. Itâ€™s hard for people to experience the â€œbetterâ€ TV+game experience. (Yes yes, youâ€™re right.)\nGenshin Impact has been popular for years, but not many know you can play it on PS4/5 with a controller. Trust me: 60â€ display + gamepad beats poking a phone screen.\nRandom aside Some things will likely die out. Let them. If people like what they like, thatâ€™s that.\n","date":"2024-01-07T21:44:00Z","permalink":"https://liguobao.github.io/p/our-tv-open-rate-fall/","title":"Chinaâ€™s TV Turnâ€‘On Rate Dropped from 70% (2016) to 30% â€” Why?"},{"content":"Source (Chinese): https://www.zhihu.com/question/637524973\nAnswer:\nFriend.\nIf someone points to an â€œeasy roadâ€ in this world, the first question to ask yourself is: Do you deserve that road?\nRight?\nSo hereâ€™s the straight answer.\nIf you believe your coding ability is weak and you donâ€™t plan to change that, then you wonâ€™t handle most directions well.\nSimple logic: coding ability is the foundation. If the foundation is weak, everything else is empty talk.\n","date":"2024-01-02T21:44:00Z","permalink":"https://liguobao.github.io/p/985-huawu-find-job/","title":"985 Undergrad, C9 Masterâ€™s Year 1, CS Major, Poor Coding â€” What Direction Should I Switch To?"},{"content":"Broadly, the industry still has a future â€” informatization continues to raise efficiency. Narrowly, things will be flatter or even tough for a while; the explosive growth phase is gone. If youâ€™re only here for the â€œbonus periodâ€, youâ€™ll struggle with the new normal. Do what you truly like.\n","date":"2024-01-02T16:00:00Z","permalink":"https://liguobao.github.io/p/can-not-join-computer/","title":"Is it too late to enter the computer industry?"},{"content":"Q: https://www.zhihu.com/question/579778692\nA:\nFirst, itâ€™s not entirely true they havenâ€™t. For everyday shortâ€‘video use, most creation happens on phones now. Many mobile apps handle both shooting and editing, and you can publish directly to platforms. In that scenario, phones have â€œreplacedâ€ PCs.\nApple even showcases iPhoneâ€‘shot films every year, with endâ€‘toâ€‘end shooting and editing on iPhone â€” a cool flex. In that context, a phone can indeed beat a PC.\nButâ€¦ broadly speaking, all of â€œusâ€ here are computers â€” phones included. If you canâ€™t beat them, join them. Phones are computers too; theyâ€™re a form factor under the umbrella of computing devices. One form factor replacing another is normal.\nWhat the question really means is: why havenâ€™t personal computers been replaced by phones?\nMy take:\nCompute isnâ€™t enough. For many workloads, phone SoCs still lag behind desktop/laptop CPUs/GPUs in sustained performance and thermal headroom.\nInteraction isnâ€™t enough. Touch-only UIs, small screens, limited input precision, multitasking constraints â€” all cap productivity for many tasks.\nEcosystem isnâ€™t enough. Tooling, workflows, peripherals, and pro software ecosystems are still PCâ€‘centric in many fields.\nOverall, it likely takes another revolution â€” which might also end both as we know them. Just kidding (sort of).\n","date":"2023-12-31T08:00:00Z","permalink":"https://liguobao.github.io/p/why-pc-not-replaced-by-mobile-test/","title":"Why Havenâ€™t PCs Been Replaced by Phones?"},{"content":"Q: If pay werenâ€™t high, with the same overtime, would you still be a programmer?\nA: Yes. I code because I enjoy it â€” the creation, the clarity (0 is 0, 1 is 1), the rigor proven by math. Even if it couldnâ€™t feed my body, it nourishes my soul. Thatâ€™s enough.\n","date":"2023-12-30T00:00:00Z","permalink":"https://liguobao.github.io/p/if-programmer-salary-is-not-high/","title":"If programmer salaries werenâ€™t high (same overtime), would you still code?"},{"content":"Official guide: Configuring a publishing source for your GitHub Pages site https://docs.github.com/zh/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site\nKey point:\nSet the correct repository branch for Pages.\nIf you use Hugo (or similar static site generators), they typically produce a public folder of static files.\nWith GitHub Actions, you build and publish those static files, usually to a dedicated branch of the repository.\nMy Example Repository:\nhttps://github.com/liguobao/liguobao.github.io The default branch (master) contains my site config and content.\nThe static output branch is:\nhttps://github.com/liguobao/liguobao.github.io/tree/gh-pages\nNow go to GitHub project Settings â†’ Pages and set â€œYour GitHub Pages siteâ€ to branch gh-pages.\nDone.\nPhoto by Pawel Czerwinski on Unsplash\n","date":"2023-12-19T13:34:08Z","image":"https://liguobao.github.io/zh/p/how-to-set-github-page/cover_hu_62e5471eb76ae943.png","permalink":"https://liguobao.github.io/p/how-to-set-github-page/","title":"How to Configure a GitHub Pages Site"},{"content":"I wonâ€™t. Real productivity is workflow and ecosystem fit; iOS suits me. If not iOS, Iâ€™d choose Android (I use a Redmi K50) rather than switch for its own sake.\n","date":"2023-12-19T00:00:00Z","image":"https://liguobao.github.io/zh/p/why-do-you-mo-to-huawei/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://liguobao.github.io/p/why-do-you-mo-to-huawei/","title":"What â€œproductivityâ€ made you ditch iPhone for Huawei?"},{"content":"Short answer: people vary; the job doesnâ€™t require vices. Many prefer quiet hobbies, fitness, or learning. Pressure and culture affect habits more than the profession.\n","date":"2023-12-15T00:00:00Z","permalink":"https://liguobao.github.io/p/programmer-smoke-drink-play-cards/","title":"Do programmers smoke, drink, or play cards more than others?"},{"content":"A small project needed MySQL full DB backups. Instead of setting up replicas or cron + mysqldump, I looked for a library and found MySqlBackup.\nSnippet (from docs):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Export string constring = \u0026#34;server=localhost;user=root;pwd=qwerty;database=test;\u0026#34;; string file = \u0026#34;C:\\\\backup.sql\u0026#34;; using (var conn = new MySqlConnection(constring)) using (var cmd = new MySqlCommand()) using (var mb = new MySqlBackup(cmd)) { cmd.Connection = conn; conn.Open(); mb.ExportToFile(file); } // Import using (var conn = new MySqlConnection(constring)) using (var cmd = new MySqlCommand()) using (var mb = new MySqlBackup(cmd)) { cmd.Connection = conn; conn.Open(); mb.ImportFromFile(file); } Productionized version wraps it in a service with endpoints to export/import/delete and to list backup files. It also includes a hosted service for scheduled nightly backups.\nTips:\nIf you hit connection timeouts, raise connect-timeout (e.g., 3600). For large imports: â€œPackets larger than max_allowed_packetâ€¦â€ â‡’ increase max_allowed_packet (e.g., 1GB). docker-compose example args: 1 2 3 4 5 6 args: - --lower-case-table-names=1 - --max-connections=4000 - --connect-timeout=3600 - --max-allowed-packet=1073741824 - --sql-mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION Scheduled backup via IHostedService (runs daily at UTC 19:00 / Beijing 03:00):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class DBBackupTimeHostedService : IHostedService, IDisposable { private readonly DBBackService _dbBackService; private Timer _timer; public DBBackupTimeHostedService(DBBackService svc) { _dbBackService = svc; } public Task StartAsync(CancellationToken ct) { _timer = new Timer(DoWork, null, GetDelayTo3AM(), TimeSpan.FromDays(1)); return Task.CompletedTask; } void DoWork(object state) { try { _dbBackService.ExportToFile(); } catch (Exception ex) { /* log */ } } public Task StopAsync(CancellationToken ct) { _timer?.Change(Timeout.Infinite, 0); return Task.CompletedTask; } public void Dispose() =\u0026gt; _timer?.Dispose(); TimeSpan GetDelayTo3AM() { var now = DateTime.UtcNow; var next = now.Date.AddHours(19); if (now.Hour \u0026gt;= 19) next = next.AddDays(1); return next - now; } } // services.AddHostedService\u0026lt;DBBackupTimeHostedService\u0026gt;(); Simple and effective â€” mind the risks and keep backups safe.\n","date":"2023-10-26T23:09:00Z","permalink":"https://liguobao.github.io/p/dotnet-core-mysql-backup/","title":"MySQL Full Backup and Restore in .NET Core"},{"content":"The Odd Failure Desktop has two NICs: onboard Ethernet and a PCIe Wiâ€‘Fi card. After reassembling hardware (P40 GPU tinkering) and rebooting, the wired NIC was â€œdownâ€.\nPreviously I had fixed similar issues by deleting the old wired profile in Ubuntu and reâ€‘adding it. This time I had no monitor available (TV too far for an Ethernet cable), so I limped along on Wiâ€‘Fi for a while.\nIn theory, resetting the NIC should suffice. In ifconfig, a normal NIC shows inet fields; the broken one didnâ€™t â€” the system wasnâ€™t recognizing it properly.\nifconfig down/up didnâ€™t help.\nUse nmtui (NetworkManager TUI) nmtui is a curses-based TUI for NetworkManager.\nRun nmtui, choose â€œActivate a connectionâ€, then review the devices. In the problematic case the Device shows an invalid NIC id.\nDelete the invalid connection Add a new one, entering the NIC name you saw in ifconfig Save and activate Done.\nRoot Cause After hardware changes, the system reinitialized and assigned a new device id to the onboard NIC, while your saved network profile still referenced the old id. Resetting the device alone doesnâ€™t fix that â€” you must recreate the connection profile so it binds to the new device id.\n","date":"2023-09-22T00:00:00Z","image":"https://liguobao.github.io/zh/p/ubuntu-wifi-loss/xiaohei_hu_4c4dd5882325198c.png","permalink":"https://liguobao.github.io/p/ubuntu-wifi-loss/","title":"Ubuntu Ethernet Drops â€” Quick Reset Notes"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, and shows whether basic HTML elements are styled in a Hugo theme.\nHeadings The following HTML â€” elements represent six levels of section headings. is the highest section level while is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo\u0026hellip; (sample filler text)\nBlockquotes Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown within a blockquote.\nTables Name Age Bob 27 Alice 23 Code Blocks 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; Lists First item Second item Third item One Two Three ","date":"2023-09-07T00:00:00Z","permalink":"https://liguobao.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo theme Stack supports interactive image galleries powered by PhotoSwipe.\nPlace images in the same bundle (folder) as this Markdown file so Hugo can read dimensions. External images are not supported.\nSyntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) Result Photos by mymind and Luke Chesser on Unsplash\n","date":"2023-08-26T00:00:00Z","image":"https://liguobao.github.io/zh/p/image-gallery/2_hu_5b8da187865385db.jpg","permalink":"https://liguobao.github.io/p/image-gallery/","title":"Image gallery"},{"content":"For more details, check out the documentation: https://stack.jimmycai.com/writing/shortcodes\nBilibili video Tencent video YouTube video Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. Gist GitLab Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nâ€• A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2023-08-25T00:00:00Z","image":"https://liguobao.github.io/zh/p/shortcodes/cover_hu_4b8e0b1c00bd9fd1.jpg","permalink":"https://liguobao.github.io/p/shortcodes/","title":"Shortcodes"},{"content":"Stack supports math typesetting via KaTeX. Enable per post with math: true in front matter, or siteâ€‘wide by setting params.article.math = true in config.toml.\nInline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887â€¦$\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n$$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$\n","date":"2023-08-24T00:00:00Z","permalink":"https://liguobao.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Build fundamentals, code and ship, read othersâ€™ code, test, document, communicate, and keep learning. Be reliable; be curious.\n","date":"2023-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/how-to-be-qualified-programmer/","title":"How to become a qualified programmer?"},{"content":"Match hardware to workload and ecosystem: prioritize RAM/SSD, keyboard/screen, and OS/toolchain needs. Macs for iOS/general dev; Windows/Linux for flexibility; consider ThinkPad/Legion/ZenBook/Framework, etc.\n","date":"2023-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/choose-laptop-for-programming/","title":"How to pick a laptop for programming?"},{"content":"Depends on interest, learning ability, and runway. Itâ€™s worth it if you enjoy building and can sustain months of study and projects; otherwise consider adjacent roles.\n","date":"2023-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/is-career-switch-to-programming-worth/","title":"Is switching careers into programming still worth it?"},{"content":"Theyâ€™re hard in different ways. Programmers are often â€œcode portersâ€; couriers are â€œmeal portersâ€. One is mental, one is physical, both repetitive. Cycles change who looks better at any given time; neither is easy.\n","date":"2023-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/which-harder-programming-or-delivery/","title":"Which is harder programming or food delivery?"},{"content":"Common culprits: endless rush without purpose, weak feedback, poor management, and health neglect. Solutions: boundaries, meaningful work, exercise, and community.\n","date":"2023-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/why-programmers-burn-out/","title":"Why do many programmers feel burned out?"},{"content":"Python Flask Best Practices: 1) Project Initialization Preface Python Flask is a simple and convenient web framework.\nItâ€™s easy to build a web site or a pure Web API with it.\nI recently needed a small web + scripting project. While setting up Flask, I couldnâ€™t find a starter that fit my taste, so I explored a bit, hit a few bumps, and then put together this tutorial on the shoulders of giants.\nProject Structure For a real project, I recommend a layered architecture (3-tier) + MVC separation to keep the code well organized. If you arenâ€™t familiar with these concepts, itâ€™s worth a quick read.\nNotes:\nsrc: all project source files src/service: service/business logic src/model: business entities src/db: database related, including model definitions and DAO/SQL src/sdk: external integrations/SDKs src/job: background jobs (often triggered via API) src/utils: utilities; configs live here too src/app.py: Flask app entrypoint manage.py: flask.cli entry, starts API + jobs Dockerfile: Docker build debug.py: local debug entry requirements.txt: all dependencies .vscode/launch.json: VS Code debug config Letâ€™s Build Personal preference: I like managing dependencies via a requirements.txt. Use your preferred approach if you like.\nrequirements.txt 1 2 3 4 5 6 7 8 9 10 flask flask-swagger flask-swagger-ui flask-bootstrap SQLAlchemy pymysql pydantic requests loguru gunicorn Brief notes:\nflask is the core; flask-swagger + flask-swagger-ui provide Swagger UI; flask-bootstrap helps quickly scaffold HTML pages. SQLAlchemy + pymysql: ORM + MySQL driver. pydantic: works with SQLAlchemy for model conversion, easing serialization/deserialization quirks. requests: call external HTTP APIs or write crawlers. loguru: simple logging; from loguru import logger and youâ€™re set. gunicorn: multi-process deployment. ","date":"2021-09-05T00:00:00Z","permalink":"https://liguobao.github.io/p/python-flask-1-project-initialization/","title":"Python Flask 1 Project Initialization"},{"content":"How to view Tencent Cloudâ€™s lowâ€‘code offering? Can â€œeveryoneâ€ become a developer? Lowâ€‘code isnâ€™t new; Tencent Cloud is late but has its approach. A quick arc:\nFrom VMs to functions â€” AWS Lambda popularized FaaS; Alibaba Cloud followed; Tencent Cloudâ€™s Serverless capabilities round out the field. Serverless improves over managed VMs/containers in ops burden and scaleâ€‘toâ€‘zero; it changes dev patterns. Tencentâ€™s edge is tighter ecosystem integration. Whatâ€™s special about Miniâ€‘Program Cloud Dev Builtâ€‘in cloud DB and storage are quite handy; competitors donâ€™t ship this integrated (as far as I know). WeChat + Tencent ecosystem integrations (payments, maps, auth) make â€œwiring things upâ€ faster. Tooling: VS Code extensions, WeChat DevTools, and ops integration. â€œEveryone is a developerâ€? What is software development, and what do developers do? Cloudâ€‘dev lowers the threshold (faster iteration, quick validation), but real products still need engineering and tradeâ€‘offs. Lowering barriers helps experimentation; complexity remains for robust systems.\nFollowâ€‘up: Does cloudâ€‘dev lower the bar? How to assess developer value? First define the â€œbarâ€:\nWhat does it take to go from idea â†’ online product? (requirements, data, UX, infra, security) Where do core CS concepts matter in real projects? Miniâ€‘programs:\nCritiques: walled garden; performance limits. Pros: onâ€‘demand usage; low entry; multiâ€‘end unification; platform quality baseline. Miniâ€‘program Cloud Dev:\nDeep integration with miniâ€‘program stack Provides DB + storage capabilities Some scenarios need no/weak backâ€‘ends or dedicated servers, lowering cost Value of developers:\nLowering the â€œbuildâ€ bar and trialâ€‘andâ€‘error cost is good; turning an experiment into a stable product still needs engineering depth. â€œFast, good, cheapâ€ requires judgment â€” where to apply theory, where to iterate quickly, how to keep quality. ","date":"2020-11-29T00:00:00Z","permalink":"https://liguobao.github.io/p/qcloud-serverless-notes-on-lowcode-miniprogram-cloud-dev/","title":"QCloud Serverless â€” Notes on Lowâ€‘Code \u0026 Miniâ€‘Program Cloud Dev"},{"content":"Symptoms: on startup, the first wave of traffic produced no responses while health checks looked OK, so the k8s LB kept routing, requests piled up, DB connections spiked, and pods flapped.\nWhat it wasnâ€™t: not a DB bottleneck (CPU/IO fine), not EF per se, not external infra issues.\nContributing factors and fixes:\nWarm up DB connections and EF metadata (limited effect). Make controller/service code truly async endâ€‘toâ€‘end (notably improved p50/p95 and reduced startup pileups). Preâ€‘warm instances before cutting traffic over to reduce coldâ€‘start jitter. Be careful with filters that do network/Redis/MaxMind work during request entry; ensure these calls are async, cached, and bounded. Root cause: a dependency injected component with expensive sync work on the hot path during startup. Fixing the DI lifetime and making the path fully async eliminated the cascade.\n","date":"2020-06-07T00:00:00Z","permalink":"https://liguobao.github.io/p/a-di-misstep-that-cascaded-into-startup-incidents/","title":"A DI Misstep That Cascaded into Startup Incidents"},{"content":"Repost summary of â€œWhatâ€™s the right way to do Code Review?â€ â€” focus on clarity, empathy, and outcomes. CR is not power play; itâ€™s a craft for improving code, systems, and teams. Review the code, not the person; discuss scenarios and tradeâ€‘offs; make each CR an opportunity to learn and teach.\n","date":"2020-04-05T00:00:00Z","permalink":"https://liguobao.github.io/p/the-right-posture-for-code-review-repost-english-summary/","title":"The Right Posture for Code Review (Repost â€” English Summary)"},{"content":"Foreword Previously we talked about Pods/Services. A Service gives you LB inside the cluster and a stable name for Podâ€‘toâ€‘Pod access. But how do we expose it to the public internet?\nIn Kubernetes, Ingress handles external access (hosts, TLS, routing) â€” think â€œlike nginxâ€, and many controllers are nginxâ€‘based.\nOn Tencent Cloud, public IP LBs cost money (e.g., HTTP/HTTPS ALB Â¥0.02/hour). If you can afford that, use the managed LB + Ingress flow in the console and click through.\nA frugal approach Your worker nodes already have public IPs. Run nginx on a node and proxy to your K8s Serviceâ€™s internal IP â€” done.\nSo how do we give a Service an internal IP reachable from the VPC? On Tencent Cloud, annotate the Service to allocate an internal LoadBalancer IP in your subnet.\nService example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Service metadata: # Request an internal LB IP in your subnet annotations: service.kubernetes.io/loadbalance-id: lb-\u0026lt;your-lb-id\u0026gt; service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-\u0026lt;your-subnet-id\u0026gt; name: codelover-blog namespace: default spec: externalTrafficPolicy: Cluster ports: - nodePort: 32278 port: 80 protocol: TCP targetPort: 80 selector: run: codelover-blog sessionAffinity: None type: LoadBalancer Check the result:\n1 2 3 4 5 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) codelover-blog LoadBalancer 172.16.255.232 172.17.0.28 80:32278/TCP curl -v 172.17.0.28 ... HTTP/1.1 200 OK ... Now set up nginx on your public node:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 server { listen 80; server_name your.domain; location / { proxy_pass http://172.17.0.28:80; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; gzip on; gzip_min_length 5k; gzip_buffers 4 16k; gzip_http_version 1.1; gzip_comp_level 3; gzip_types text/plain application/json application/javascript text/css application/xml text/javascript image/jpeg image/gif image/png; gzip_vary on; } Done.\n","date":"2019-08-31T00:00:00Z","permalink":"https://liguobao.github.io/p/you-can-definitely-get-tencent-cloud-k8s-service/ingress-working/","title":"You Can Definitely Get Tencent Cloud K8s Service/Ingress Working"},{"content":"Foreword Service: .NET Core API + EF, from ~1k DAU to several tens of thousands. 6â€“8 Docker instances; QPS â‰ˆ 100 overall. Perâ€‘instance concurrency isnâ€™t huge.\nIssue: after starting and receiving a burst of traffic, requests pile up. Logs show requests arriving but not reaching business logic or DB â€” they seem to await scheduling. Sometimes instances crash on start; occasionally later when a sudden surge arrives.\nKnowns:\nCrashes mostly at startup under burst; sometimes later under spikes. Instance CPU/RAM look fine; host is healthy; no heavy I/O; network is normal. Before the issue, DB connections are normal. As instances flap or hang, DB connections spike and can exhaust the pool. Guess 1: Database trouble? EF firstâ€‘use warmâ€‘up is slow; many firstâ€‘call queries might hit DB concurrently. Tried preâ€‘warming DbContext pool at startup:\n1 2 3 4 5 6 7 8 9 10 11 12 13 static void InitDataContextService(IApplicationBuilder app) { Console.WriteLine($\u0026#34;Init start: {DateTime.Now}\u0026#34;); var tasks = new List\u0026lt;Task\u0026gt;(); for (var i = 0; i \u0026lt;= 10; i++) { using var scope = app.ApplicationServices.CreateScope(); var context = scope.ServiceProvider.GetRequiredService\u0026lt;XXXContext\u0026gt;(); tasks.Add(context.Database.CanConnectAsync()); } Task.WaitAll(tasks.ToArray()); Console.WriteLine($\u0026#34;Init finish: {DateTime.Now}\u0026#34;); } Effect: negligible.\nGuess 2: Sync calls block resources â†’ pileups? Early code used synchronous controllers/services. Switched to async endâ€‘toâ€‘end:\n1 2 3 [HttpGet(\u0026#34;v1/books/{id}\u0026#34;, Name = \u0026#34;GetBook\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; GetBook([FromRoute] string id) =\u0026gt; Ok(new { data = await _bookService.FindBookDetailForUser(id), code = 0 }); Result: adâ€‘hoc tests showed ~30â€“50% gains; pileups reduced.\nâ€œWarming helpsâ€ (anecdotally) Preâ€‘warm instances before cutting traffic: fewer startup crashes; more stable startup. Temporary mitigation.\nRegression from a filter One iteration worsened crashes. The largest change was an ActionFilter that parsed X-Forwarded-For, looked up location in Redis, fell back to MaxMind DB, then wrote back to Redis:\n1 2 3 4 5 6 7 8 9 10 public override void OnActionExecuting(ActionExecutingContext ctx) { var dic = new Dictionary\u0026lt;string,string\u0026gt;(); string ip = ctx.HttpContext.Request.Headers.GetHeaderValue(\u0026#34;X-Forwarded-For\u0026#34;); if (ip.Contains(\u0026#39;,\u0026#39;)) ip = ip.Split(\u0026#39;,\u0026#39;)[0]; var iso = _redis.ReadHashValueByKey\u0026lt;string\u0026gt;(ConstRedisKey.IPLocations, ip) ?? _maxMind.GetIPLocationISOCode(ip).Tap(x =\u0026gt; _redis.WriteHash(ConstRedisKey.IPLocations, ip, x)); dic.Add(CommonConst.LocationHeaderKey, iso); _httpHeaderTools.CurrentXHeaders = new ThreadLocal\u0026lt;Dictionary\u0026lt;string,string\u0026gt;\u0026gt;(() =\u0026gt; dic); } This added synchronous I/O on the hot path; under cold start and bursty load, it exacerbated pileups.\nTakeaways Make the entire request path truly async; avoid sync I/O and heavy work in filters/middleware. Preâ€‘warm critical pools/metadata to reduce coldâ€‘start jitter. Measure under realistic burst/coldâ€‘start scenarios. ","date":"2019-08-03T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-async-tuning-a-tale-of-cold-starts-and-pileups/","title":".NET Core â€œAsync Tuningâ€ â€” A Tale of Cold Starts and Pileups"},{"content":"Swagger for ASP.NET Core Web API Swagger/OpenAPI has become the de facto way to document REST APIs. Instead of maintaining a YAML/JSON manually, let Swashbuckle generate docs from your controllers.\nGetting Started Microsoft tutorial: https://docs.microsoft.com/aspnet/core/tutorials/getting-started-with-swashbuckle Repo: https://github.com/domaindrivendev/Swashbuckle.AspNetCore Install packages:\n1 2 dotnet add package Swashbuckle.AspNetCore --version 4.0.1 dotnet add package Swashbuckle.AspNetCore.Annotations --version 4.0.1 Startup configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public void ConfigureServices(IServiceCollection services) { services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_2); services.AddSwaggerGen(c =\u0026gt; { c.SwaggerDoc(\u0026#34;v1\u0026#34;, new Info { Title = \u0026#34;XX-404 API\u0026#34;, Version = \u0026#34;v1\u0026#34; }); c.EnableAnnotations(); }); } public void Configure(IApplicationBuilder app, IHostingEnvironment env) { app.UseSwagger(); app.UseSwaggerUI(c =\u0026gt; { c.RoutePrefix = \u0026#34;docs\u0026#34;; c.SwaggerEndpoint(\u0026#34;/swagger/v1/swagger.json\u0026#34;, \u0026#34;Zhihu-404 API\u0026#34;); }); app.UseMvc(); } Example controllers (abbrev.):\n1 2 3 4 5 6 7 [Route(\u0026#34;api/v1/health\u0026#34;)] [ApiController] public class HealthController : ControllerBase { [HttpGet] public ActionResult\u0026lt;IEnumerable\u0026lt;string\u0026gt;\u0026gt; Get() =\u0026gt; new [] { \u0026#34;value1\u0026#34;, \u0026#34;value2\u0026#34; }; } 1 2 3 4 5 6 7 8 9 10 11 12 13 [ApiController] public class ExtendDataController : ControllerBase { [HttpPost(\u0026#34;api/v1/extend-data\u0026#34;)] [SwaggerResponse(200, \u0026#34;\u0026#34;, typeof(int))] public ActionResult AddExtendData([FromBody, SwaggerParameter(\u0026#34;raw data\u0026#34;)] List\u0026lt;DBExtendData\u0026gt; extendData) { if (extendData == null || extendData.Any(a =\u0026gt; a.Id == default)) throw new Exception(\u0026#34;Invalid input; check each Id\u0026#34;); var count = 10; // bulk insert return Ok(new { code = 0, data = count }); } } Open http://localhost:5000/docs to view the generated UI.\n","date":"2019-07-06T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-web-api-generate-swagger-docs-with-swashbuckle/","title":".NET Core Web API â€” Generate Swagger Docs with Swashbuckle"},{"content":"Kubernetes on Tencent Cloud: From Zero to Running Intro Tencent Cloudâ€™s managed k8s had been around for a while and supports joining your own hosts. Time to experiment.\nWhat is k8s (quickly)? Kubernetes is an open-source system for automating deployment, scaling, and management of containerized apps. It groups containers into logical units for easy management and discovery.\nPlainly: we containerized our services already; k8s orchestrates and manages those containers.\nStep 1: Create a Cluster In your cloud console, find â€œContainer Serviceâ€, create a managed cluster, click through the wizard. Join your existing or new instances as worker nodes. Two node types:\nmaster: runs k8s control-plane (etcd, etc.) worker: runs your workloads (Pods) Managed clusters hide master ops; you manage/join your workers.\nStep 2: kubectl Install kubectl and connect to the cluster (see your cloud docs). Verify:\n1 kubectl version Concepts: Pod and Service Pod: one or more container instances of an app. Service: stable, virtual IP/endpoint inside the cluster that load-balances to Pod replicas. Example: A scalable web API runs 3 replicas in one Deployment/Pod template; a Service exposes a single endpoint that forwards to replicas.\nFirst Deployment \u0026amp; Service Save as sample.yaml, then run kubectl apply -f sample.yaml.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: first-web-size name: first-web-size spec: replicas: 2 selector: matchLabels: run: first-web-size strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: run: first-web-size spec: containers: - image: mcr.microsoft.com/dotnet/core/samples:aspnetapp imagePullPolicy: IfNotPresent name: first-web-size livenessProbe: httpGet: path: /v1/health port: 80 initialDelaySeconds: 120 periodSeconds: 30 resources: limits: cpu: 500m memory: 512Mi requests: cpu: 50m memory: 128Mi dnsPolicy: ClusterFirst restartPolicy: Always terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: name: first-web-size annotations: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxx spec: selector: run: first-web-size ports: - protocol: TCP port: 80 targetPort: 80 type: LoadBalancer externalTrafficPolicy: Cluster Apply and check:\n1 2 3 kubectl apply -f sample.yaml kubectl get pods kubectl get service first-web-size From a host on the same VPC:\n1 curl http://\u0026lt;CLUSTER-IP\u0026gt; Cleanup:\n1 kubectl delete -f sample.yaml ","date":"2019-06-09T00:00:00Z","permalink":"https://liguobao.github.io/p/k8s-from-first-steps-to-cleanup-on-tencent-cloud/","title":"K8s From First Steps to Cleanup on Tencent Cloud"},{"content":"macOS Installing pycrypto â€” Clang â€˜stdio.hâ€™ Not Found While installing a Python package that builds a C extension with Clang, I kept seeing:\n1 2 3 4 5 6 clang-4.0: warning: argument unused during compilation: \u0026#39;-L/usr/local/lib\u0026#39; [-Wunused-command-line-argument] src/_fastmath.c:29:10: fatal error: \u0026#39;stdio.h\u0026#39; file not found #include \u0026lt;stdio.h\u0026gt; ^~~~~~~~~ 1 error generated. error: command \u0026#39;clang\u0026#39; failed with exit status 1 Checked Clang:\n1 2 3 4 clang --version clang version 4.0.1 (tags/RELEASE_401/final) Target: x86_64-apple-darwin18.5.0 InstalledDir: /usr/local/opt/llvm@4/bin Looks fineâ€¦ Tried setting env:\n1 export \u0026#34;CFLAGS=-I/usr/local/include -L/usr/local/lib\u0026#34; No luck. From Stack Overflow, the fix was to install SDK headers:\n1 2 3 cd /Library/Developer/CommandLineTools/Packages open macOS_SDK_headers_for_macOS_xx.pgk # choose the pkg matching your macOS version Run the installer UI, proceedâ€¦ It works. Then pip install pycrypto succeeds.\n","date":"2019-05-09T00:00:00Z","permalink":"https://liguobao.github.io/p/macos-installing-pycrypto-clang-stdio.h-not-found/","title":"macOS Installing pycrypto â€” Clang â€˜stdio.hâ€™ Not Found"},{"content":"Server-Side Caching for APIs with an Action Filter Instead of sprinkling cache lookups/writes across controllers, add a reusable ActionFilter that caches responses in Redis based on the request path + querystring.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public class DefaultCacheFilterAttribute : ActionFilterAttribute { protected TimeSpan _expireTime; // override in subclass for custom TTLs private readonly RedisService _redisService; public DefaultCacheFilterAttribute(RedisService redisService) =\u0026gt; _redisService = redisService; public override void OnActionExecuting(ActionExecutingContext context) { if (context.HttpContext.Request.Query.ContainsKey(\u0026#34;refresh\u0026#34;)) return; var redisKey = GetRequestRedisKey(context.HttpContext); var cached = _redisService.ReadCache\u0026lt;JToken\u0026gt;(redisKey); if (cached != null) context.Result = new ObjectResult(cached); } public override void OnActionExecuted(ActionExecutedContext context) { var obj = (ObjectResult)context.Result; if (obj == null) return; var key = GetRequestRedisKey(context.HttpContext); _redisService.WriteCache(key, JToken.FromObject(obj.Value)); } private KeyConfig GetRequestRedisKey(HttpContext http) { var path = http.Request.Path.Value; if (!string.IsNullOrEmpty(http.Request.QueryString.Value)) path += http.Request.QueryString.Value; if (http.Request.Query.ContainsKey(\u0026#34;refresh\u0026#34;)) path = path.Replace(\u0026#34;?refresh=true\u0026#34;, \u0026#34;\u0026#34;).Replace(\u0026#34;refresh=true\u0026#34;, \u0026#34;\u0026#34;); var key = ConstRedisKey.HTTPRequest.CopyOne(path); if (_expireTime != default) key.ExpireTime = _expireTime; return key; } } public static class ConstRedisKey { public static readonly KeyConfig HTTPRequest = new KeyConfig { Key = \u0026#34;lemon_req_\u0026#34;, ExpireTime = TimeSpan.FromMinutes(30), DBName = 5 }; } Usage:\n1 2 3 4 [HttpGet(\u0026#34;v1/xxx/latest\u0026#34;)] [ServiceFilter(typeof(DefaultCacheFilterAttribute))] public IActionResult GetLatestList([FromQuery] int page = 0, [FromQuery] int pageSize = 30) =\u0026gt; Ok(new { data = _service.LoadLatest(page, pageSize), code = 0 }); Register DefaultCacheFilterAttribute in DI (Startup). Optionally subclass to set different TTLs per action.\n","date":"2019-05-05T00:00:00Z","permalink":"https://liguobao.github.io/p/asp.net-core-add-server-side-cache-via-action-filter/","title":"ASP.NET Core â€” Add Server-Side Cache via Action Filter"},{"content":"Test Coverage with coverlet + ReportGenerator Need coverage for .NET Core? Use coverlet to collect, and ReportGenerator to render HTML.\ncoverlet Install either as a global tool or as an MSBuild package:\n1 dotnet tool install --global coverlet.console or add to your test project:\n1 \u0026lt;PackageReference Include=\u0026#34;coverlet.msbuild\u0026#34; Version=\u0026#34;2.5.0\u0026#34; /\u0026gt; Then run tests with coverage:\n1 2 3 4 dotnet test \\ /p:CollectCoverage=true \\ /p:CoverletOutput=\u0026#39;./results/\u0026#39; \\ /p:CoverletOutputFormat=opencover Outputs coverage.opencover.xml under results/.\nReportGenerator Render humanâ€‘readable reports from XML:\n1 2 dotnet tool install --global dotnet-reportgenerator-globaltool reportgenerator \u0026#39;-reports:UnitTests/results/*.xml\u0026#39; \u0026#39;-targetdir:UnitTests/results\u0026#39; Open UnitTests/results/index.htm to view the coverage report.\n","date":"2019-04-08T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-unit-test-coverage-with-coverlet--reportgenerator/","title":".NET Core Unit Test Coverage with coverlet + ReportGenerator"},{"content":"Adopt the 202 Accepted + polling pattern; implement with Redis Sorted Sets + pub/sub and a background worker to process tasks. Details mirror the Chinese post.\n","date":"2019-04-05T00:00:00Z","permalink":"https://liguobao.github.io/p/task-queue-and-async-api-pattern-in-.net-core/","title":"Task Queue and Async API Pattern in .NET Core"},{"content":"Ops Tale: The Day rm -rf /* Took My Box The Trigger Users reported mobile web showing a blank page. On desktop: same â€” HTML loads, but a core JS fails. Error: â€œWeâ€™re sorry but house doesnâ€™t work properly without JavaScript enabledâ€¦â€. Another network error: ERR_INCOMPLETE_CHUNKED_ENCODING.\nTried the usual:\nRestart the Docker container â€” no luck Roll back build â€” no luck Restart nginx â€” no luck Nginx logs â€” nothing helpful Lightbulb: maybe disk full. df -h: 99.99% used. One Docker container using 34G â€” probably Elasticsearch. Killed it, cleared data, restarted nginx â€” back to normal. Problem solvedâ€¦ for now.\nBut ES needs to come back. Letâ€™s attach and mount a separate data disk.\nThe Mistake Followed the cloud console to attach a disk, partitioned, and attempted to mount. Saw a warning, tried another device, mounted again, then â€œclean up the diskâ€ â€” and ran:\n1 rm -rf ./* Suddenly some files were â€œbusyâ€, then ls vanished, cat vanished, cp failedâ€¦ only cd still worked. Yep â€” I had mounted the system disk path and wiped it.\nPostmortem Didnâ€™t format before mounting; first mount failed Didnâ€™t check why; tried another mount blindly Mounted the system disk path and deleted blindly rm -rf ./* â€” game over Recovery Attempt Services still running via Docker; images kept in registry, so recoverable. But configs/certs lived on the host. With most tools gone, not much to read. A teammate suggested checking for snapshots â€” I had a system image from two weeks earlier.\nRestored from image, updated service images â€” back online.\nTakeaways Separate data and system disks; donâ€™t let app data kill the OS Enable snapshot policy if budget allows; at worst you roll back a week Before destructive ops, create a system image â€” a literal lifesaver ","date":"2018-11-20T00:00:00Z","permalink":"https://liguobao.github.io/p/ops-tale-the-day-rm-rf-/-took-my-box/","title":"Ops Tale The Day rm -rf /* Took My Box"},{"content":"Site moved to a new address New domain: https://house2048.cn/app/house-map/#/ New version link Click here: https://house2048.cn/app/house-map A WeChat Mini Program is online. Follow the â€œäººç”Ÿåˆ é™¤æŒ‡å—â€ official account to get it. Project repo: https://github.com/liguobao/58HouseSearch Latest user guide Guide (Chinese): https://github.com/liguobao/58HouseSearch/blob/master/%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B.md\nFor the latest address, see the guide ","date":"2018-09-04T00:00:00Z","permalink":"https://liguobao.github.io/p/house-search-map-project-migration-house2048.cn/","title":"House Search Map Project Migration (house2048.cn)"},{"content":"Hydra is an OpenID Connect certified OAuth2 server. The post walks through running it with Docker and Postgres/MySQL.\n","date":"2018-07-28T00:00:00Z","permalink":"https://liguobao.github.io/p/weekly-open-source-hydra-oauth2/openid-connect-server/","title":"Weekly Open Source â€” Hydra (OAuth2/OpenID Connect Server)"},{"content":"APM and distributed tracing with SkyWalking; installation links and quick tips.\n","date":"2018-07-15T00:00:00Z","permalink":"https://liguobao.github.io/p/weekly-open-source-apache-skywalking/","title":"Weekly Open Source â€” Apache SkyWalking"},{"content":"Featuring a timed job library for .NET Core and examples of scheduled tasks.\n","date":"2018-07-01T00:00:00Z","permalink":"https://liguobao.github.io/p/weekly-open-source-timed-jobs-in-.net-core/","title":"Weekly Open Source â€” Timed Jobs in .NET Core"},{"content":"Backend API Starter Guide Key concepts and references for building RESTful APIs in common stacks.\nRESTful API Understand HTTP and REST design References: Ruanyifeng: Understanding RESTful Architecture Ruanyifeng: RESTful API Design Guide RESTful API resources collection Dependency Injection (DI) Intro to DI and software architecture patterns Java JDK 1.8+ IDE: IntelliJ IDEA DB: MySQL 5.7+, cache: Redis Data layer: MyBatis (or JPA), build: Maven/Gradle Framework: Spring Boot + Spring MVC Goal: build CRUD Web APIs with Spring Boot C# (.NET) .NET: .NET Core 2.x IDE: VS Code + SDK (or Visual Studio) DB: MySQL 5.7+, cache: Redis Data layer: Dapper Framework: ASP.NET Core MVC with builtâ€‘in DI Goal: build CRUD Web APIs with ASP.NET Core Python Python 3.6+ IDE: VS Code (+ Python extension) or PyCharm DB: MySQL 5.7+, cache: Redis ORM: SQLAlchemy Framework: Flask Goal: build CRUD Web APIs with Flask PHP PHP 7.1+ IDE: VS Code (+ PHP Debug) + nginx + phpâ€‘fpm DB: MySQL 5.7+, cache: Redis Framework: Laravel; build: Composer Goal: build CRUD Web APIs with Laravel Node.js Node.js 9+ IDE: VS Code DB: MySQL 5.7+, cache: Redis ORM: Sequelize (or orm2) Build: npm Framework: Express Goal: build CRUD Web APIs with Express ","date":"2018-06-18T00:00:00Z","permalink":"https://liguobao.github.io/p/backend-api-starter-guide/","title":"Backend API Starter Guide"},{"content":"Overview of configuration providers, options pattern, environmentâ€‘specific json, and injecting typed options.\n","date":"2018-05-31T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-configuration-appsettings-environments-di/","title":".NET Core Configuration â€” AppSettings, Environments, DI"},{"content":"Followâ€‘up to Part 1: scaffolding a web app, controllers/views/routes, DI, and configuration basics.\n","date":"2018-05-30T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-mvc-getting-started-part-2/","title":".NET Core MVC â€” Getting Started (Part 2)"},{"content":".NET Core (Getting Started) Environment Install the .NET SDK: https://www.microsoft.com/net/learn/get-started\nWindows: Visual Studio 2017 (select .NET Core workload) macOS/Linux: SDK + VS Code (+ C# extension) Verify:\n1 2 dotnet --version dotnet help Create a project List templates: dotnet new\nCreate and run a console app:\n1 2 3 dotnet new console -n FirstApplication cd FirstApplication dotnet run Open in VS Code, install the C# extension, and debug (set breakpoints, F5/F10/F11).\nExample loop:\n1 2 3 4 5 for (var i = 0; i \u0026lt; 10; i++) { sum += i; Console.WriteLine($\u0026#34;i:{i}, sum:{sum}\u0026#34;); } About .csproj Console:\n1 2 3 4 5 6 \u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;OutputType\u0026gt;Exe\u0026lt;/OutputType\u0026gt; \u0026lt;TargetFramework\u0026gt;netcoreapp2.0\u0026lt;/TargetFramework\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/Project\u0026gt; Web projects use Microsoft.NET.Sdk.Web and PackageReference entries for dependencies (e.g., ASP.NET Core, Dapper, Newtonsoft.Json). More details in the next part.\n","date":"2018-05-29T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-hands-on-part-1-getting-started/","title":".NET Core Hands-on (Part 1 Getting Started)"},{"content":"Quickstart: use the AMap JS API (Autocomplete, ArrivalRange, Transfer) on the front end and Python 3 on the back end to serve listings. The HTML and scripts from the Chinese post are unchanged.\n","date":"2018-05-23T00:00:00Z","permalink":"https://liguobao.github.io/p/build-a-simple-rental-map-in-30-minutes-python--amap/","title":"Build a Simple Rental Map in 30 Minutes (Python + AMap)"},{"content":"Debugging PHP on macOS with VS Code (xdebug) Windows guide: https://zhuanlan.zhihu.com/p/25844268\nHomebrew Install brew from https://brew.sh or:\n1 /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; PHP7 + nginx + php-fpm + xdebug PHP 7 1 2 3 4 brew install php@7.1 where php # /usr/local/opt/php@7.1/bin/php # /usr/bin/php php.ini is typically under /usr/local/etc/php/7.1 (weâ€™ll need it later).\nInstall xdebug Homebrewâ€™s php71-xdebug was removed. Build manually. Download from https://xdebug.org/files/ and compile:\n1 2 3 4 5 6 7 8 mkdir ~/tool \u0026amp;\u0026amp; cd ~/tool wget https://xdebug.org/files/xdebug-2.6.0.tgz tar xvzf xdebug-2.6.0.tgz cd xdebug-2.6.0 phpize ./configure --enable-xdebug --with-php-config=/usr/local/Cellar/php@7.1/7.1.17/bin/php-config make \u0026amp;\u0026amp; make test # xdebug-2.6.0/modules/xdebug.so will be generated Install nginx 1 brew install nginx nginx.conf Create directories and a minimal config:\n1 2 3 mkdir -p /usr/local/var/logs/nginx mkdir -p /usr/local/etc/nginx/{sites-available,sites-enabled,conf.d,ssl} sudo mkdir -p /var/www \u0026amp;\u0026amp; sudo chown :staff /var/www \u0026amp;\u0026amp; sudo chmod 777 /var/www /usr/local/etc/nginx/nginx.conf:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 user root wheel; worker_processes 1; error_log /usr/local/var/logs/nginx/error.log debug; pid /usr/local/var/run/nginx.pid; events { worker_connections 256; } http { include mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /usr/local/var/logs/access.log main; sendfile on; keepalive_timeout 65; port_in_redirect off; include /usr/local/etc/nginx/sites-enabled/*; } /usr/local/etc/nginx/conf.d/php-fpm:\n1 2 3 4 5 6 7 location ~ \\.php$ { try_files $uri =404; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_intercept_errors on; include /usr/local/etc/nginx/fastcgi.conf; } Site file /usr/local/etc/nginx/sites-available/default:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 server { listen 80; server_name example.local; root /var/www/pet/public; access_log /usr/local/var/logs/nginx/default.access.log main; index index.php index.html index.htm; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } Enable the site by symlinking into sites-enabled, configure php-fpm and xdebug in php.ini, and set up VS Codeâ€™s PHP debug extension.\n","date":"2018-05-21T00:00:00Z","permalink":"https://liguobao.github.io/p/debugging-php-on-macos-with-vs-code-xdebug/","title":"Debugging PHP on macOS with VS Code (xdebug)"},{"content":"logstash grok Configuration Patterns logstash.conf Configure grok { match =\u0026gt; ... } to split logs into indexed fields (matches are essentially regex patterns).\nRaw log:\n1 2018-04-13 16:03:49.822 INFO o.n.p.j.c.XXXXX - Star Calculator grok match:\n1 match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{DATA:log_date} %{TIME:log_localtime} %{WORD:log_type} %{JAVAFILE:log_file} - %{GREEDYDATA:log_content}\u0026#34;} Parsed fields:\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;log_date\u0026#34;: [[\u0026#34;2018-04-13\u0026#34;]], \u0026#34;log_localtime\u0026#34;: [[\u0026#34;16:03:49.822\u0026#34;]], \u0026#34;HOUR\u0026#34;: [[\u0026#34;16\u0026#34;]], \u0026#34;MINUTE\u0026#34;: [[\u0026#34;03\u0026#34;]], \u0026#34;SECOND\u0026#34;: [[\u0026#34;49.822\u0026#34;]], \u0026#34;log_type\u0026#34;: [[\u0026#34;INFO\u0026#34;]], \u0026#34;log_file\u0026#34;: [[\u0026#34;o.n.p.j.c.XXXX\u0026#34;]], \u0026#34;log_content\u0026#34;: [[\u0026#34;Star Calculator\u0026#34;]] } All extracted fields are ES mapping indices and can be queried.\nTest patterns at: http://grokdebug.herokuapp.com/ and view available patterns at http://grokdebug.herokuapp.com/patterns\nWe use a config like /logstash/logstash-k8s.conf.\nQ: How to force data types in mapping? A: grok is regex-based; defaults are strings. You can cast numeric fields inline, e.g. %{NUMBER:response_time:int}.\nFull example:\n1 2 3 match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{DATA:log_date} %{TIME:log_localtime} %{WORD:log_type} %{JAVAFILE:log_file} - %{WORD:method} %{URIPATHPARAM:uri} %{NUMBER:status:int} %{NUMBER:size:int} %{NUMBER:response_time:int}\u0026#34; } Q: Store indices by date? A: In output, set index format, e.g. index =\u0026gt; \u0026quot;k8s-%{+YYYY.MM.dd}\u0026quot;.\nFull output:\n1 2 3 4 5 6 7 output { elasticsearch { hosts =\u0026gt; \u0026#34;${ES_URL}\u0026#34; manage_template =\u0026gt; false index =\u0026gt; \u0026#34;k8s-%{+YYYY.MM.dd}\u0026#34; } } Full logstash.conf:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 input { beats { host =\u0026gt; \u0026#34;0.0.0.0\u0026#34; port =\u0026gt; 5043 } } filter { if [type] == \u0026#34;kube-logs\u0026#34; { mutate { rename =\u0026gt; [\u0026#34;log\u0026#34;, \u0026#34;message\u0026#34;] } date { match =\u0026gt; [\u0026#34;time\u0026#34;, \u0026#34;ISO8601\u0026#34;] remove_field =\u0026gt; [\u0026#34;time\u0026#34;] } grok { match =\u0026gt; { \u0026#34;source\u0026#34; =\u0026gt; \u0026#34;/var/log/containers/%{DATA:pod_name}_%{DATA:namespace}_%{GREEDYDATA:container_name}-%{DATA:container_id}.log\u0026#34; } match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{DATA:log_date} %{TIME:log_localtime} %{WORD:log_type} %{JAVAFILE:log_file} - %{WORD:method} %{URIPATHPARAM:uri} %{NUMBER:status:int} %{NUMBER:size:int} %{NUMBER:response_time:int}\u0026#34; } remove_field =\u0026gt; [\u0026#34;source\u0026#34;] break_on_match =\u0026gt; false } } } output { elasticsearch { hosts =\u0026gt; \u0026#34;${ES_URL}\u0026#34; manage_template =\u0026gt; false index =\u0026gt; \u0026#34;k8s-%{+YYYY.MM.dd}\u0026#34; } } ","date":"2018-05-20T00:00:00Z","permalink":"https://liguobao.github.io/p/logstash-grok-configuration-patterns/","title":"logstash grok Configuration Patterns"},{"content":"Hexo Install and scaffold:\n1 2 3 4 5 npm install hexo-cli -g hexo init blog cd blog npm install hexo server Docs: https://hexo.io\nDocker Deployment Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 FROM node:latest AS build-env RUN mkdir -p /usr/src/hexo-blog WORKDIR /usr/src/hexo-blog COPY . . RUN npm --registry=https://registry.npm.taobao.org install hexo-cli -g \u0026amp;\u0026amp; npm install RUN hexo clean \u0026amp;\u0026amp; hexo g FROM nginx:latest ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone WORKDIR /usr/share/nginx/html COPY --from=build-env /usr/src/hexo-blog/public /usr/share/nginx/html EXPOSE 80 Build and run:\n1 2 docker build -t hexo-blog:latest . docker run -p 80:80 -d hexo-blog:latest Nginx TLS Config Obtain a cert (e.g., via https://freessl.org/), then use an nginx.conf like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 events { worker_connections 1024; } http { server { listen 443; server_name example.com; root /usr/share/nginx/html; index index.html index.htm; ssl on; ssl_certificate /etc/nginx/full_chain.pem; ssl_certificate_key /etc/nginx/private.key; include /etc/nginx/mime.types; gzip on; gzip_min_length 5k; gzip_buffers 4 16k; gzip_http_version 1.1; gzip_comp_level 3; gzip_types text/plain application/json application/javascript text/css application/xml text/javascript image/jpeg image/gif image/png; gzip_vary on; } server { listen 80; server_name example.com; root /usr/share/nginx/html; index index.html index.htm; include /etc/nginx/mime.types; } } Run with mounts for config and certs:\n1 2 3 4 5 6 docker run -p 80:80 -p 443:443 \\ --name hexo-site \\ -v ~/docker-data/site/nginx.conf:/etc/nginx/nginx.conf \\ -v ~/docker-data/site/ssl/full_chain.pem:/etc/nginx/full_chain.pem \\ -v ~/docker-data/site/ssl/private.key:/etc/nginx/private.key \\ --restart=always -d hexo-blog:latest Example reverse-proxy config (non-static): define upstreams and proxy_pass in server blocks as needed.\n","date":"2018-05-18T00:00:00Z","permalink":"https://liguobao.github.io/p/build-and-deploy-a-hexo-blog-with-docker/","title":"Build and Deploy a Hexo Blog with Docker"},{"content":"Install JDK 8, then run Jenkins from the war:\n1 2 3 4 5 6 7 8 sudo add-apt-repository ppa:webupd8team/java sudo apt-get update sudo apt-get install oracle-java8-installer wget http://mirrors.jenkins.io/war-stable/2.107.2/jenkins.war mkdir ~/jenkins-home \u0026amp;\u0026amp; export JENKINS_HOME=~/jenkins-home tmux java -jar jenkins.war Open port 8080, use the initial admin password from .../secrets/initialAdminPassword, install suggested plugins, create admin.\nWeâ€™ll return to Jenkins after preparing Docker builds for our .NET Core app.\nDockerfile for .NET Core In your project folder, Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 FROM microsoft/aspnetcore-build:2.0 AS build-env WORKDIR /app COPY *.csproj ./ RUN dotnet restore COPY . ./ RUN dotnet publish -c Release -o out FROM microsoft/aspnetcore:2.0 WORKDIR /app COPY --from=build-env /app/out . ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;YourApp.dll\u0026#34;] Optional local build/run script 1 2 3 4 5 6 7 image_version=$(date +%Y%m%d%H%M) docker stop house-web || true \u0026amp;\u0026amp; docker rm house-web || true docker build -t house-web:$image_version . docker run -p 8080:80 \\ -v ~/docker-data/house-web/appsettings.json:/app/appsettings.json \\ -v ~/docker-data/house-web/NLogFile/:/app/NLogFile \\ --restart=always --name house-web -d house-web:$image_version Jenkins Job New item â†’ Freestyle project Source Code Management â†’ Git (configure credentials/keys as needed) Build â†’ â€œExecute shellâ€ with either the local build script above, or a remote ssh user@host '~/start_XXX.sh'. Better builds with Aliyun Container Registry (ACR) Instead of building on Jenkins/host, connect your repo to Aliyun Container Registry (ACR) and enable â€œauto build on code changesâ€. ACR will build and host your images.\nThen, use a webhook from ACR to trigger Jenkins after a successful build.\nGeneric Webhook Trigger Install the â€œGeneric Webhook Triggerâ€ plugin. The trigger URL looks like:\n1 http://\u0026lt;user\u0026gt;:\u0026lt;apiToken\u0026gt;@\u0026lt;jenkins-host\u0026gt;:8080/generic-webhook-trigger/invoke?token=\u0026lt;job-token\u0026gt; Set the same token under the jobâ€™s â€œTrigger builds remotelyâ€. Disable CSRF protection if needed for webhook calls.\nFlow:\nrepo change â†’ ACR auto build â†’ ACR webhook â†’ Jenkins job â†’ deployment script pulls and runs the new image.\nDeployment step becomes:\n1 2 3 docker stop house-web || true \u0026amp;\u0026amp; docker rm house-web || true docker pull \u0026lt;your-acr-image\u0026gt; docker run --restart=always --name house-web \u0026lt;your-acr-image\u0026gt; Summary Stand up Jenkins. 2) Add Dockerfile. 3) Use ACR to build images. 4) Use ACR webhook â†’ Jenkins to deploy. 5) Works for other stacks too â€” just change the Dockerfile. ","date":"2018-05-08T00:00:00Z","permalink":"https://liguobao.github.io/p/ci/cd-auto-deploy-.net-core-with-jenkins--docker/","title":"CI/CD Auto-Deploy .NET Core with Jenkins + Docker"},{"content":"Previously I posted on V2EX about this tool:\nNew interaction update for â€œMapâ€‘based Rental Searchâ€: https://www.v2ex.com/t/530011 Scraped â€œCityâ€â€‘tagged sublet/rental posts into the app: https://www.v2ex.com/t/509447#reply8 Ad slot + major frontâ€‘end revamp + mini program live: https://www.v2ex.com/t/497674#reply4 Mapâ€‘based rental search: https://house-map.cn/#/ This time itâ€™s for real. The earlier mini program failed qualification; later I teamed up with a classmate to ship it.\nFeatures City listings Screenshots show perâ€‘city feeds and map overlays.\nPerâ€‘platform views Group sources (e.g., different sites) with individual tabs.\nFiltered search Filter by administrative district Filter by â€œwithin N daysâ€ recency Filter by listing type + price Listing details + map Detail pages include map view/location.\nFavorites + feedback Star listings and submit feedback inâ€‘app.\nWhatâ€™s new (Beta 9) District filters; cityâ€‘only fallback when no district â€œWithin N daysâ€ filter Listing type filter; combined with price Show publish time in lists Bug fixes + new bugs Faster loads via caching Added an â€œadâ€ slot About the ad slot It doesnâ€™t earn much; cumulative project income over 2â€“3 years is around âˆ’Â¥10,000. Â¯\\(ãƒ„)/Â¯\nFollow the WeChat account â€œäººç”Ÿåˆ é™¤æŒ‡å—â€ to get the mini program, or scan the QR codes in the original post.\nNotes\nNewly launched; expect bugs. Thanks for your patience! Suggestions welcome via the account or email. Be kind :-) ","date":"2018-03-22T00:00:00Z","permalink":"https://liguobao.github.io/p/housemap-mini-program-launched/","title":"Houseâ€‘Map Mini Program Launched"},{"content":"Consumers and Producers in ASP.NET Core Docs: http://www.rabbitmq.com/ â€” .NET client: http://www.rabbitmq.com/dotnet.html\nConsumer with IHostedService Run consumers with the app lifecycle using IHostedService.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class RabbitListener : IHostedService { private readonly IConnection _conn; private readonly IModel _channel; protected string RouteKey; protected string QueueName; public RabbitListener(IOptions\u0026lt;AppConfiguration\u0026gt; options) { var f = new ConnectionFactory { HostName = options.Value.RabbitHost, UserName = options.Value.RabbitUserName, Password = options.Value.RabbitPassword, Port = options.Value.RabbitPort, }; _conn = f.CreateConnection(); _channel = _conn.CreateModel(); } public Task StartAsync(CancellationToken ct) { Register(); return Task.CompletedTask; } public Task StopAsync(CancellationToken ct) { _conn.Close(); return Task.CompletedTask; } public virtual bool Process(string message) =\u0026gt; throw new NotImplementedException(); void Register() { _channel.ExchangeDeclare(exchange: \u0026#34;message\u0026#34;, type: \u0026#34;topic\u0026#34;); _channel.QueueDeclare(queue: QueueName, exclusive: false); _channel.QueueBind(queue: QueueName, exchange: \u0026#34;message\u0026#34;, routingKey: RouteKey); var consumer = new EventingBasicConsumer(_channel); consumer.Received += (m, ea) =\u0026gt; { var msg = Encoding.UTF8.GetString(ea.Body); if (Process(msg)) _channel.BasicAck(ea.DeliveryTag, false); }; _channel.BasicConsume(queue: QueueName, consumer: consumer); } } Subclass and resolve scoped services via IServiceProvider.CreateScope() inside Process.\nRegister: services.AddHostedService\u0026lt;YourListener\u0026gt;();\nProducer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class RabbitMQClient { private readonly IModel _channel; private readonly ILogger _logger; public RabbitMQClient(IOptions\u0026lt;AppConfiguration\u0026gt; opts, ILogger\u0026lt;RabbitMQClient\u0026gt; logger) { var f = new ConnectionFactory { HostName = opts.Value.RabbitHost, UserName = opts.Value.RabbitUserName, Password = opts.Value.RabbitPassword, Port = opts.Value.RabbitPort, }; var conn = f.CreateConnection(); _channel = conn.CreateModel(); _logger = logger; } public void PushMessage(string routingKey, object message) { _logger.LogInformation($\u0026#34;PushMessage,routingKey:{routingKey}\u0026#34;); _channel.QueueDeclare(queue: \u0026#34;message\u0026#34;, durable: false, exclusive: false, autoDelete: false); var body = Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(message)); _channel.BasicPublish(exchange: \u0026#34;message\u0026#34;, routingKey: routingKey, basicProperties: null, body: body); } } Register as singleton: services.AddSingleton\u0026lt;RabbitMQClient\u0026gt;();\n","date":"2018-01-15T00:00:00Z","permalink":"https://liguobao.github.io/p/using-rabbitmq-in-asp.net-core-the-right-way/","title":"Using RabbitMQ in ASP.NET Core the Right Way"},{"content":"Checklist and lessons moving workloads within/out of Tencent Cloud: networking, storage, DNS cutover, and monitoring.\n","date":"2018-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/migrating-services-on-tencent-cloud-notes/","title":"Migrating Services on Tencent Cloud â€” Notes"},{"content":"Using online schema changes to avoid downtime; tools and strategies (native InnoDB online DDL, gh-ost/pt-osc), caveats and verification.\n","date":"2018-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/mysql-online-ddl-practical-guide/","title":"MySQL Online DDL â€” Practical Guide"},{"content":"Highlights: using Helm charts, persistent storage, external access, runners, and upgrades. Common pitfalls and resource sizing.\n","date":"2018-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/running-gitlab-on-kubernetes-notes/","title":"Running GitLab on Kubernetes â€” Notes"},{"content":"Features overview and setup notes for SkyWalkingâ€™s commercial extensions.\n","date":"2018-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/skywalking-x-pack-notes/","title":"SkyWalking X-Pack â€” Notes"},{"content":"Pattern: create scopes in background workers and resolve DbContext from IServiceProvider.CreateScope() to respect scoped lifetimes.\n","date":"2018-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/using-ef-dbcontext-in-background-tasks-ihostedservice/","title":"Using EF DbContext in Background Tasks (IHostedService)"},{"content":"Configuring signature, APIs, and common pitfalls when integrating the WeChat JS SDK in web apps.\n","date":"2018-01-01T00:00:00Z","permalink":"https://liguobao.github.io/p/wechat-js-sdk-integration-notes/","title":"WeChat JS SDK Integration Notes"},{"content":"Linux Ops Cheatsheet â€” Install, Config, and Quick Commands Ports \u0026amp; Processes 1 2 3 4 5 6 # Check whatâ€™s using a port netstat -anp | grep \u0026#34;5000\u0026#34; # Kill a process kill -9 2553 # Run in background nohup command \u0026amp; Install Shadowsocks (SS) 1 2 3 4 5 6 apt-get update # Install ss apt-get install python-pip pip install shadowsocks # Run ss server nohup ssserver -s \u0026lt;IP_ADDRESS\u0026gt; -k \u0026lt;PASSWORD\u0026gt; \u0026amp; Install MySQL 5.7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Ref: http://tecadmin.net/install-mysql-5-on-ubuntu/ sudo apt-get install software-properties-common sudo add-apt-repository -y ppa:ondrej/mysql-5.7 sudo apt-get update sudo apt-get install mysql-server # Edit my.cnf to allow remote login # /etc/mysql/my.cnf # set bind-address = 0.0.0.0 # Restart MySQL /etc/init.d/mysql restart MySQL: Add Users 1 2 3 4 5 6 7 8 9 -- Grant all to root from anywhere GRANT ALL PRIVILEGES ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;root\u0026#39;; FLUSH PRIVILEGES; -- Example: specific privileges for a host GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP ON \u0026lt;db\u0026gt;.* TO \u0026#39;joe\u0026#39;@\u0026#39;10.163.225.87\u0026#39; IDENTIFIED BY \u0026#39;123\u0026#39;; -- Another example GRANT ALL ON xxxDB.* TO \u0026#39;xxx\u0026#39;@\u0026#39;%\u0026#39;; MySQL error: â€œChecking for tables which need an upgrade, are corrupt or were not closed cleanlyâ€ Steps:\n1 2 3 4 5 6 7 8 9 10 11 12 sudo service mysql stop sudo /etc/init.d/apparmor reload sudo service mysql start # Or: if multiple mysql instances/processes are running ps -A | grep mysql sudo pkill mysql ps -A | grep mysqld sudo pkill mysqld service mysql restart mysql -u root -p 7z Quick Ops 1 2 3 4 # Compress 7z a -t7z -r manager.7z /home/manager/* # Extract 7z x xx.zip Tencent Cloud: change password, allow root login 1 2 3 sudo passwd root sudo vi /etc/ssh/sshd_config sudo service ssh restart ","date":"2017-03-25T00:00:00Z","permalink":"https://liguobao.github.io/p/linux-ops-cheatsheet-install-config-and-quick-commands/","title":"Linux Ops Cheatsheet â€” Install, Config, and Quick Commands"},{"content":"Debugging PHP in VS Code (Windows) macOS guide: https://zhuanlan.zhihu.com/p/37128419\nStack: VS Code + PHP Debug extension + phpStudy + Xdebug.\nInstall VS Code Download from https://code.visualstudio.com/ and install.\nInstall PHP Debug extension Search for â€œPHP Debugâ€ in the Marketplace and install it.\nphpStudy Use phpStudy for a quick PHP + Apache + MySQL stack. Switch PHP versions as needed.\nVerify phpMyAdmin opens and connects locally. Then pick your target PHP version.\nEnable Xdebug phpStudy ships with Xdebug. Open php.ini (Other options â†’ Open config file â†’ php-ini) and add:\n1 2 3 xdebug.remote_enable = 1 xdebug.remote_autostart = 1 zend_extension=\u0026#34;C:\\\\phpStudy\\\\php\\\\php-5.5.38\\\\ext\\\\php_xdebug.dll\u0026#34; Restart phpStudy services.\nVS Code settings and launch config Set php.validate.executablePath to your php.exe path (from phpStudy). Then add a PHP debug configuration (Listen for Xdebug) in launch.json.\nStart debugging, set breakpoints, and hit the target page â€” VS Code should break into the PHP code. Use F10/F11/F5 as usual.\nNotes for other stacks: configure php.ini, install the matching Xdebug DLL, and set php.validate.executablePath correctly.\n","date":"2017-03-17T00:00:00Z","permalink":"https://liguobao.github.io/p/debug-php-with-visual-studio-code-on-windows/","title":"Debug PHP with Visual Studio Code on Windows"},{"content":"A Hands-On Guide to Writing a Crawler with .NET Core Preface After migrating 58HouseSearch to .NET Core, I launched another side project, Dy2018Crawler, to crawl movie listings from dy2018. This post outlines the approach for building a crawler with .NET Core.\nSetup (.NET Core) Install the .NET Core SDK (crossâ€‘platform). With SDK installed, any editor works. For convenience, the VS .NET Core templates are fine to start with.\nAnatomy of a Crawler Analyze the page Identify where the data lives in the HTML (ids, classes, attributes). For dy2018â€™s homepage, movie items live inside div.co_content222, with details in a elements.\nGoal: find the div.co_content222, then extract all a links from within.\nCode I use AngleSharp for HTML parsing in .NET.\nProject: https://anglesharp.github.io/ NuGet: Install-Package AngleSharp Fetch movie list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 private static HtmlParser htmlParser = new HtmlParser(); private ConcurrentDictionary\u0026lt;string, MovieInfo\u0026gt; _cdMovieInfo = new ConcurrentDictionary\u0026lt;string, MovieInfo\u0026gt;(); private void AddToHotMovieList() { Task.Factory.StartNew(() =\u0026gt; { var htmlDoc = HTTPHelper.GetHTMLByURL(\u0026#34;http://www.dy2018.com/\u0026#34;); var dom = htmlParser.Parse(htmlDoc); var lstDivInfo = dom.QuerySelectorAll(\u0026#34;div.co_content222\u0026#34;); if (lstDivInfo != null) { foreach (var divInfo in lstDivInfo.Take(3)) { divInfo.QuerySelectorAll(\u0026#34;a\u0026#34;).Where(a =\u0026gt; a.GetAttribute(\u0026#34;href\u0026#34;).Contains(\u0026#34;/i/\u0026#34;)).ToList().ForEach(a =\u0026gt; { var onlineURL = \u0026#34;http://www.dy2018.com\u0026#34; + a.GetAttribute(\u0026#34;href\u0026#34;); // ... add to dictionary, etc. }); } } }); } Fetch movie details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 private MovieInfo FillMovieInfoFormWeb(AngleSharp.Dom.IElement a, string onlineURL) { var movieHTML = HTTPHelper.GetHTMLByURL(onlineURL); var movieDoc = htmlParser.Parse(movieHTML); var zoom = movieDoc.GetElementById(\u0026#34;Zoom\u0026#34;); var lstDownLoadURL = movieDoc.QuerySelectorAll(\u0026#34;[bgcolor=\u0026#39;#fdfddf\u0026#39;]\u0026#34;); var updatetime = movieDoc.QuerySelector(\u0026#34;span.updatetime\u0026#34;); var pubDate = DateTime.Now; if (updatetime != null \u0026amp;\u0026amp; !string.IsNullOrEmpty(updatetime.InnerHtml)) { DateTime.TryParse(updatetime.InnerHtml.Replace(\u0026#34;å‘å¸ƒæ—¶é—´ï¼š\u0026#34;, \u0026#34;\u0026#34;), out pubDate); } var movieInfo = new MovieInfo { MovieName = a.InnerHtml.Replace(\u0026#34;\u0026lt;font color=\\\u0026#34;#0c9000\\\u0026#34;\u0026gt;\u0026#34;,\u0026#34;\u0026#34;).Replace(\u0026#34;\u0026lt;font color=\\\u0026#34;\\t#0c9000\\\u0026#34;\u0026gt;\u0026#34;,\u0026#34;\u0026#34;).Replace(\u0026#34;\u0026lt;/font\u0026gt;\u0026#34;, \u0026#34;\u0026#34;), Dy2018OnlineUrl = onlineURL, MovieIntro = zoom != null ? WebUtility.HtmlEncode(zoom.InnerHtml) : \u0026#34;æš‚æ— ä»‹ç»...\u0026#34;, XunLeiDownLoadURLList = lstDownLoadURL?.Select(d =\u0026gt; d.FirstElementChild.InnerHtml).ToList(), PubDate = pubDate, }; return movieInfo; } HTTPHelper dy2018 uses GB2312. .NET Core needs System.Text.Encoding.CodePages and Encoding.RegisterProvider(CodePagesEncodingProvider.Instance) to support it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public static string GetHTMLByURL(string url) { try { var wRequest = System.Net.WebRequest.Create(url); wRequest.ContentType = \u0026#34;text/html; charset=gb2312\u0026#34;; wRequest.Method = \u0026#34;get\u0026#34;; var wResp = wRequest.GetResponseAsync().Result; using (var reader = new StreamReader(wResp.GetResponseStream(), Encoding.GetEncoding(\u0026#34;GB2312\u0026#34;))) { return reader.ReadToEnd(); } } catch { return string.Empty; } } Scheduled jobs Use Pomelo.AspNetCore.TimedJob for scheduled tasks.\nNuGet: Pomelo.AspNetCore.TimedJob Register in Startup:\n1 2 services.AddTimedJob(); app.UseTimedJob(); Define a job:\n1 2 3 4 5 6 7 8 public class AutoGetMovieListJob : Job { [Invoke(Begin = \u0026#34;2016-11-29 22:10\u0026#34;, Interval = 1000 * 3600 * 3, SkipWhileExecuting = true)] public void Run() { // logic } } Publish Adjust project.json (for older tooling) â€” add runtimes, comment out scripts (if Node/Bower not present), and remove type under Microsoft.NETCore.App.\nBuild \u0026amp; publish:\n1 2 dotnet restore dotnet publish -r ubuntu.14.04-x64 -c Release -o \u0026#34;C:\\\\code\\\\website\\\\Dy2018Crawler\u0026#34; Code: https://github.com/liguobao/Dy2018Crawler\nLive: http://codelover.win/\n","date":"2016-12-04T00:00:00Z","permalink":"https://liguobao.github.io/p/a-hands-on-guide-to-writing-a-crawler-with-.net-core/","title":"A Hands-On Guide to Writing a Crawler with .NET Core"},{"content":"Auto-Rebase Multiple Git Repositories When preparing releases, I often need to update several repos/branches before merging. Typing the same commands repeatedly gets old, so hereâ€™s a small shell script to rebase multiple repos in one go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 printf \u0026#34;Start rebase 58HouseSearch.\\n\u0026#34; cd ./58HouseSearch git checkout master git pull --rebase origin master printf \u0026#34;Finish Pull Rebase 58HouseSearch master.\\n\u0026#34; read -p \u0026#34;Press any key to continue.\u0026#34; cd .. printf \u0026#34;Start rebase hexoforblog.\\n\u0026#34; cd ./hexoforblog git checkout master git pull --rebase origin master printf \u0026#34;Finish Pull Rebase hexoforblog.\\n\u0026#34; read -p \u0026#34;Press any key to continue.\u0026#34; cd .. Notes:\nprintf prints progress messages read -p pauses for confirmation between steps Output mirrors what you see in Git Bash ","date":"2016-11-25T00:00:00Z","permalink":"https://liguobao.github.io/p/script-to-auto-rebase-multiple-git-repositories/","title":"Script to Auto-Rebase Multiple Git Repositories"},{"content":"Preface Early on, JS went straight into script tags. As projects grew, we split into .js files and ordered \u0026lt;script\u0026gt; tags to model dependencies â€” brittle and blocking.\nRequireJS (AMD) addresses both: it loads modules asynchronously and manages dependencies explicitly.\nInclude RequireJS 1 \u0026lt;script src=\u0026#34;js/require.js\u0026#34; defer async=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Use data-main to specify the entry module:\n1 \u0026lt;script src=\u0026#34;js/require.js\u0026#34; data-main=\u0026#34;js/home\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Configure In home.js:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 require.config({ baseUrl: \u0026#39;/DomainJS/\u0026#39;, paths: { jquery: \u0026#39;lib/jquery-1.11.3.min\u0026#39;, AMUI: \u0026#39;lib/amazeui.2.7.1.min\u0026#39;, \u0026#39;jquery.range\u0026#39;: \u0026#39;lib/jquery.range\u0026#39;, es5: \u0026#39;lib/es5\u0026#39;, mapController: \u0026#39;mapController\u0026#39;, addToolbar: \u0026#39;addToolbar\u0026#39; }, shim: { addToolbar: { deps: [\u0026#39;jquery\u0026#39;] }, \u0026#39;jquery.range\u0026#39;: { deps: [\u0026#39;jquery\u0026#39;] } } }); Load modules 1 2 3 4 5 6 7 8 9 10 require([\u0026#39;domready!\u0026#39;, \u0026#39;jquery\u0026#39;, \u0026#39;AMUI\u0026#39;, \u0026#39;mapController\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;commuteGo\u0026#39;], function (doc, $, AMUI, mapController, city, commuteGo) { city.initAllCityInfo(); mapController.init(); $(\u0026#34;input[name=\u0026#39;locationType\u0026#39;]\u0026#34;).on(\u0026#39;click\u0026#39;, mapController.locationMethodOnChange); $(\u0026#34;input[name=\u0026#39;vehicle\u0026#39;]\u0026#34;).on(\u0026#39;click\u0026#39;, commuteGo.go); // ... omitted ajax and UI code ... }); The first parameter lists dependencies; the callback runs after all load successfully.\nWrite AMD modules No deps:\n1 2 3 4 5 6 7 8 9 10 // helper.js define(function () { var getQueryString = function (name) { var reg = new RegExp(\u0026#39;(^|\u0026amp;)\u0026#39; + name + \u0026#39;=([^\u0026amp;]*)(\u0026amp;|$)\u0026#39;); var r = window.location.search.substr(1).match(reg); if (r != null) return unescape(r[2]); return null; }; return { getQueryString: getQueryString }; }); With deps:\n1 2 3 4 // marker.js define([\u0026#39;mapSignleton\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;transfer\u0026#39;], function (mapSignleton, city, transfer) { // ... }); For more on config and optimization, see:\nhttps://segmentfault.com/a/1190000002401665 https://segmentfault.com/a/1190000002403806 ","date":"2016-10-22T00:00:00Z","permalink":"https://liguobao.github.io/p/writing-modular-code-with-requirejs/","title":"Writing Modular Code with RequireJS"},{"content":"Commonly used Visual Studio shortcuts:\nCtrl+S â€” Save (the allâ€‘time #1) Ctrl+Shift+S â€” Save all open files Ctrl+O â€” Open file Ctrl+Shift+O â€” Open project Ctrl+Shift+A â€” Add new item to current project Ctrl+F4 â€” Close current tab Ctrl+F6 â€” Next tab Ctrl+Shift+F6 â€” Previous tab F12 â€” Go to definition (functions/variables) Ctrl+â€“ â€” Navigate back Shift+Ctrl+â€“ â€” Navigate forward Ctrl+Tab â€” Switch tabs Ctrl+F â€” Find Ctrl+F3 â€” Find selection Ctrl+K, Ctrl+D â€” Format document Ctrl+K, Ctrl+F â€” Format selection Ctrl+K, Ctrl+C â€” Comment selection Ctrl+K, Ctrl+U â€” Uncomment selection Ctrl+L â€” Delete line / selection Shift+Alt+Enter â€” Toggle full screen for editor Ctrl+M, Ctrl+M â€” Toggle outlining (collapse/expand) Ctrl+Space â€” IntelliSense Ctrl+Enter â€” Insert line above Ctrl+Shift+Enter â€” Insert line below Shift+F12 â€” Find all references Ctrl+G â€” Go to line Ctrl+Shift+Up â€” Previous reference Ctrl+Shift+Down â€” Next reference Shift+Alt+Up â€” Column select upward Shift+Alt+Down â€” Column select downward Shift+Up â€” Extend selection up Shift+Down â€” Extend selection down ","date":"2016-10-14T00:00:00Z","permalink":"https://liguobao.github.io/p/visual-studio-shortcuts/","title":"Visual Studio Shortcuts"},{"content":"Installing â€œMicrosoft .NET Core 1.0.0 VS 2015 Tooling Preview 2â€ was a bumpy ride. The bootstrapper downloads MSIs during install (online installer), but links were failing (502s) and network stability made it worse.\nWorkaround: download required packages manually and place them where the installer expects, or use an offline installer when available. For example, DotNetCore.1.0.1-SDK.1.0.0.Preview2-003133-x64.exe could be fetched via a reliable mirror/tool, while dependent MSIs were trickier due to broken links at the time.\nIf you hit 0x80070003 errors (path not found) during install, verify the cached package paths, disable unstable mirrors, and try a fully offline SDK installer. Alternatively, move to newer .NET Core SDK versions which have stable distribution and improved installers.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/.net-core-1.0.0-vs2015-tooling-preview-2-0x80070003/","title":".NET Core 1.0.0 VS2015 Tooling Preview 2 â€” 0x80070003"},{"content":"Summary of proxyipcenter (by @virjar): a server that scrapes, deâ€‘dupes, enriches, validates, and scores proxies; and a client that manages a perâ€‘target pool for crawlers. See Chinese post for full details and JSON examples.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/a-pragmatic-guide-to-proxy-ips-for-crawlers/","title":"A Pragmatic Guide to Proxy IPs for Crawlers"},{"content":"Online: http://codelover.link:8080/\nRepo: https://github.com/liguobao/58HouseSearch\nThis post highlights AMap JS APIs used in the rental map project:\nMap initialization via new AMap.Map(...) IP location with AMap.CitySearch() and map.setBounds(...) Autoâ€‘recenter: listen for map.on('moveend', ...) and call map.getCity() Converting city names to 58.com subdomains (e.g., ä¸Šæµ·â†’sh, åŒ—äº¬â†’bj), using a scraped mapping instead of naive pinyin initials Code snippets remain as in the Chinese version (see original for details). The key idea is to keep the map state in sync with user context (IP/city/center), and normalize city identifiers for downstream requests to 58.com.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/amap--58.com-rental-map-javascript-notes/","title":"AMap + 58.com Rental Map â€” JavaScript Notes"},{"content":"Tried ASP.NET Core with a teammate â€” hereâ€™s a quick handsâ€‘on note.\nEnvironment: VS2015 Update 2 (any edition).\nDownload and install the .NET Core Runtime and SDK https://www.microsoft.com/net/download\nGrab both the .NET Core installer and the SDK, then install them (next, next, finish).\nInstall the VS2015 tooling (Preview 1 at the time) â€œ.NET Core Tooling Preview 1 for Visual Studio 2015â€ https://go.microsoft.com/fwlink/?LinkId=798481\nThis one takes a bit longer â€” wait until it finishes.\nCreate a project Open VS â†’ New Project â†’ Web. You should now see the ASP.NET Core templates available. Create a project and youâ€™re set. Weâ€™ll dive into ASP.NET Core project structure next time.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/asp.net-core-first-look/","title":"ASP.NET Core â€” First Look"},{"content":"ASP.NET Core Hosting â€” Startup Options In classic ASP.NET, apps depended on IIS to start. IIS registered an ISAPI filter and launched w3wp to run the ASP.NET runtime. Pre-.NET Core, Windows + IIS was the default. Although Linux options like jexus (via mono) existed, they werenâ€™t as robust as Microsoftâ€™s native stack.\nWith .NET Core, ASP.NET Core became cross-platform, so its hosting model was redesigned.\n1) Kestrel and IIS (Http Platform Handler) ASP.NET Core rewrote the runtime. It runs like a console app and ships with a high-performance I/O server: Kestrel. Kestrel handles HTTP but doesnâ€™t provide IIS-like process management and protection. If you still host behind IIS, you need a â€œbridgeâ€ â€” historically the Http Platform Handler â€” configured in web.config to start your ASP.NET Core app and forward requests.\nExample:\n1 2 3 4 5 6 7 8 \u0026lt;system.webServer\u0026gt; \u0026lt;handlers\u0026gt; \u0026lt;add name=\u0026#34;httpPlatformHandler\u0026#34; path=\u0026#34;*\u0026#34; verb=\u0026#34;*\u0026#34; modules=\u0026#34;httpPlatformHandler\u0026#34; resourceType=\u0026#34;Unspecified\u0026#34;/\u0026gt; \u0026lt;/handlers\u0026gt; \u0026lt;httpPlatform processPath=\u0026#34;WebApp.exe\u0026#34; arguments=\u0026#34;\u0026#34; stdoutLogEnabled=\u0026#34;false\u0026#34; startupTimeLimit=\u0026#34;3600\u0026#34;/\u0026gt; \u0026lt;/system.webServer\u0026gt; In this setup, IIS accepts the HTTP request, forwards it to your app (e.g., WebApp.exe), your app boots Kestrel, and the request flows through ASP.NET Core. IIS acts as a simple reverse proxy. Note: newer IIS integration modules supersede Http Platform Handler.\n2) Main() Like other .NET apps, ASP.NET Core starts from static void Main(). Example:\n1 2 3 4 5 6 7 8 9 10 11 12 public static void Main(string[] args) { var host = new WebHostBuilder() .UseServer(\u0026#34;Microsoft.AspNetCore.Server.Kestrel\u0026#34;) .UseContentRoot(Directory.GetCurrentDirectory()) .UseDefaultConfiguration(args) .UseIISPlatformHandlerUrl() .UseStartup\u0026lt;Startup\u0026gt;() .Build(); host.Run(); } Key calls:\nUseServer(...): choose the server (e.g., Kestrel). You can implement your own if it conforms to IServerFactory. UseContentRoot(...): set the working directory (defaults to the appâ€™s folder). UseDefaultConfiguration(args): provide hosting config via args (app key, environment, server factory location, content root). These can also come from appsettings.json. 3) Startup UseStartup\u0026lt;Startup\u0026gt;() designates the Startup class, typically with:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public class Startup { public IConfigurationRoot Configuration { get; } public Startup(IHostingEnvironment env) { var builder = new ConfigurationBuilder() .SetBasePath(env.ContentRootPath) .AddJsonFile(\u0026#34;appsettings.json\u0026#34;, optional: true, reloadOnChange: true) .AddJsonFile($\u0026#34;appsettings.{env.EnvironmentName}.json\u0026#34;, optional: true) .AddEnvironmentVariables(); Configuration = builder.Build(); } public void ConfigureServices(IServiceCollection services) { services.AddEntityFramework() .AddDbContext\u0026lt;BlogsContext\u0026gt;(o =\u0026gt; o.UseSqlServer(Configuration[\u0026#34;Data1:DefaultConnection:ConnectionString\u0026#34;])); services.AddMvc(); } public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory) { loggerFactory.AddConsole(Configuration.GetSection(\u0026#34;Logging\u0026#34;)); loggerFactory.AddDebug(); app.UseStaticFiles(); app.UseMvcWithDefaultRoute(); } } ConfigureServices registers services (EF, MVC, etc.). Configure wires middleware and their order. 4) Build and Run Build(): constructs the hosting service, combines services/middleware/content root/app name/configuration, and initializes the host engine. Run(): starts the host and hooks CancelKeyPress so you can stop it with Ctrl+C. Original (Chinese): ASP.NET Core çš„å¯åŠ¨æ–¹å¼ (Hosting) https://dotblogs.com.tw/aspnetshare/2016/03/28/20160327\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/asp.net-core-hosting-startup-options/","title":"ASP.NET Core Hosting â€” Startup Options"},{"content":"See the full English article here:\n/content/en/post/2020/core-middleware.md\nIt covers concepts, sample middleware, ordering, and internals.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/asp.net-core-middleware-concepts-and-example/","title":"ASP.NET Core Middleware â€” Concepts and Example"},{"content":"Bookmark Memorandum Markdown references Markdown (Simplified Chinese): http://wowubuntu.com/markdown/ Markdown Basics (quick start): http://wowubuntu.com/markdown/basic.html Build a free static blog (GitHub + Hexo) Guide: http://wsgzao.github.io/post/hexo-guide/ Install Hexo\n1 2 3 4 5 npm install hexo-cli -g npm install hexo --save # If npm is slow, try the taobao registry npm install -g cnpm --registry=https://registry.npm.taobao.org Install Hexo plugins\n1 2 3 4 5 6 7 8 9 10 11 12 13 npm install hexo-generator-index --save npm install hexo-generator-archive --save npm install hexo-generator-category --save npm install hexo-generator-tag --save npm install hexo-server --save npm install hexo-deployer-git --save npm install hexo-deployer-heroku --save npm install hexo-deployer-rsync --save npm install hexo-deployer-openshift --save npm install hexo-renderer-marked@0.2 --save npm install hexo-renderer-stylus@0.2 --save npm install hexo-generator-feed@1 --save npm install hexo-generator-sitemap@1 --save Git tips Fix â€œFilename too longâ€ on Windows\n1 git config --global core.longpaths true Git + SSH usage: https://segmentfault.com/a/1190000002645623\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/bookmark-memorandum/","title":"Bookmark Memorandum"},{"content":"Sometimes we encrypt data and send it over the network. A common flow is AES-256 â†’ byte[] â†’ string â†’ transmit â†’ decrypt on the other side.\nIn C#, we often convert a byte array to string with Convert.ToBase64String():\n1 2 3 4 5 6 7 8 9 public string BytesToString(byte[] buff) { return Convert.ToBase64String(buff); } public byte[] StringToBytes(string input) { return Encoding.UTF8.GetBytes(input); } This usually works, but if you pass the data via URL, special characters like +, space, % can break query strings and cause mismatches, leading to decryption failures.\nReferences (Chinese):\nURL encoding basics: http://www.ruanyifeng.com/blog/2010/02/url_encoding.html Handling +, space, =, %, \u0026amp;, # in URL params: http://blog.csdn.net/luo_deng/article/details/12186535 You might consider HttpUtility.UrlEncode / HttpUtility.UrlDecode. But beware: browsers may auto-decode once, and if your backend also decodes, youâ€™ll end up decoding twice â€” corrupting the data.\nConclusion: Convert.ToBase64String() can yield URL-unsafe characters. One workaround: encode the bytes as hex strings (0â€“255 â†’ two hex chars), avoiding reserved characters entirely.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 /// byte[] â†’ hex string private static string BytesToString(byte[] bytes) { if (bytes == null) return string.Empty; return string.Join(string.Empty, bytes.Select(b =\u0026gt; string.Format(\u0026#34;{0:x2}\u0026#34;, b)).ToArray()); } /// hex string â†’ byte[] private static byte[] StringToBytes(string str) { if (string.IsNullOrEmpty(str)) return null; byte[] bytes = new byte[str.Length / 2]; for (int i = 0; i \u0026lt; str.Length; i += 2) { bytes[i / 2] = Convert.ToByte(\u0026#34;0x\u0026#34; + str[i] + str[i + 1], 16); } return bytes; } Problem solved.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/byte-array-to-string-safer-url-transport-in-c/","title":"Byte Array to String â€” Safer URL Transport in C#"},{"content":"Found a crawler tutorial on Zhihu over a weekend â€” â€œAmap API + Python to solve rentingâ€. It hit my pain point as I was looking for a new place, so I jumped in right away. The tutorial on Shiyanlou has step-by-step instructions.\nThe project breaks down into two steps:\nUse Python to crawl data and generate a data file Import the data file, display listings on a map, select your workplace, and auto-calc routes and commute time After trying the tutorial, I felt it was too rough for practical use, and itâ€™s Beijing-only while Iâ€™m in Shanghai. You can tweak the Python data source and the front-end JS, but it still felt clunky. So I decided to build my own.\nFirst, the original Python (excerpt):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # -*- coding:utf-8 -*- from bs4 import BeautifulSoup from urlparse import urljoin import requests import csv url = \u0026#34;http://bj.58.com/pinpaigongyu/pn/{page}/?minprice=2000_4000\u0026#34; page = 0 csv_file = open(\u0026#34;rent.csv\u0026#34;,\u0026#34;wb\u0026#34;) csv_writer = csv.writer(csv_file, delimiter=\u0026#39;,\u0026#39;) while True: page += 1 response = requests.get(url.format(page=page)) html = BeautifulSoup(response.text) house_list = html.select(\u0026#34;.list \u0026gt; li\u0026#34;) if not house_list: break for house in house_list: house_title = house.select(\u0026#34;h2\u0026#34;)[0].string.encode(\u0026#34;utf8\u0026#34;) house_url = urljoin(url, house.select(\u0026#34;a\u0026#34;)[0][\u0026#34;href\u0026#34;]) house_info_list = house_title.split() if \u0026#34;å…¬å¯“\u0026#34; in house_info_list[1] or \u0026#34;é’å¹´ç¤¾åŒº\u0026#34; in house_info_list[1]: house_location = house_info_list[0] else: house_location = house_info_list[1] house_money = house.select(\u0026#34;.money\u0026#34;)[0].select(\u0026#34;b\u0026#34;)[0].string.encode(\u0026#34;utf8\u0026#34;) csv_writer.writerow([house_title, house_location, house_money, house_url]) csv_file.close() It scrapes http://bj.58.com/pinpaigongyu/pn/{page}/?minprice=2000_4000, then writes to CSV for downstream use. Each listing is an li element:\n1 2 3 4 5 6 7 8 \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;/pinpaigongyu/...\u0026#34; tongji_label=\u0026#34;listclick\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;des\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;ã€åˆç§Ÿã€‘èŠå›­æ–°åŒº æŸ³æ¹–æ™¯åº­ 3å®¤æ¬¡å§\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;money\u0026#34;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;b\u0026gt;1100\u0026lt;/b\u0026gt;å…ƒ/æœˆ\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; While the Python extracts from li, I noticed the key info actually lives in the a[tongji_label=\u0026quot;listclick\u0026quot;] element. Rather than regex, I used HtmlAgilityPack in .NET for HTML parsing/manipulation.\nInstall via NuGet:\n1 Install-Package HtmlAgilityPack Controller snippet (core logic abbreviated):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public ActionResult Get58CityRoomData(int costFrom, int costTo, string cnName) { // ... validate params ... var lstHouse = new List\u0026lt;HouseInfo\u0026gt;(); string tempURL = \u0026#34;http://\u0026#34; + cnName + \u0026#34;.58.com/pinpaigongyu//pn/{0}/?minprice=\u0026#34; + costFrom + \u0026#34;_\u0026#34; + costTo; Uri uri = new Uri(tempURL); var htmlResult = HTTPHelper.GetHTMLByURL(string.Format(tempURL, 1)); HtmlDocument htmlDoc = new HtmlDocument(); htmlDoc.LoadHtml(htmlResult); var countNodes = htmlDoc.DocumentNode.SelectSingleNode(\u0026#34;.//span[contains(@class,\u0026#39;list\u0026#39;)]\u0026#34;); int pageCount = 10; if (countNodes != null \u0026amp;\u0026amp; countNodes.HasChildNodes) { pageCount = Convert.ToInt32(countNodes.ChildNodes[0].InnerText) / 20; if(pageCount==0) { return Json(new { IsSuccess = false, Error = \u0026#34;No results in this price range.\u0026#34; }); } } for (int pageIndex = 1; pageIndex \u0026lt;= pageCount; pageIndex++) { htmlResult = HTTPHelper.GetHTMLByURL(string.Format(tempURL, pageIndex)); htmlDoc.LoadHtml(htmlResult); var roomList = htmlDoc.DocumentNode.SelectNodes(\u0026#34;.//a[contains(@tongji_label,\u0026#39;listclick\u0026#39;)]\u0026#34;); foreach (var room in roomList) { var houseTitle = room.SelectSingleNode(\u0026#34;.//h2\u0026#34;).InnerHtml; var houseURL = uri.Host + room.Attributes[\u0026#34;href\u0026#34;].Value; var house_info_list = houseTitle.Split(\u0026#39; \u0026#39;); var house_location = (house_info_list[1].Contains(\u0026#34;å…¬å¯“\u0026#34;) || house_info_list[1].Contains(\u0026#34;é’å¹´ç¤¾åŒº\u0026#34;)) ? house_info_list[0] : house_info_list[1]; var money = room.SelectSingleNode(\u0026#34;.//b\u0026#34;).InnerHtml; lstHouse.Add(new HouseInfo { HouseTitle = houseTitle, HouseLocation = house_location, HouseURL = houseURL, Money = money }); } } return Json(new { IsSuccess = true, HouseInfos = lstHouse }); } Two key points:\nThe first page contains a hidden total count like \u0026lt;span class=\u0026quot;listsum\u0026quot;\u0026gt;\u0026lt;em\u0026gt;1813\u0026lt;/em\u0026gt;æ¡ç»“æœ\u0026lt;/span\u0026gt;, so pages = total/20. Target the a with tongji_label=\u0026quot;listclick\u0026quot;; inside it, h2 has the title/location, and b inside .money has the price. Thatâ€™s the backend part. The frontend (Amap/GAODE integration) will be for another day â€” time to play games with my partner :-)\nSource code: https://github.com/liguobao/58HouseSearch\nLive: 58 Apartment Map Search (China): https://woyaozufang.live\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-58.com-branded-apartments-crawler/","title":"C# â€” 58.com Branded Apartments Crawler"},{"content":"C# Avoid Throwing Exceptions in Functions or Operations Introduction In some scenarios, your function or operation needs to operate on a sequence of objects, and an exception is thrown during the process. At this time, if there is no data such as status records, we do not know how much data has been processed, nor do we know what strategy to use to roll back, so we cannot return to the previous state.\nLet\u0026rsquo;s look at the following code:\n1 2 var allEmp = FindAllEmployees(); allEmp.ForEach(e =\u0026gt; e.MonthlySalary *=1.05M); This code looks fine. But one day, when this program runs, an exception is thrown. The location where the exception is thrown may be unknown, causing some employees to get a raise, while others do not. The result is that except for manually checking the data, we have no way to recover the lost state.\nThis way of modifying elements leads to the above problem. This code does not follow the \u0026ldquo;strong exception safety guarantee\u0026rdquo; rule. In other words, when an error occurs at runtime, we cannot know what happened and what did not happen.\nPrinciple: If we can guarantee that the program\u0026rsquo;s state will not change when the method cannot complete, such problems will not occur. We have several methods to achieve this requirement, but each method has its own advantages and risks.\nThrowing Exceptions in Functions/Operations Obviously, not all methods will encounter such problems (exceptions leading to state loss). Many times we just check the elements in the sequence, access them without modifying them. We don\u0026rsquo;t need to be too careful about this kind of behavior. Now let\u0026rsquo;s go back to the beginning, for the above scenario (giving each employee a 5% raise), if we want to follow the \u0026ldquo;strong exception safety guarantee\u0026rdquo; principle, how should we modify this method?\nFirst exception: Exception when getting data In the above example, even if the FindAllEmployees() function throws an exception, causing us to fail to correctly give employees a raise. Although this situation does not cause problems with our data, the people who should get a raise did not get it, which is a frustrating thing.\nSolution: Rewrite the operation method given earlier with lambda expression (i.e. FindAllEmployees() method), so that it never throws an exception. Many times, before we start modifying data, it is not very difficult to validate the legitimacy of the data and exclude erroneous data (if allowed). We can take this approach to achieve our goal. But here, we must strictly handle the operation method so that it can meet the needs in all situations.\nSecond exception: Exception in lambda expression when operating data Also in the above example, if we throw an exception when executing the raise operation, raising the salary of those employees who have already resigned, causing the program to interrupt and lose state. In this case, filtering out resigned employees before executing the raise operation is a correct approach.\nSolution: Validate and filter before operating data Such as:\nallEmp.Where(emp=\u0026gt;emp.Active).ForEach(e =\u0026gt; e.MonthlySalary *=1.05M);\nThird exception: Exception when executing operation Sometimes, we cannot guarantee whether an exception will be thrown when processing. At this time, we must adopt some more expensive processing methods.\nSolution: Create a copy to try the operation, and execute the real operation after the copy is correct When writing this kind of code, we should consider the handling plan after throwing an exception. This means that our operation should first execute on the original data copy, and then replace the original data only after the operation succeeds.\nSuch as:\n1 2 3 4 5 6 7 8 9 var updatas = (from e in allEmp select new Emp { EmpID=e.EmpID, ...... MonthlySalary =e.MonthlySalary *=1.05M }).ToList(); allEmp = updatas; But such modifications also cause other problems: the amount of code increases, and generating copies also consumes a lot of resources. Such an approach also has a benefit, when we encounter exceptions when operating on copy data, we have ample \u0026ldquo;space\u0026rdquo; to handle these data.\nIn practice, this means that we let the query expression return a new sequence instead of modifying the elements in the original sequence. In this way, we try to complete all operations at the same time, even if it fails, it will not affect the original state of our program.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-avoid-throwing-exceptions-in-functions-or-operations/","title":"C# Avoid Throwing Exceptions in Functions or Operations"},{"content":"C# Class Fields and Properties Fields Fields represent read-only or readable/writable data values. Fields can be static, which are considered part of the type state. Fields can also be instance (non-static), which are considered part of the object state. It is strongly recommended to declare fields as private to prevent the type or object\u0026rsquo;s state from being corrupted by external code. Properties Properties allow setting or querying the logical state of a type or object using simple, field-style syntax, while ensuring the state is not corrupted. Properties that act on types are called static properties, and those that act on objects are called instance properties. Properties can have no parameters or multiple parameters (rare, but common in collection classes).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 using System; public sealed class SomeType { // 1 // Nested class private class SomeNestedType { } // 2 // Constant, readÂ­only, and static read/write field private const Int32 c_SomeConstant = 1; // 3 private readonly String m_SomeReadOnlyField = \u0026#34;2\u0026#34;; // 4 private static Int32 s_SomeReadWriteField = 3; // 5 // Type constructor static SomeType() { } // 6 // Instance constructors public SomeType(Int32 x) { } // 7 public SomeType() { } // 8 // Instance and static methods private String InstanceMethod() { return null; } // 9 public static void Main() { } // 10 // Instance property public Int32 SomeProp { // 11 get { return 0; } // 12 set { } // 13 } // Instance parameterful property (indexer) public Int32 this[String s] { // 14 get { return 0; } // 15 set { } // 16 } // Instance event public event EventHandler SomeEvent; // 17 } ","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-class-fields-and-properties/","title":"C# Class Fields and Properties"},{"content":"It may be because we are all accustomed to explicit definitions. Generally speaking, in daily life, we rarely use anonymous objects.\nHowever, for implementing those short-lived types that do not dominate the application logic, using anonymous objects is a good choice. On the other hand, it may also be because the life cycle of anonymous types cannot span the method containing the type, which makes many people feel that anonymous objects are not easy to use because they cannot be passed between multiple methods.\nThis is not accurate. We can completely write generic methods for anonymous types. However, if so, we cannot handle any special elements or write any special logic in the generic type method.\nBelow we will write a simple example, the example function: return all elements in the collection that are equal to the object to be found.\n1 2 3 4 5 6 7 8 static IEnumerable\u0026lt;T\u0026gt; FindValue(IEnumerable\u0026lt;T\u0026gt; enumerable, T value) { foreach (T element in enumerable) { if (element.Equals(value)) yield return element; } } This method can be used with anonymous types, but this method is essentially a generic method and does not understand the information of anonymous types.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-define-local-functions-for-anonymous-types/","title":"C# Define Local Functions for Anonymous Types"},{"content":"Join Quick notes on inner/left/right/outer joins â€” focusing on LINQâ€™s Join and Enumerable.Join.\nEnumerable.Join signature:\n1 2 3 4 5 6 public static IEnumerable\u0026lt;TResult\u0026gt; Join\u0026lt;TOuter, TInner, TKey, TResult\u0026gt;( this IEnumerable\u0026lt;TOuter\u0026gt; outer, IEnumerable\u0026lt;TInner\u0026gt; inner, Func\u0026lt;TOuter, TKey\u0026gt; outerKeySelector, Func\u0026lt;TInner, TKey\u0026gt; innerKeySelector, Func\u0026lt;TOuter, TInner, TResult\u0026gt; resultSelector) Where:\nouter/inner: sequences to join outerKeySelector/innerKeySelector: key selectors resultSelector: projector for the joined pair MSDN-style example:\n1 2 3 4 5 6 7 var query = people.Join( pets, person =\u0026gt; person, pet =\u0026gt; pet.Owner, (person, pet) =\u0026gt; new { OwnerName = person.Name, Pet = pet.Name }); foreach (var x in query) Console.WriteLine($\u0026#34;{x.OwnerName} - {x.Pet}\u0026#34;); LINQ query syntax equivalent (inner join):\n1 2 3 var query = from person in people join pet in pets on person equals pet.Owner select new { OwnerName = person.Name, Pet = pet.Name }; Left join via GroupJoin:\n1 2 3 var queryGroup = from person in people join pet in pets on person equals pet.Owner into ps select new { OwnerName = person.Name, Pet = ps }; References:\nhttps://msdn.microsoft.com/zh-cn/library/bb311040.aspx https://msdn.microsoft.com/zh-cn/library/bb534675(v=vs.110).aspx ","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-join-and-linq-join-basics/","title":"C# Join and LINQ Join Basics"},{"content":"LINQ Advantages Summary (Reprint) Original link: http://www.cnblogs.com/c-jquery-linq-sql-net-problem/archive/2011/01/15/LINQ_Merit.html\nI\u0026rsquo;ve been reading a book on LINQ called \u0026lsquo;Essential LINQ\u0026rsquo;, and here I share it with everyone.\nSince a deep summary of LINQ requires a lot of space, I\u0026rsquo;ll divide it into several parts here.\n(*I read the English version of \u0026lsquo;Essential LINQ\u0026rsquo;, please forgive me if some terms cannot be translated into proper Chinese explanations)\nAdvantages of LINQ:\nLINQ basically has the following seven advantages, let me illustrate them one by one:\nIntegrated: The so-called Integrated (integration), LINQ embodies integration from the following aspects: (1): Integrate query syntax into languages like C#(VB), making it a syntax. This way, it can support the same as other syntax in C#:\nStatement highlighting, type checking, allowing debugger debugging\n(2): Integrate and encapsulate the previous complex pre-query work, allowing developers to focus on the query.\n(3): The integrated syntax is clearer and more readable.\n1 2 3 4 5 6 7 8 9 10 11 12 Comparison: //Original format SqlConnection sqlConn = new SqlConnection(connectionString);\u0026gt; sqlConn.Open(); SqlCommand command = new SqlCommand(); command.Connection = sqlConn; command.CommandText = \u0026#34;Select * From Customer\u0026#34;; SqlDataReader dataReader = command.ExecuteReader(CommandBehavior.CloseConnection); //LINQ format NORTHWNDDataContext dc = new NORTHWNDDataContext(); var query = from c in dc.Customers select c; Unitive: The so-called Unitive (unification) means using unified query syntax for any type of external and internal data sources (object collections, xml, database data). The benefits of using a unified query language are as follows:\nYou don\u0026rsquo;t have to spend a lot of effort learning unfamiliar data sources, you can quickly and simply use LINQ syntax to query them.\nSince unified syntax is used, code maintenance becomes simpler.\nThe following code embodies LINQ\u0026rsquo;s unification:\n1 2 3 4 5 6 7 8 9 10 //Data source: object collection var query = from c in GetCustomers() select c; //Data source: SQL var query1 = from c in dc.Customers select c; //Data source: XML var query2 = from c in customers.Descendants(\u0026#34;Customer\u0026#34;) select c; Extensible: The so-called Extensible (extensibility) refers to the following 2 aspects: (1). Extension of queryable data sources. LINQ provides a LINQ provider model, you can create or provide providers for LINQ to support more data sources.\n(2). Extensible query methods. Developers can rewrite and extend query methods for LINQ according to their own needs.\nHere are some third-party LINQ providers:\nLINQ Extender, LINQ to JavaScript, LINQ to JSON, LINQ to MySQL, LINQ to Flickr, LINQ to Google\nDeclarative: The so-called Declarative (declarative), simply put, means that developers only tell the program what to do, and the program judges how to do it. The advantages of Declarative programming are reflected in the following 2 points:\n(1). Improve development speed. Because developers don\u0026rsquo;t need to write a lot of code to concretize execution steps, just tell the program what to do.\n(2). Improve code optimization space. Because developers don\u0026rsquo;t interfere with the specific steps of program execution, this provides more space for the compiler to optimize the code.\nFor example, in SQL, LINQ-generated SQL statements are often better than SQL statements written by developers with average SQL skills.\nCompare Declarative programming with Imperative programming:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 //Declarative programming List\u0026lt;List\u0026lt;int\u0026gt;\u0026gt; lists = new List\u0026lt;List\u0026lt;int\u0026gt;\u0026gt; { new List\u0026lt;int\u0026gt; { 1, 2, 3 }, new List\u0026lt;int\u0026gt; { 4, 5 } }; var query = from list in lists from num in list where num % 3 == 0 orderby num descending select num; //Imperative programming List\u0026lt;int\u0026gt; list1 = new List\u0026lt;int\u0026gt;(); list1.Add(1); list1.Add(2); list1.Add(3); List\u0026lt;int\u0026gt; list2 = new List\u0026lt;int\u0026gt;(); list2.Add(4); list2.Add(5); List\u0026lt;List\u0026lt;int\u0026gt;\u0026gt; lists1 = new List\u0026lt;List\u0026lt;int\u0026gt;\u0026gt;(); lists1.Add(list1); lists1.Add(list2); List\u0026lt;int\u0026gt; newList = new List\u0026lt;int\u0026gt;(); foreach (var item in lists1) foreach (var num in item) if (num % 3 == 0) newList.Add(num); newList.Reverse(); Hierarchical: The so-called Hierarchical (hierarchical) refers to abstracting data in an object-oriented way. SQL is a relational database, it describes data and data relationships in a relational way, but our programs are designed to be object-oriented, so the database data we get in the program are often rectangular grid (planar display data). But LINQ converts relational to object-to-object description of data through the so-called O-R Mapping method.\nThe benefits are: Developers can directly operate data in an object way, and for developers accustomed to object-oriented, the object-oriented model is easier to understand.\nComposable: The so-called Composable (composable) means that LINQ can split a complex query into multiple simple queries. The results returned by LINQ are all based on the interface: IEnumerable, so you can continue querying the query results, and LINQ has the characteristic of deferred execution, so splitting execution will not affect efficiency.\nThe advantages are:\n(1). Convenient debugging. Split complex queries into simple queries, then debug them one by one.\n(2). Easy code maintenance. Splitting the code can make the code easier to understand.\nThe following code embodies composability:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //The following code embodies Composable List\u0026lt;List\u0026lt;int\u0026gt;\u0026gt; lists = new List\u0026lt;List\u0026lt;int\u0026gt;\u0026gt; { new List\u0026lt;int\u0026gt; { 1, 2, 3 }, new List\u0026lt;int\u0026gt; { 4, 5 } }; var query1 = from list in lists from num in list select num; var query2 = from num in query1 where num % 3 == 0 select num; var query3 = from num in query2 orderby num descending select num; Transformative: The so-called Transformative (transformative) means that LINQ can convert the content of one data source to other data sources. Convenient for users to do data migration.\nThe following code embodies the transformation feature:\n1 2 3 4 5 6 //Convert relational data to XML type var query = new XElement(\u0026#34;Orders\u0026#34;, from c in dc.Customers where c.City == \u0026#34;Paris\u0026#34; select new XElement(\u0026#34;Order\u0026#34;, new XAttribute(\u0026#34;Address\u0026#34;, c.Address))); The above are the main advantages of LINQ, I\u0026rsquo;m glad to share here with everyone. If there are any shortcomings, please supplement and correct, thank you for visiting the hut.\n//2011/1/28 Supplement (LINQ TO SQL)\nIn terms of LINQ TO SQL, if you use LINQ TO SQL, you can effectively prevent SQL injection, LINQ TO SQL will treat the injected code as useless parameters.\nhttp://www.cnblogs.com/c-jquery-linq-sql-net-problem/archive/2011/01/15/LINQ_Merit.html\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-linq-advantages-summary-reprint/","title":"C# LINQ Advantages Summary (Reprint)"},{"content":"First look at a piece of code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #region test1 Closure public static void test1() { int index = 0; Func\u0026lt;IEnumerable\u0026lt;int\u0026gt;\u0026gt; sequence =()=\u0026gt;GetEnumrableInt(index); index = 20; foreach(int n in sequence()) Console.WriteLine(n); Console.WriteLine(\u0026#34;Done\u0026#34;); index = 100; foreach (int n in sequence()) Console.WriteLine(n); } public static IEnumerable\u0026lt;int\u0026gt; GetEnumrableInt(int index) { List\u0026lt;int\u0026gt; l = new List\u0026lt;int\u0026gt;(); for(int i=index;i\u0026lt;index+30;i++) { l.Add(i); } return l; } #endregion The above code demonstrates the situation where external variables are used in closures, and then these variables are modified externally, resulting in outputting numbers from 20-50, and then outputting numbers between 100-130. This behavior is a bit weird, but it does have meaning\u0026hellip; (The book says so, I think it\u0026rsquo;s rarely used.)\nTo convert query expressions into executable code, the C# compiler does a lot of work. Generally speaking, the C# compiler converts queries and lambda expressions into \u0026ldquo;static delegates\u0026rdquo;, \u0026ldquo;instance delegates\u0026rdquo; or \u0026ldquo;closures\u0026rdquo;. The compiler will choose an implementation method based on the code in the lambda expression. Which method is chosen depends on the body of the lambda expression. This seems to be some implementation details of the language, but it can significantly affect our code. The choice of implementation by the compiler may lead to subtle changes in behavior.\nNot all lambda expressions generate code with the same structure.\nFor the compiler, the simplest behavior is to generate delegates for code in the following form.\n1 2 3 4 5 6 7 8 9 10 11 12 //Our lambda expression public static void test2() { int[] someNum = {0,1,2,3,4,5,6,7,8,9,10 }; IEnumerable\u0026lt;int\u0026gt; ans = from n in someNum select n * n; foreach (int i in ans) Console.WriteLine(i); } The compiler will use static delegates to implement the n*n lambda expression, and the code generated for the above code is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 //Code generated by the compiler for our lambda #region Equivalent to test2() private static int HiddenFunc(int n) { return n * n; } //Static delegate private static Func\u0026lt;int, int\u0026gt; HiddenDelegate; public void test2_1() { int[] someNum = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }; if(HiddenDelegate==null) { HiddenDelegate = new Func\u0026lt;int, int\u0026gt;(HiddenFunc); } IEnumerable\u0026lt;int\u0026gt; ans = someNum.Select\u0026lt;int, int\u0026gt;(HiddenDelegate); foreach(int i in ans) Console.WriteLine(i); } #endregion The body of this lambda expression does not access any instance variables or local variables. The lambda expression only accesses its parameters. In this case, the C# compiler will create a static method as the target of the delegate. This is also the simplest processing performed by the compiler. If the expression can be implemented through a private static method, the compiler will generate the private static method and the corresponding delegate definition. For the code example above and expressions that only access static variables, the compiler will adopt this approach.\nNext, introduce another relatively simple situation:\nThe lambda expression needs to access the instance variables of the type, but does not need to access the local variables in the outer method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 public class ModFilter { private readonly int modules; public ModFilter(int mod) { modules = mod; } public IEnumerable\u0026lt;int\u0026gt; FindValues(IEnumerable\u0026lt;int\u0026gt; sequence) { return from n in sequence where n % modules == 0 //Newly added expression select n * n; //Same as the previous example } } /* In this case, the compiler will create an instance method to wrap the delegate for the expression. The basic concept is the same as the previous case, except that an instance method is used here to read and modify the state of the current object. Like the static delegate example, here the compiler will convert the lambda expression into familiar code. Which includes the definition of the delegate and the method call. As follows: */ public class ModFilter_Other { private readonly int modules; //Instance method private bool WhereClause(int n) { return ((n%this.modules) ==0); } private static int SelectClause(int n) { return n * n; } private static Func\u0026lt;int, int\u0026gt; SelectDelegate; public ModFilter_Other(int mod) { modules = mod; } public IEnumerable\u0026lt;int\u0026gt; FindValues(IEnumerable\u0026lt;int\u0026gt; sequence) { if(SelectDelegate==null) { SelectDelegate = new Func\u0026lt;int, int\u0026gt;(SelectClause); } return sequence.Where\u0026lt;int\u0026gt;( new Func\u0026lt;int, bool\u0026gt;(this.WhereClause)). Select\u0026lt;int, int\u0026gt;(SelectClause); } } In summary: If the code in the lambda expression accesses member variables in the object instance, then the compiler will generate instance methods to represent the code in the lambda expression. In fact, there is nothing special about thisâ€”the compiler saves us some code input work, and the code becomes much cleaner. Essentially, this is still an ordinary method call.\nHowever, if the lambda expression accesses local variables or method parameters in the outer method, then the compiler will do a lot of work for you.\nClosures are used here. The compiler will generate a private nested type to implement closures for local variables.\nLocal variables must be passed into the delegate that implements the body of the lambda expression.\nIn addition, all modifications made by the lambda expression to these local variables must be accessible externally.\nOf course, there may be more than one variable shared between the inner and outer layers in the code, and there may be more than one query expression.\nLet\u0026rsquo;s modify the instance method to access a local variable.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 public class ModFilter { private readonly int modules; public ModFilter(int mod) { modules = mod; } public IEnumerable\u0026lt;int\u0026gt; FindValues(IEnumerable\u0026lt;int\u0026gt; sequence) { int numValues = 0; return from n in sequence where n % modules == 0 //Newly added expression select n * n / ++ numValues; //Access local variable } } Note that the select clause needs to access the local variable numValues. In order to create this closure, the compiler needs to use a nested type to implement the behavior you need. Below is the code generated by the compiler for you. public class ModFilter { private sealed class Closure { public ModFilter outer; public int numValues; public int SelectClause(int n) { return ((n * n) / ++this.numValues); } } private readonly int modules; //Instance method private bool WhereClause(int n) { return ((n % this.modules) == 0); } public ModFilter(int mod) { modules = mod; } public IEnumerable\u0026lt;int\u0026gt; FindValues(IEnumerable\u0026lt;int\u0026gt; sequence) { Closure c = new Closure(); c.outer = this; c.numValues = 0; return sequence.Where\u0026lt;int\u0026gt;( new Func\u0026lt;int, bool\u0026gt;(this.WhereClause)). Select\u0026lt;int, int\u0026gt;(c.SelectClause); } } In the above code, the compiler specifically creates a nested class to contain all variables that will be accessed or modified in the lambda expression. In fact, these local variables will be completely replaced by the fields of the nested class. The code inside the lambda expression and the code outside (but still in the current method) access the same field, and the logic in the lambda expression is also compiled into a method of the inner class.\nFor the parameters of the outer method that will be used in the lambda expression, the compiler will also implement them in the same way as local variables: the compiler will copy these parameters into the nested class representing the closure.\nGoing back to the initial example, this is something we should be able to understand this seemingly strange behavior. The variable index was passed into the closure, but was modified by external code before the query started executing. That is to say, you modified the internal state of the closure, and then expected it to return to its previous state to start executing, which is obviously impossible.\nConsidering the interaction in deferred execution and the way the compiler implements closures, modifying variables bound between the query and external code may cause erroneous behavior.\nTherefore, we should try to avoid modifying variables in methods that will be passed into closures and used in closures.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23-tips-to-avoid-modifying-bound-variables/","title":"C# Tips to Avoid Modifying Bound Variables"},{"content":".NET Managed Heap and Garbage Collection Basics of Managed Heap Brief description: Every program needs to use resources of one kind or another, including files, memory buffers, screen space, network connections\u0026hellip;.. In fact, in an object-oriented environment, each type represents a resource available to the program. To use these resources, memory must be allocated for the type representing the resource.\nThe following are the steps required to access a resource:\nCall the IL instruction newobj to allocate memory for the type representing the resource. (C# new operator) Initialize the memory to set the initial state of the resource. (Generally refers to the constructor) Access the type\u0026rsquo;s members to use the resource. (Use member variables, methods, properties, etc.) Destroy the resource\u0026rsquo;s state for cleanup. (Dispose???) Free the memory. (GC) Allocating Resources from the Managed Heap CLR requires all objects to be allocated from the managed heap.\nWhen the process initializes, CLR allocates an address space region as the managed heap. CLR also maintains a pointer, let\u0026rsquo;s call it NextObjPtr, which points to the allocation position of the next object in the heap. At the beginning, NextObjPtr is set to the base address of the address space region.\nWhen a region is filled with non-garbage objects, CLR allocates more regions.\nThis process repeats until the entire process address space is filled. So, application memory is limited by the process\u0026rsquo;s virtual address space.\n32-bit processes can allocate up to 1.5GB, 64-bit processes can allocate up to 8T.\nNote: Related materials on process memory size\nMemory Support and Windows Operating Systems\nProcess Address Space\nMaximum memory available for C/C++ programs in 32-bit mode\nThe new operator in C# causes CLR to perform the following operations: Calculate the number of bytes required for the type\u0026rsquo;s fields (and fields inherited from base types).\nAdd the bytes required for object overhead. Each object has two overhead fields: type object pointer and sync block index. For 32-bit applications, these two fields each require 32 bits, so each object needs to add 8 bytes. For 64-bit applications, these two fields each require 64 bits, so each object adds 16 bytes.\nCLR checks if there are enough bytes in the region to allocate the object. If the managed heap has enough available space, the object is placed at the address pointed to by the NetxObjPtr pointer, and the bytes allocated for the object are zeroed. Then the type\u0026rsquo;s constructor is called (passing NextObjPtr as the this parameter), the new operator returns the object reference. Just before returning the object reference, the value of the NextObjPtr pointer is increased by the number of bytes occupied by the object to get a new value, which is the address where the next object will be placed in the managed heap. As shown in the figure:\nGarbage Collection Algorithm CLR uses reference tracking algorithm. Reference tracking algorithm only cares about reference type variables, because only this type of variable can reference objects on the heap;\nValue type variables directly contain value type instances. Reference type variables can be used in many situations, including static and instance fields of classes, or parameters and local variables of methods. Here we call all reference type variables roots.\nWhen CLR starts GC, it first suspends all threads. (This prevents threads from accessing objects and changing their state during CLR inspection.) Then CLR enters the GC marking phase. In this phase, CLR traverses all objects in the heap, setting a bit in the sync block index field to 0. This indicates that all objects should be deleted. Then, CLR checks all active roots to see which objects they reference. This is why CLR\u0026rsquo;s GC is called reference tracking GC. If a root contains null, CLR ignores this root and continues to check the next root.\nThe figure below shows a heap containing several objects.\nThe application\u0026rsquo;s roots directly reference objects A, C, D, F. All objects have been marked. When marking object D, GC finds that this object contains a field referencing object H, causing object H to also be marked. The marking process continues until all roots of the application have been checked.\nAfter checking, the objects in the heap are either marked or unmarked. Marked objects cannot be garbage collected because at least one root is referencing them. We say that such objects are reachable because the application can reach them through variables referencing them. Unmarked objects are unreachable because there is no root in the application that can make the object accessible again.\nAfter CLR knows which objects can survive and which can be deleted, it enters the GC compression (similar to defragmentation) phase. In the compression phase, CLR \u0026ldquo;moves\u0026rdquo; the marked objects in the heap to organize all surviving objects to occupy contiguous memory.\nThe benefits of doing this are:\nAll surviving objects are next to each other in memory, restoring the \u0026ldquo;locality\u0026rdquo; of references, reducing the application\u0026rsquo;s working set, thereby improving performance when accessing these objects in the future;\nAfter defragmentation, the available space is also contiguous, liberating the entire address space segment, allowing other things to reside.\nAfter moving objects in memory, there is a problem that needs to be solved urgently. The roots referencing surviving objects now reference the original location of the object in memory, not the moved location. When suspended threads resume execution, they will access the old memory location, causing memory corruption. This is obviously intolerable, so as part of the compression phase, CLR also subtracts the number of bytes the referenced object is offset in memory from each root. This ensures that each root still references the same object as before, only the object\u0026rsquo;s position in memory has changed.\nAs shown in the figure:\nGenerations: Improving Performance (To be continued) CLR\u0026rsquo;s GC is a generational garbage collector, it makes the following assumptions about your code:\nThe newer the object, the shorter the lifetime.\nThe older the object, the longer the lifetime.\nRecycling part of the heap is faster than recycling the entire heap.\nExtensive research shows that these assumptions hold for most applications today, and they affect the implementation of the garbage collector. Here we will explain how generations work.\nThe managed heap does not include objects when initialized. Objects added to the heap become generation 0 objects. Simply put, generation 0 objects are those newly constructed objects that the garbage collector has never checked. As shown in the figure, a newly started application has allocated 5 objects (A to E). After a while, C and E become unreachable.\nCLR initializes generation 0 objects by selecting a budget capacity. If allocating a new object causes generation 0 to exceed the budget, a GC must be started. Assuming objects A to E just fill generation 0\u0026rsquo;s space, allocating object F must start GC. After GC, surviving objects become generation 1 objects. As shown in the figure:\nAfter one GC, generation 0 does not contain any objects. As before, new objects will be allocated to generation 0. Newly allocated objects F to K all go to generation 0.\nThen, the program continues to run, B, H, J become unreachable, their memory will be recycled at some point.\nAssume that allocating new object L will cause generation 0 to exceed the budget, causing GC to start.\nWhen starting garbage collection, the garbage collector must decide which generations to check. As mentioned earlier, CLR initializes by selecting a budget for generation 0 objects. In fact, it must also select a budget for generation 1.\nWhen starting a garbage collection, the garbage collector also checks how much memory generation 1 occupies. In this example, since generation 1 occupies much less memory than the budget, the garbage collector decides to check only generation 0 objects. Recalling the first assumption made by the generational garbage collector: the newer the object, the shorter the lifetime. Therefore, generation 0 contains more garbage, and more memory can be recycled. By ignoring objects in generation 1, the garbage collection speed is accelerated.\nObviously, ignoring objects in generation 1 can improve the performance of the garbage collector. But what has a greater performance boost is that now there is no need to traverse every object in the managed heap. If a root or object references an object in an older generation, the garbage collector can ignore all references inside the old object, and can construct the reachable object graph in a shorter time. Of course, if the fields of old objects may also reference new objects. To ensure that updated fields of old objects are checked, the garbage collector uses a mechanism inside the JIT compiler. This mechanism sets a corresponding flag when the reference field of an object changes. In this way, the garbage collector knows which old objects (if any) have been written to since the last garbage collection. Only old objects whose fields have changed need to be checked to see if they reference any new objects in generation 0.\nThe generational garbage collector also assumes that the older the object, the longer it lives. That is, generation 1 objects are likely to continue to be reachable in the application. If the garbage collector checks the objects in generation 1, it is likely to find little garbage, and as a result, little memory can be recycled. Therefore, garbage collecting generation 1 is likely to be a waste of time. If there is really garbage in generation 1, the garbage will stay there. As shown in the figure:\nThe program continues to run, continues to allocate objects to generation 0, and at the same time the program stops using some objects in generation 1.\nAs shown in the figure:\nAllocating object P causes generation 0 to exceed the budget, starting GC. All objects in generation 1 still occupy less than the budget, the garbage collector decides again to recycle only generation 0. Ignore garbage objects in generation 1. As shown in the figure:\nThe program continues to run, assuming that the growth of generation 1 causes all its objects to occupy the full budget. At this time, the application allocates objects P to S, making the generation 0 objects reach their budget total. As shown in the figure:\nAt this time, the application is ready to allocate object T, since generation 1 is full, GC must start. But this time the garbage collector finds that generation 1 occupies too much memory, using up the budget. Since the previous GC on generation 0, there may already be many unreachable objects in generation 1. So this time the garbage collector decides to check all objects in generation 1 and generation 0. After garbage collection of both generations, the heap looks like the figure:\nThe managed heap only supports three generations: generation 0, generation 1, and generation 2.\nWhen CLR initializes, it selects a budget for each generation.\nHowever, CLR\u0026rsquo;s garbage collection is self-adjusting.\nThis means that the garbage collector learns about the program\u0026rsquo;s behavior during the garbage collection process.\nFor example: Assume that the application constructs many objects, but each object has a very short lifetime.\nIn this case, garbage collection of generation 0 will recycle a large amount of memory. In fact, all objects in generation 0 may be recycled.\nIf the garbage collector finds that after recycling generation 0, few objects survive, it may reduce the budget for generation 0. The reduction in allocated space means that garbage collection will occur more frequently, but the garbage collector does less each time, which reduces the process\u0026rsquo;s working set.\nOn the other hand, if the garbage collector recycles generation 0 and finds that many objects survive, little memory can be recycled, it will increase the budget for generation 0.\nThe same heuristic algorithm adjusts the budget for generation 1 and generation 2.\nQuoted from: \u0026ldquo;CLR VIA C# - Chapter 21\u0026rdquo;\nAutomatic Memory Management\nFundamentals of Garbage Collection\nGenerations\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23.net-managed-heap-and-garbage-collection/","title":"C#.NET Managed Heap and Garbage Collection"},{"content":"Managed Heap and Garbage Collection (Continued) Large Objects CLR divides objects into large objects and small objects. Currently, objects of 85000 bytes or larger are considered large objects. CLR treats large objects differently.\nLarge objects are not allocated in the address space of small objects, but in other places in the process address space.\nThe current version of GC does not \u0026ldquo;compress\u0026rdquo; large objects because moving them in memory is too costly. But this may cause address space fragmentation between large objects in the process, leading to OutOfMemoryException. Future versions of CLR may compress large objects.\nLarge objects are always generation 2, never generation 0 or generation 1. So only create large objects for resources that need to live long. Allocating short-lived large objects will cause generation 2 to be recycled more frequently, losing performance. Large objects are generally large strings (XML/JSON) or byte arrays used for I/O operations (reading bytes from files/network into buffers for processing).\nGarbage Collection Modes When CLR starts, it selects a GC mode, and the mode in the process will not change before.\nThere are two basic GC modes.\nWorkstation This mode optimizes GC for client applications. The delay caused by GC is very low, and the application thread suspension time is short, avoiding user anxiety. In this mode, GC assumes that other applications running on the machine do not consume too much CPU resources. Server This mode optimizes GC for server applications. What is optimized is mainly throughput and resource utilization. GC assumes that no other applications are running on the machine (whether client or server applications), and assumes that all CPUs on the machine can be used to assist in completing GC. In this mode, the managed heap is split into several regions, one for each CPU. When starting garbage collection, the garbage collector runs a special thread on each CPU; each thread recycles its own region concurrently with other threads. For server applications where worker threads behave consistently, concurrent recycling can work well. This feature requires the application to run on a multi-CPU computer, so that threads can truly work simultaneously, thereby gaining performance improvement. Applications run in \u0026ldquo;workstation\u0026rdquo; GC mode by default. Server applications hosting CLR (such as ASP.NET) can request CLR to load server GC. But if the application runs on a single-processor computer, CLR always uses \u0026ldquo;workstation\u0026rdquo; GC mode.\nStandalone applications can create a configuration file to tell CLR to use the server recycler. The configuration file needs to add the gcServer element for the application. Below is an example configuration file:\n1 2 3 4 5 \u0026lt;configuration\u0026gt; \u0026lt;runtime\u0026gt; \u0026lt;gcServer enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;/runtime\u0026gt; \u0026lt;/configuration\u0026gt; You can use the read-only Boolean property IsServerGC of the GCSettings class to get whether CLR is in \u0026ldquo;server\u0026rdquo; GC mode.\nIn addition to these two modes, GC also supports two sub-modes: concurrent (default) or non-concurrent.\nIn concurrent mode, the garbage collector has an additional background thread that can mark objects concurrently while the application is running. While the program is running, the garbage collector runs a normal priority background thread to find unreachable objects. After finding them, the garbage collector suspends all threads again, determines whether to \u0026ldquo;compress\u0026rdquo; memory. If it decides to compress, memory is compressed, root references are corrected, and application threads resume running. This garbage collection takes less time than usual because the unreachable object collection has been constructed. But the garbage collector may also decide not to compress memory; in fact, the garbage collector tends not to compress. The more available memory, the less likely the garbage collector is to compress the heap; this helps enhance performance, but increases the program\u0026rsquo;s working set. Using concurrent garbage collector, applications usually consume more memory than using non-concurrent garbage collector.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/c%23.net-managed-heap-and-garbage-collection-continued/","title":"C#.NET Managed Heap and Garbage Collection (Continued)"},{"content":"The CLR provides each AppDomain with a GC Handle Table that lets applications monitor or explicitly control object lifetimes. The table is empty when the AppDomain is created.\nEach entry contains:\nA reference to an object on the managed heap A flag describing how to monitor/control that object You typically interact with this via System.Runtime.InteropServices.GCHandle (e.g., GCHandleType.Weak, WeakTrackResurrection, Normal, Pinned). Use cases include pinning objects for interop or creating weak references without allocating WeakReference.\nExample (pinning):\n1 2 3 4 5 6 7 8 9 10 11 var bytes = new byte[1024]; var handle = GCHandle.Alloc(bytes, GCHandleType.Pinned); try { IntPtr ptr = handle.AddrOfPinnedObject(); // pass ptr to native API } finally { handle.Free(); } Example (weak):\n1 2 3 4 5 6 var target = new object(); var handle = GCHandle.Alloc(target, GCHandleType.Weak); target = null; GC.Collect(); var alive = handle.Target != null; // may be null after collection handle.Free(); Prefer high-level WeakReference unless you specifically need GCHandle semantics (e.g., pinning or resurrection tracking). Always Free() to avoid leaks.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/clr-gc-handle-table-monitoring-and-controlling-object-lifetime/","title":"CLR GC Handle Table â€” Monitoring and Controlling Object Lifetime"},{"content":" Download mysql-connector-net installation mysql-connector-net\nAfter mysql-connector-net installation is complete, go to the corresponding installation directory and copy the corresponding MySQL .NET dll to CodeSmith\u0026rsquo;s bin directory and SchemaProviders directory. The general DLL directory is:\nC:\\Program Files (x86)\\MySQL\\MySQL Connector Net 6.9.8\\Assemblies\\v4.0\nRestart CodeSmith to take effect Other solutions: Solution for codesmith unable to connect to Mysql\nSolution for codesmith6.5 connecting to Mysql prompting \u0026ldquo;Cannot find the requested .Net Framework Data Provider. It may not be installed.\u0026rdquo;\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/codesmith-connect-mysql-database-error-cant-find-.net-framework-data-provider/","title":"CodeSmith Connect MySQL Database Error \"Can't Find .NET Framework Data Provider\""},{"content":"CodeSmith is a capable code generator on .NET. I used to target SQL Server with a set of templates for Model/DAL/BLL. After switching to MySQL, I found fewer resources, so I adapted templates and noted the process here.\nWorkflow: copy the template folder into CodeSmithâ€™s template path, open CodeSmith, rightâ€‘click Main.cst â†’ Execute â†’ select the MySQL connection â†’ choose tables â†’ Generate.\nProblem: the original Main.cst used SchemaExplorer.TableSchema so you could only pick a single table. Solution: change it to SchemaExplorer.TableSchemaCollection and loop.\nSnippet:\n1 2 3 4 5 6 7 8 9 10 \u0026lt;%@ Property Name=\u0026#34;SourceTables\u0026#34; Type=\u0026#34;SchemaExplorer.TableSchemaCollection\u0026#34; %\u0026gt; \u0026lt;%@ Register Name=\u0026#34;SE\u0026#34; Template=\u0026#34;CreatSingleTable.cst\u0026#34; %\u0026gt; \u0026lt;% foreach(TableSchema ts in SourceTables) { SE s = new SE(); s.SourceTable = ts; s.RootNamespace = RootNamespace; s.OutputDirectory = OutputDirectory; s.Render(this.Response); } %\u0026gt; This generates Model/DAL/BLL for all selected tables in one go.\nField comments: the default MySQL provider SchemaExplorer.MySQLSchemaProvider.dll doesnâ€™t populate column.Description. Replace the provider with a patched version to read column comments:\nHowâ€‘to (Chinese): http://www.cnblogs.com/LonelyShadow/p/4147743.html\nTemplate repo: https://github.com/liguobao/CodeSmith-for-MySQL-Template\nNotes:\nThe template trims the first three chars of table names (prefix like tbl_ recommended), or adjust the template. Backup and replace SchemaExplorer.MySQLSchemaProvider.dll to enable column comments. ","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/codesmith-templates-for-mysql-multi-table-generation/","title":"CodeSmith Templates for MySQL â€” Multi-Table Generation"},{"content":"Battery usage report (Windows) Source: Microsoft Answers â€” Battery report / history\nOpen the taskbar search box, search for â€œPowerShellâ€. Rightâ€‘click â€œWindows PowerShellâ€ â†’ Run as administrator. Run this oneâ€‘liner to generate a localized HTML battery report on Desktop:\n1 $HTML=[System.Environment]::GetFolderPath(\u0026#39;Desktop\u0026#39;)+\u0026#34;\\\u0026#34;+(Get-Date -Format \u0026#39;yyyy-MM-dd\u0026#39;)+\u0026#34;-ç”µæ± è®°å½•.html\u0026#34;;POWERCFG /BATTERYREPORT /OUTPUT \u0026#34;$HTML\u0026#34;;$TF=Get-Content \u0026#34;$HTML\u0026#34;;$TF| %{$_.Replace(\u0026#34;Battery report\u0026#34;,\u0026#34;ç”µæ± æŠ¥å‘Š\u0026#34;)}| %{$_.Replace(\u0026#34;COMPUTER NAME\u0026#34;,\u0026#34;è®¡ç®—æœºå\u0026#34;)}| %{$_.Replace(\u0026#34;SYSTEM PRODUCT NAME\u0026#34;,\u0026#34;è®¡ç®—æœºå‹å·\u0026#34;)}| %{$_.Replace(\u0026#34;OS BUILD\u0026#34;,\u0026#34;æ“ä½œç³»ç»Ÿå†…éƒ¨ç‰ˆæœ¬\u0026#34;)}| %{$_.Replace(\u0026#34;PLATFORM ROLE\u0026#34;,\u0026#34;å¹³å°è§’è‰²\u0026#34;)}| %{$_.Replace(\u0026#34;CONNECTED STANDBY\u0026#34;,\u0026#34;InstantGoï¼ˆè¿æ¥å¾…æœºï¼‰\u0026#34;)}| %{$_.Replace(\u0026#34;REPORT TIME\u0026#34;,\u0026#34;æŠ¥å‘Šæ—¶é—´\u0026#34;)}| %{$_.Replace(\u0026#34;Installed batteries\u0026#34;,\u0026#34;å·²å®‰è£…çš„ç”µæ± \u0026#34;)}| %{$_.Replace(\u0026#34;Information about each currently installed battery\u0026#34;,\u0026#34;æŸ¥çœ‹å½“å‰å·²å®‰è£…ç”µæ± çš„ä¿¡æ¯\u0026#34;)}| %{$_.Replace(\u0026#34;NAME\u0026#34;,\u0026#34;åç§°\u0026#34;)}| %{$_.Replace(\u0026#34;MANUFACTURER\u0026#34;,\u0026#34;åˆ¶é€ å•†\u0026#34;)}| %{$_.Replace(\u0026#34;SERIAL NUMBER\u0026#34;,\u0026#34;åºåˆ—å·\u0026#34;)}| %{$_.Replace(\u0026#34;CHEMISTRY\u0026#34;,\u0026#34;åŒ–å­¦æˆåˆ†\u0026#34;)}| %{$_.Replace(\u0026#34;AT DESIGN CAPACITY\u0026#34;,\u0026#34;è®¾è®¡å®¹é‡æ—¶\u0026#34;)}| %{$_.Replace(\u0026#34;DESIGN CAPACITY\u0026#34;,\u0026#34;è®¾è®¡å®¹é‡\u0026#34;)}| %{$_.Replace(\u0026#34;FULL CHARGE CAPACITY\u0026#34;,\u0026#34;å®Œå…¨å……ç”µå®¹é‡\u0026#34;)}| %{$_.Replace(\u0026#34;CYCLE COUNT\u0026#34;,\u0026#34;å¾ªç¯è®¡æ•°\u0026#34;)}| %{$_.Replace(\u0026#34;Recent usage\u0026#34;,\u0026#34;æœ€è¿‘ä½¿ç”¨æƒ…å†µ\u0026#34;)}| %{$_.Replace(\u0026#34;Power states over the last 3 days\u0026#34;,\u0026#34;è¿‡å»72å°æ—¶å†…çš„ç”µæºçŠ¶æ€\u0026#34;)}| %{$_.Replace(\u0026#34;START TIME\u0026#34;,\u0026#34;å¼€å§‹æ—¶é—´\u0026#34;)}| %{$_.Replace(\u0026#34;STATE\u0026#34;,\u0026#34;çŠ¶æ€\u0026#34;)}| %{$_.Replace(\u0026#34;SOURCE\u0026#34;,\u0026#34;ç”µæº\u0026#34;)}| %{$_.Replace(\u0026#34;CAPACITY REMAINING\u0026#34;,\u0026#34;å‰©ä½™å®¹é‡\u0026#34;)}| %{$_.Replace(\u0026#34;Battery usage\u0026#34;,\u0026#34;ç”µæ± ä½¿ç”¨æƒ…å†µ\u0026#34;)}| %{$_.Replace(\u0026#34;Battery drains over the last 3 days\u0026#34;,\u0026#34;è¿‡å»72å°æ—¶å†…çš„ç”µæ± æ¶ˆè€—\u0026#34;)}| %{$_.Replace(\u0026#34;DURATION\u0026#34;,\u0026#34;ä½¿ç”¨æ—¶é—´\u0026#34;)}| %{$_.Replace(\u0026#34;ENERGY DRAINED\u0026#34;,\u0026#34;æ¶ˆè€—çš„èƒ½é‡\u0026#34;)}| %{$_.Replace(\u0026#34;Usage history\u0026#34;,\u0026#34;ä½¿ç”¨å†å²è®°å½•\u0026#34;)}| %{$_.Replace(\u0026#34;History of system usage on AC and battery\u0026#34;,\u0026#34;æœ‰å…³äº¤æµç”µæºå’Œç”µæ± çš„ä½¿ç”¨è®°å½•\u0026#34;)}| %{$_.Replace(\u0026#34;BATTERY DURATION\u0026#34;,\u0026#34;ç”µæ± ä½¿ç”¨æ—¶é—´\u0026#34;)}| %{$_.Replace(\u0026#34;AC DURATION \u0026#34;,\u0026#34;äº¤æµç”µæºä½¿ç”¨æ—¶é—´\u0026#34;)}| %{$_.Replace(\u0026#34;PERIOD\u0026#34;,\u0026#34;å‘¨æœŸ\u0026#34;)}| %{$_.Replace(\u0026#34;ACTIVE\u0026#34;,\u0026#34;æ´»åŠ¨\u0026#34;)}| %{$_.Replace(\u0026#34;Battery capacity history\u0026#34;,\u0026#34;ç”µæ± è®¾è®¡å®¹é‡å†å²è®°å½•\u0026#34;)}| %{$_.Replace(\u0026#34;Charge capacity history of the system\u0026#39;s batteries\u0026#34;,\u0026#34;ç”µæ± å……ç”µèƒ½åŠ›å†å²è®°å½•\u0026#34;)}| %{$_.Replace(\u0026#34;Battery life estimates based on observed drains\u0026#34;,\u0026#34;ä»¥è§‚å¯Ÿåˆ°çš„æ¶ˆè€—æƒ…å†µé¢„è®¡ç”µæ± å¯¿å‘½\u0026#34;)}| %{$_.Replace(\u0026#34;Battery life estimates\u0026#34;,\u0026#34;é¢„è®¡ç”µæ± å¯¿å‘½\u0026#34;)}| %{$_.Replace(\u0026#34;AT FULL CHARGE\u0026#34;,\u0026#34;å®Œå…¨å……ç”µæ—¶\u0026#34;)}| %{$_.Replace(\u0026#34;Current estimate of battery life based on all observed drains since OS install\u0026#34;,\u0026#34;ä»¥æ“ä½œç³»ç»Ÿå®‰è£…åæ‰€æœ‰è§‚å¯Ÿåˆ°çš„æ¶ˆè€—è®°å½•ä¸ºåŸºç¡€é¢„è®¡çš„å½“å‰ç”µæ± å¯¿å‘½\u0026#34;)}| %{$_.Replace(\u0026#34;Since OS install\u0026#34;,\u0026#34;ä»æ“ä½œç³»ç»Ÿå®‰è£…å\u0026#34;)}| %{$_.Replace(\u0026#34;Supported\u0026#34;,\u0026#34;æ”¯æŒ\u0026#34;)}| %{$_.Replace(\u0026#34;Not supported\u0026#34;,\u0026#34;ä¸æ”¯æŒ\u0026#34;)}| %{$_.Replace(\u0026#34;BATTERY\u0026#34;,\u0026#34;ç”µæ± \u0026#34;)}| %{$_.Replace(\u0026#34;Suspended\u0026#34;,\u0026#34;å·²æš‚åœ\u0026#34;)}| %{$_.Replace(\u0026#34;Active\u0026#34;,\u0026#34;æ´»åŠ¨\u0026#34;)}| %{$_.Replace(\u0026#34;Unspecified\u0026#34;,\u0026#34;æœªçŸ¥\u0026#34;)}| %{$_.Replace(\u0026#34;Mobile\u0026#34;,\u0026#34;ç§»åŠ¨\u0026#34;)}| %{$_.Replace(\u0026#34;Desktop\u0026#34;,\u0026#34;æ¡Œé¢\u0026#34;)}| %{$_.Replace(\u0026#34;Workstation\u0026#34;,\u0026#34;å·¥ä½œç«™\u0026#34;)}| %{$_.Replace(\u0026#34;Report generated\u0026#34;,\u0026#34;ç”Ÿæˆå½“å‰æŠ¥å‘Š\u0026#34;)}| %{$_.Replace(\u0026#34;Battery\u0026#34;,\u0026#34;ç”µæ± \u0026#34;)}| %{$_.Replace(\u0026#34;AC\u0026#34;,\u0026#34;äº¤æµç”µæº\u0026#34;)}|Out-File \u0026#34;$HTML\u0026#34;;pause ","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/handy-scripts-collection/","title":"Handy Scripts Collection"},{"content":"As we all know, when HTTPS pages request HTTP resources, modern browsers will intercept, prompt the user whether to continue, or directly intercept without prompting.\nRecently, I made a quick bookmark tool for myself. Clicking the bookmark directly sends the bookmark to the server address and saves it to my website.\nAt first, everything was normal, but when I encountered HTTPS websites, it crashed.\nAt first, I saw that HTTPS certificates are charged, so I thought forget it, as long as it works. A few days ago, I occasionally saw an open source software for free HTTPS application, and it looked good. These days I had time and started working on it. Below is a tutorial, and I applied for the certificate almost according to this.\nGet Free Certificate with Let\u0026rsquo;s Encrypt\nAbout this Let\u0026rsquo;s Encrypt, Wikipedia introduces it like this:\nLet\u0026rsquo;s Encrypt is a digital certificate certification authority that will be launched at the end of 2015, which will provide free SSL/TLS certificates for secure websites through an automated process designed to eliminate the complexity of manually creating and installing certificates. Let\u0026rsquo;s Encrypt is a service provided by the Internet Security Research Group (ISRG, a public welfare organization). Major sponsors include the Electronic Frontier Foundation, Mozilla Foundation, Akamai, and Cisco. On April 9, 2015, ISRG and the Linux Foundation announced cooperation to implement the protocol for this new digital certificate certification authority called Automatic Certificate Management Environment (ACME). There is a draft of this specification on GitHub, and a version of the proposal has been published as an Internet draft. Let\u0026rsquo;s Encrypt claims that this process will be very simple, automated, and free. On August 7, 2015, the service updated its launch plan, expecting to release the first certificate on September 7, 2015, and then issue a small number of certificates to whitelisted domains and gradually expand. If everything goes as planned, the service is expected to fully start providing on November 16, 2015.\nThe entire project has code on Github, mainly through the client to generate https certificates for our website.\nFirst, we download the client, as follows:\n1 git clone https://github.com/letsencrypt/letsencrypt.git Then enter this repository and execute the following code:\n1 ./letsencrypt-auto certonly -a webroot --webroot-path website path (such as: /var/www/web/) -d your domain (such as: test.online) -d www.your domain (such as: www.test.online) Here, note that I added line breaks for typesetting here, remember to remove the line breaks when running this command.\nLine breaks are in front of webroot and -d.\nIf everything goes well, we can see four files in the /etc/letsencrypt/live/domain/ directory, namely:\nDomain certificate file\nCertificate chain file that issued the domain certificate\nDomain certificate + certificate chain file\nPrivate key file\nAs shown in the figure:\nThen set the certificate for the website.\nJexus setting HTTPS requires changing the jws.conf document and the website configuration document.\nOperation steps are as follows:\nModify jws.conf Enter the Jexus folder, open \u0026ldquo;jws.conf\u0026rdquo;, add the following two sentences:\n1 2 CertificateFile = /etc/letsencrypt/live/domain/fullchain.pem CertificateKeyFile = /etc/letsencrypt/live/domain/privkey.pem The effect after modification is as follows:\nEnable HTTPS for the website Enter the siteconf/ folder, find the corresponding website conf file,\nChange the website service port to 443:\nport=443\nEnable https:\nUseHttps=true\nThe effect after modification is as follows:\nThen restart jexus.\nAfter completion, you can access through HTTPS.\nFinally, upload a picture of the HTTPS certificate to prove that this is feasible.\nSprinkle flowers, see you next time.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/jexus-supports-https-protocol/","title":"Jexus Supports HTTPS Protocol"},{"content":"Cause: TLS/SSL configuration and cert chain issues under Mono. Fixes include enabling proper TLS versions and trusting root CAs.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/mono-webrequest-https-exception-fix/","title":"Mono WebRequest HTTPS Exception â€” Fix"},{"content":"Steps to migrate the original ASP.NET MVC project to ASP.NET Core:\nInstall VS2015 + .NET Core SDK + VS2015 Tooling (Preview 2 at the time); fix common installer issues (0x80072f8a) per linked guide.\nCreate a new empty ASP.NET Core app; add NuGet packages (AngleSharp, Newtonsoft.Json, etc.).\nCopy Controllers/Views; in Core, static files live under wwwroot, so move assets accordingly.\nConfigure Startup:\n1 2 3 4 5 6 7 8 9 public void ConfigureServices(IServiceCollection services) =\u0026gt; services.AddMvc(); public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory lf) { lf.AddConsole(); if (env.IsDevelopment()) app.UseDeveloperExceptionPage(); app.UseStaticFiles(); app.UseMvc(routes =\u0026gt; { routes.MapRoute(name: \u0026#34;default\u0026#34;, template: \u0026#34;{controller=House}/{action=Index}/{id?}\u0026#34;); }); } Rewrite GetHTMLByURL using WebRequest (Core changed HttpWebRequest usage) and proper encodings. The rest is adapting namespaces/usings and resolving API differences.\n","date":"2016-10-04T00:00:00Z","permalink":"https://liguobao.github.io/p/porting-the-58.com-rental-map-to-asp.net-core/","title":"Porting the 58.com Rental Map to ASP.NET Core"},{"content":"Chat Logs with My Grandparents Part 1 â€” On Landlords\nMe: It probably wasnâ€™t them. In village XX (near my hometown) there was a landlord family. A classmate of mine is the greatâ€‘grandson; his grandfather (surname Deng) passed away when I was in high school. His grandma was the daughter of a landlord from another place. Deng Mingguang? Is that the person who owned the small threeâ€‘story cement house (whitewashed) in XX from the Republic era?\nGrandpa: That would be Deng Mingguangâ€™s son, right?\nGrandma: Itâ€™s noon now â€” people are probably not around. (She chimed in; I donâ€™t recall why.)\nâ€¦\nPart 2 â€” Fishing Nets\nMe: Grandpa, how did you make fishing nets back then?\nGrandpa: Hm?\nMe: The nets you used for fishing â€” how were they made?\nâ€¦ (Original Chinese continues with daily life details; translated summary retained.)\n","date":"2015-08-31T00:00:00Z","permalink":"https://liguobao.github.io/p/chat-logs-with-my-grandparents/","title":"Chat Logs with My Grandparents"}]